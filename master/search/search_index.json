{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome NPC Engine is an inference engine that allows you to design game NPC AI using natural language. It uses state of the art deep learning models and allows you to: Generate NPC lines based on the natural language descriptions of a character (e.g. his persona , location). Voice those lines with multiple voices (current model has 127) Trigger discrete actions based on semantic similarity between an action description and a player or NPC line. Create APIs to your own deep learning models. Use npc-engine Unity Unreal Engine 4 Godot Other Extend npc-engine Export model using existing API Add new API","title":"Home"},{"location":"#welcome","text":"NPC Engine is an inference engine that allows you to design game NPC AI using natural language. It uses state of the art deep learning models and allows you to: Generate NPC lines based on the natural language descriptions of a character (e.g. his persona , location). Voice those lines with multiple voices (current model has 127) Trigger discrete actions based on semantic similarity between an action description and a player or NPC line. Create APIs to your own deep learning models.","title":"Welcome"},{"location":"#use-npc-engine","text":"Unity Unreal Engine 4 Godot Other","title":"Use npc-engine"},{"location":"#extend-npc-engine","text":"Export model using existing API Add new API","title":"Extend npc-engine"},{"location":"benchmarks/","text":"What about performance? Here are the numbers: i5-9600K + GTX1070 with default models GPU VRAM Before starting inference engine: memory.used [MiB] 1213 MiB After starting inference engine: memory.used [MiB] 4310 MiB Text to speech Latency (time before first result): 1.0473401546478271 seconds All the next iterations have real-time factor < 1.0 Semantic similarity Similarity betwen short phrases 'I will help you' and 'I shall provide you my assistance' is computed in 0.06283211708068848s Chatbot Chatbot reply Hello partner! Ornament please. Such a delightful pup! You are loyally loyal to your master? to a big context generated in 1.411924123764038s Run the benchmark test on your computer Warning Requires Nvidia GPU because it uses nvidia-smi command line tool Comment out test skipping in tests\\benchmarks\\benchmark.py Run it with pytest tests\\benchmarks\\benchmark.py -s At the moment the output is not pretty, it's a work in progress.","title":"Benchmarks"},{"location":"benchmarks/#here-are-the-numbers","text":"","title":"Here are the numbers:"},{"location":"benchmarks/#i5-9600k-gtx1070-with-default-models","text":"","title":"i5-9600K + GTX1070 with default models"},{"location":"benchmarks/#gpu-vram","text":"Before starting inference engine: memory.used [MiB] 1213 MiB After starting inference engine: memory.used [MiB] 4310 MiB","title":"GPU VRAM"},{"location":"benchmarks/#text-to-speech","text":"Latency (time before first result): 1.0473401546478271 seconds All the next iterations have real-time factor < 1.0","title":"Text to speech"},{"location":"benchmarks/#semantic-similarity","text":"Similarity betwen short phrases 'I will help you' and 'I shall provide you my assistance' is computed in 0.06283211708068848s","title":"Semantic similarity"},{"location":"benchmarks/#chatbot","text":"Chatbot reply Hello partner! Ornament please. Such a delightful pup! You are loyally loyal to your master? to a big context generated in 1.411924123764038s","title":"Chatbot"},{"location":"benchmarks/#run-the-benchmark-test-on-your-computer","text":"Warning Requires Nvidia GPU because it uses nvidia-smi command line tool Comment out test skipping in tests\\benchmarks\\benchmark.py Run it with pytest tests\\benchmarks\\benchmark.py -s At the moment the output is not pretty, it's a work in progress.","title":"Run the benchmark test on your computer"},{"location":"not_ready/","text":"This integration is not ready yet But don't get discouraged, it's quite easy to create! Check out the integration checklist . If you plan to create one, please consider contributing! To contribute a new integration: Create an issue to discuss the integration architecture with us. Develop the integration, preferably with demo scene. Share it with permissive open source license. Create a pull request to npc-engine repo with documentation about your integration.","title":"This integration is not ready yet"},{"location":"not_ready/#this-integration-is-not-ready-yet","text":"But don't get discouraged, it's quite easy to create! Check out the integration checklist . If you plan to create one, please consider contributing! To contribute a new integration: Create an issue to discuss the integration architecture with us. Develop the integration, preferably with demo scene. Share it with permissive open source license. Create a pull request to npc-engine repo with documentation about your integration.","title":"This integration is not ready yet"},{"location":"inference_engine/api_classes/","text":"API class is an abstract class that corresponds to a certain task a service should perform (e.g. text-to-speech or chatbot) and defines interface methods for such a task as well as abstract methods for specific servicess to implement. All API classes are children of the BaseService class that handles registering service implementations and running them. Important It also should list the methods that are to be exposed as API via API_METHODS class variable. Important To be discovered correctly api classes must be imported into npc_engine.services module Existing APIs These are the existing API classes and corresponding API_METHODS: npc_engine.services.sequence_classifier.sequence_classifier_base.SequenceClassifierAPI Bases: BaseService Abstract base class for text classification models. __init__ ( cache_size = 0 , * args , ** kwargs ) Empty initialization method for API to be similar to other model base classes. classify ( texts ) Classify a list of texts. Parameters: Name Type Description Default texts List [ str ] A list of texts to classify. required Returns: Type Description List [ List [ float ]] List of scores for each text. compute_scores_batch ( texts ) abstractmethod Compute scores for a list of texts. Parameters: Name Type Description Default texts List [ str ] A list of texts to compute scores for. required Returns: Type Description np . ndarray List of scores for each text. get_api_name () classmethod Get the API name. npc_engine.services.chatbot.chatbot_base.ChatbotAPI Bases: BaseService Abstract base class for Chatbot models. __init__ ( template_string , * args , ** kwargs ) Initialize prompt formatting variables. Parameters: Name Type Description Default template_string str Template string to be rendered as prompt. required generate_reply ( context , * args , ** kwargs ) Format the model prompt and generate response. Parameters: Name Type Description Default context Dict [ str , Any ] Prompt context. required Returns: Type Description str Text response to a prompt. get_api_name () classmethod Get the API name. get_context_template () Return context template. Returns: Type Description Dict [ str , Any ] Example context get_prompt_template () Return prompt template string used to render model prompt. Returns: Type Description str A template string. get_special_tokens () abstractmethod Return dictionary mapping for special tokens. To be implemented by child class. Can then be used in template string as fields Returns: Type Description Dict [ str , str ] Dictionary of special tokens run ( prompt , temperature = 1 , topk = None ) abstractmethod Abstract method for concrete implementation of generation. Parameters: Name Type Description Default prompt str Fromatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text npc_engine.services.similarity.similarity_base.SimilarityAPI Bases: BaseService Abstract base class for text similarity models. __init__ ( cache_size = 0 , * args , ** kwargs ) Empty initialization method for API to be similar to other model base classes. cache ( context ) Cache embeddings of given sequences. Parameters: Name Type Description Default context List [ str ] A list of sentences to cache. required compare ( query , context ) Compare a query to the context. Parameters: Name Type Description Default query str A sentence to compare. required context List [ str ] A list of sentences to compare to. This will be cached if caching is enabled required Returns: Type Description List [ float ] List of similarities compute_embedding ( line ) abstractmethod Compute sentence embedding. Parameters: Name Type Description Default line str Sentence to embed required Returns: Type Description np . ndarray Embedding of shape (1, embedding_size) compute_embedding_batch ( lines ) abstractmethod Compute line embeddings in batch. Parameters: Name Type Description Default lines List [ str ] List of sentences to embed required Returns: Type Description np . ndarray Embedding batch of shape (batch_size, embedding_size) get_api_name () classmethod Get the API name. metric ( embedding_a , embedding_b ) abstractmethod Compute distance between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Parameters: Name Type Description Default embedding_a np . ndarray Embedding of shape (1 or batch_size, embedding_size) required embedding_b np . ndarray Embedding of shape (1 or batch_size, embedding_size) required Returns: Type Description np . ndarray Vector of distances (batch_size or 1,) npc_engine.services.tts.tts_base.TextToSpeechAPI Bases: BaseService Abstract base class for text-to-speech models. __init__ ( * args , ** kwargs ) Empty initialization method for API to be similar to other model base classes. _chain_run ( speaker_id , sentences , n_chunks ) Chain the run method to be used in the generator. get_api_name () classmethod Get the API name. get_speaker_ids () abstractmethod Get list of available speaker ids. Returns: Type Description List [ str ] The return value. True for success, False otherwise. run ( speaker_id , text , n_chunks ) abstractmethod Create a generator for iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Returns: Type Description Iterable [ np . ndarray ] Generator that yields next chunk of speech in the form of f32 ndarray. tts_get_results () Retrieve the next chunk of generated speech. Returns: Type Description Iterable [ np . ndarray ] Next chunk of speech in the form of f32 ndarray. tts_start ( speaker_id , text , n_chunks ) Initiate iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Creating new APIs You can use this dummy API example to create your own: from npc_engine.services.base_service import BaseService class EchoAPI(BaseService): API_METHODS: List[str] = [\"echo\"] def __init__(self, *args, **kwargs): pass def echo(self, text): return text Dont forget Import new API to npc-engine.services so that it is discovered. Models that are implemented for the API should appear there too. Calling other APIs There are a set of service clients that can be used to call other APIs that are running on the server. They will block other services so they should be used carefully. Module implementing the clients for services. chatbot_client Huggingface chatbot interface client implementation. ChatbotClient Bases: ServiceClient Json rpc client for chatbot service. Source code in npc_engine\\service_clients\\chatbot_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class ChatbotClient ( ServiceClient ): \"\"\"Json rpc client for chatbot service.\"\"\" def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str = \"ChatbotAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , service_id ) def generate_reply ( self , context : Dict [ str , Any ]) -> str : \"\"\"Send a chatbot request to the server. Args: context: A dictionary containing the chatbot request. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"generate_reply\" , \"id\" : 0 , \"params\" : [ context ], } reply = self . send_request ( request ) return reply def get_prompt_template ( self ) -> str : \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_prompt_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) def get_context_template ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_context_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) def get_special_tokens ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_special_tokens\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) __init__ ( zmq_context , port , service_id = 'ChatbotAPI' ) Connect to the server on the port. Source code in npc_engine\\service_clients\\chatbot_client.py 10 11 12 13 14 def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str = \"ChatbotAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , service_id ) generate_reply ( context ) Send a chatbot request to the server. Parameters: Name Type Description Default context Dict [ str , Any ] A dictionary containing the chatbot request. required Source code in npc_engine\\service_clients\\chatbot_client.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def generate_reply ( self , context : Dict [ str , Any ]) -> str : \"\"\"Send a chatbot request to the server. Args: context: A dictionary containing the chatbot request. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"generate_reply\" , \"id\" : 0 , \"params\" : [ context ], } reply = self . send_request ( request ) return reply get_context_template () Send a chatbot request to the server. Source code in npc_engine\\service_clients\\chatbot_client.py 41 42 43 44 45 46 47 48 49 def get_context_template ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_context_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) get_prompt_template () Send a chatbot request to the server. Source code in npc_engine\\service_clients\\chatbot_client.py 31 32 33 34 35 36 37 38 39 def get_prompt_template ( self ) -> str : \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_prompt_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) get_special_tokens () Send a chatbot request to the server. Source code in npc_engine\\service_clients\\chatbot_client.py 51 52 53 54 55 56 57 58 59 def get_special_tokens ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_special_tokens\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) control_client Control interface client implementation. ControlClient Bases: ServiceClient Json rpc client for control requests. Source code in npc_engine\\service_clients\\control_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 class ControlClient ( ServiceClient ): \"\"\"Json rpc client for control requests.\"\"\" def __init__ ( self , zmq_context : zmq . Context , port : str ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , \"control\" ) def start_service ( self , service_id ): \"\"\"Send a start service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"start_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) def stop_service ( self , service_id ): \"\"\"Send a stop service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"stop_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) def get_service_status ( self , service_id ) -> str : \"\"\"Send a get service status request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_service_status\" , \"id\" : 0 , \"params\" : [ service_id ], } return self . send_request ( request ) def restart_service ( self , service_id ): \"\"\"Send a restart service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"restart_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) def get_services_metadata ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Send a get services metadata request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_services_metadata\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) __init__ ( zmq_context , port ) Connect to the server on the port. Source code in npc_engine\\service_clients\\control_client.py 10 11 12 def __init__ ( self , zmq_context : zmq . Context , port : str ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , \"control\" ) get_service_status ( service_id ) Send a get service status request to the server. Source code in npc_engine\\service_clients\\control_client.py 34 35 36 37 38 39 40 41 42 def get_service_status ( self , service_id ) -> str : \"\"\"Send a get service status request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_service_status\" , \"id\" : 0 , \"params\" : [ service_id ], } return self . send_request ( request ) get_services_metadata () Send a get services metadata request to the server. Source code in npc_engine\\service_clients\\control_client.py 54 55 56 57 58 59 60 61 62 def get_services_metadata ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Send a get services metadata request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_services_metadata\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) restart_service ( service_id ) Send a restart service request to the server. Source code in npc_engine\\service_clients\\control_client.py 44 45 46 47 48 49 50 51 52 def restart_service ( self , service_id ): \"\"\"Send a restart service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"restart_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) start_service ( service_id ) Send a start service request to the server. Source code in npc_engine\\service_clients\\control_client.py 14 15 16 17 18 19 20 21 22 def start_service ( self , service_id ): \"\"\"Send a start service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"start_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) stop_service ( service_id ) Send a stop service request to the server. Source code in npc_engine\\service_clients\\control_client.py 24 25 26 27 28 29 30 31 32 def stop_service ( self , service_id ): \"\"\"Send a stop service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"stop_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) sequence_classifier_client Huggingface chatbot interface client implementation. SequenceClassifierClient Bases: ServiceClient Json rpc client for chatbot service. Source code in npc_engine\\service_clients\\sequence_classifier_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class SequenceClassifierClient ( ServiceClient ): \"\"\"Json rpc client for chatbot service.\"\"\" def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str = \"SequenceClassifierAPI\" , ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , service_id ) def classify ( self , texts : List [ Union [ str , Tuple [ str , str ]]]) -> str : \"\"\"Send a classify request to the SequenceClassifierAPI. Args: texts: List of texts to classify. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"classify\" , \"id\" : 0 , \"params\" : [ texts ], } reply = self . send_request ( request ) return reply __init__ ( zmq_context , port , service_id = 'SequenceClassifierAPI' ) Connect to the server on the port. Source code in npc_engine\\service_clients\\sequence_classifier_client.py 10 11 12 13 14 15 16 17 def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str = \"SequenceClassifierAPI\" , ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , service_id ) classify ( texts ) Send a classify request to the SequenceClassifierAPI. Parameters: Name Type Description Default texts List [ Union [ str , Tuple [ str , str ]]] List of texts to classify. required Source code in npc_engine\\service_clients\\sequence_classifier_client.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def classify ( self , texts : List [ Union [ str , Tuple [ str , str ]]]) -> str : \"\"\"Send a classify request to the SequenceClassifierAPI. Args: texts: List of texts to classify. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"classify\" , \"id\" : 0 , \"params\" : [ texts ], } reply = self . send_request ( request ) return reply service_client Module that implements ZMQ base client communication over JSON-RPC 2.0 (https://www.jsonrpc.org/specification). ServiceClient Base json rpc client. Source code in npc_engine\\service_clients\\service_client.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class ServiceClient : \"\"\"Base json rpc client.\"\"\" def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str ): \"\"\"Connect to the server on the port.\"\"\" self . context = zmq_context self . socket = self . context . socket ( zmq . REQ ) self . socket . setsockopt ( zmq . LINGER , 0 ) self . socket . setsockopt ( zmq . IDENTITY , service_id . encode ( \"utf-8\" )) self . socket . connect ( f \"tcp://localhost: { port } \" ) logger . info ( \"Connected to server\" ) def send_request ( self , request : Dict [ str , Any ]) -> Any : \"\"\"Send request to the server and return the response. Args: request: The request to send to the server. Returns: The result from the server. \"\"\" logger . trace ( f \"Sending request: { request } \" ) self . socket . send_json ( request ) response = self . socket . recv_json () logger . trace ( f \"Received response: { response } \" ) if \"result\" in response : return response [ \"result\" ] elif \"code\" in response : raise RuntimeError ( f \"code: { response [ 'code' ] } . { response [ 'message' ] } \" ) __init__ ( zmq_context , port , service_id ) Connect to the server on the port. Source code in npc_engine\\service_clients\\service_client.py 12 13 14 15 16 17 18 19 def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str ): \"\"\"Connect to the server on the port.\"\"\" self . context = zmq_context self . socket = self . context . socket ( zmq . REQ ) self . socket . setsockopt ( zmq . LINGER , 0 ) self . socket . setsockopt ( zmq . IDENTITY , service_id . encode ( \"utf-8\" )) self . socket . connect ( f \"tcp://localhost: { port } \" ) logger . info ( \"Connected to server\" ) send_request ( request ) Send request to the server and return the response. Parameters: Name Type Description Default request Dict [ str , Any ] The request to send to the server. required Returns: Type Description Any The result from the server. Source code in npc_engine\\service_clients\\service_client.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def send_request ( self , request : Dict [ str , Any ]) -> Any : \"\"\"Send request to the server and return the response. Args: request: The request to send to the server. Returns: The result from the server. \"\"\" logger . trace ( f \"Sending request: { request } \" ) self . socket . send_json ( request ) response = self . socket . recv_json () logger . trace ( f \"Received response: { response } \" ) if \"result\" in response : return response [ \"result\" ] elif \"code\" in response : raise RuntimeError ( f \"code: { response [ 'code' ] } . { response [ 'message' ] } \" ) similarity_client Huggingface chatbot interface client implementation. SimilarityClient Bases: ServiceClient Json rpc client for chatbot service. Source code in npc_engine\\service_clients\\similarity_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class SimilarityClient ( ServiceClient ): \"\"\"Json rpc client for chatbot service.\"\"\" def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str = \"SimilarityAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , service_id ) def compare ( self , query : str , context : List [ str ]) -> str : \"\"\"Send a chatbot request to the server. Args: context: A dictionary containing the chatbot request. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"compare\" , \"id\" : 0 , \"params\" : [ query , context ], } reply = self . send_request ( request ) return reply __init__ ( zmq_context , port , service_id = 'SimilarityAPI' ) Connect to the server on the port. Source code in npc_engine\\service_clients\\similarity_client.py 10 11 12 13 14 def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str = \"SimilarityAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , service_id ) compare ( query , context ) Send a chatbot request to the server. Parameters: Name Type Description Default context List [ str ] A dictionary containing the chatbot request. required Source code in npc_engine\\service_clients\\similarity_client.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def compare ( self , query : str , context : List [ str ]) -> str : \"\"\"Send a chatbot request to the server. Args: context: A dictionary containing the chatbot request. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"compare\" , \"id\" : 0 , \"params\" : [ query , context ], } reply = self . send_request ( request ) return reply","title":"API Classes"},{"location":"inference_engine/api_classes/#existing-apis","text":"These are the existing API classes and corresponding API_METHODS:","title":"Existing APIs"},{"location":"inference_engine/api_classes/#npc_engine.services.sequence_classifier.sequence_classifier_base.SequenceClassifierAPI","text":"Bases: BaseService Abstract base class for text classification models.","title":"SequenceClassifierAPI"},{"location":"inference_engine/api_classes/#npc_engine.services.sequence_classifier.sequence_classifier_base.SequenceClassifierAPI.__init__","text":"Empty initialization method for API to be similar to other model base classes.","title":"__init__()"},{"location":"inference_engine/api_classes/#npc_engine.services.sequence_classifier.sequence_classifier_base.SequenceClassifierAPI.classify","text":"Classify a list of texts. Parameters: Name Type Description Default texts List [ str ] A list of texts to classify. required Returns: Type Description List [ List [ float ]] List of scores for each text.","title":"classify()"},{"location":"inference_engine/api_classes/#npc_engine.services.sequence_classifier.sequence_classifier_base.SequenceClassifierAPI.compute_scores_batch","text":"Compute scores for a list of texts. Parameters: Name Type Description Default texts List [ str ] A list of texts to compute scores for. required Returns: Type Description np . ndarray List of scores for each text.","title":"compute_scores_batch()"},{"location":"inference_engine/api_classes/#npc_engine.services.sequence_classifier.sequence_classifier_base.SequenceClassifierAPI.get_api_name","text":"Get the API name.","title":"get_api_name()"},{"location":"inference_engine/api_classes/#npc_engine.services.chatbot.chatbot_base.ChatbotAPI","text":"Bases: BaseService Abstract base class for Chatbot models.","title":"ChatbotAPI"},{"location":"inference_engine/api_classes/#npc_engine.services.chatbot.chatbot_base.ChatbotAPI.__init__","text":"Initialize prompt formatting variables. Parameters: Name Type Description Default template_string str Template string to be rendered as prompt. required","title":"__init__()"},{"location":"inference_engine/api_classes/#npc_engine.services.chatbot.chatbot_base.ChatbotAPI.generate_reply","text":"Format the model prompt and generate response. Parameters: Name Type Description Default context Dict [ str , Any ] Prompt context. required Returns: Type Description str Text response to a prompt.","title":"generate_reply()"},{"location":"inference_engine/api_classes/#npc_engine.services.chatbot.chatbot_base.ChatbotAPI.get_api_name","text":"Get the API name.","title":"get_api_name()"},{"location":"inference_engine/api_classes/#npc_engine.services.chatbot.chatbot_base.ChatbotAPI.get_context_template","text":"Return context template. Returns: Type Description Dict [ str , Any ] Example context","title":"get_context_template()"},{"location":"inference_engine/api_classes/#npc_engine.services.chatbot.chatbot_base.ChatbotAPI.get_prompt_template","text":"Return prompt template string used to render model prompt. Returns: Type Description str A template string.","title":"get_prompt_template()"},{"location":"inference_engine/api_classes/#npc_engine.services.chatbot.chatbot_base.ChatbotAPI.get_special_tokens","text":"Return dictionary mapping for special tokens. To be implemented by child class. Can then be used in template string as fields Returns: Type Description Dict [ str , str ] Dictionary of special tokens","title":"get_special_tokens()"},{"location":"inference_engine/api_classes/#npc_engine.services.chatbot.chatbot_base.ChatbotAPI.run","text":"Abstract method for concrete implementation of generation. Parameters: Name Type Description Default prompt str Fromatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text","title":"run()"},{"location":"inference_engine/api_classes/#npc_engine.services.similarity.similarity_base.SimilarityAPI","text":"Bases: BaseService Abstract base class for text similarity models.","title":"SimilarityAPI"},{"location":"inference_engine/api_classes/#npc_engine.services.similarity.similarity_base.SimilarityAPI.__init__","text":"Empty initialization method for API to be similar to other model base classes.","title":"__init__()"},{"location":"inference_engine/api_classes/#npc_engine.services.similarity.similarity_base.SimilarityAPI.cache","text":"Cache embeddings of given sequences. Parameters: Name Type Description Default context List [ str ] A list of sentences to cache. required","title":"cache()"},{"location":"inference_engine/api_classes/#npc_engine.services.similarity.similarity_base.SimilarityAPI.compare","text":"Compare a query to the context. Parameters: Name Type Description Default query str A sentence to compare. required context List [ str ] A list of sentences to compare to. This will be cached if caching is enabled required Returns: Type Description List [ float ] List of similarities","title":"compare()"},{"location":"inference_engine/api_classes/#npc_engine.services.similarity.similarity_base.SimilarityAPI.compute_embedding","text":"Compute sentence embedding. Parameters: Name Type Description Default line str Sentence to embed required Returns: Type Description np . ndarray Embedding of shape (1, embedding_size)","title":"compute_embedding()"},{"location":"inference_engine/api_classes/#npc_engine.services.similarity.similarity_base.SimilarityAPI.compute_embedding_batch","text":"Compute line embeddings in batch. Parameters: Name Type Description Default lines List [ str ] List of sentences to embed required Returns: Type Description np . ndarray Embedding batch of shape (batch_size, embedding_size)","title":"compute_embedding_batch()"},{"location":"inference_engine/api_classes/#npc_engine.services.similarity.similarity_base.SimilarityAPI.get_api_name","text":"Get the API name.","title":"get_api_name()"},{"location":"inference_engine/api_classes/#npc_engine.services.similarity.similarity_base.SimilarityAPI.metric","text":"Compute distance between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Parameters: Name Type Description Default embedding_a np . ndarray Embedding of shape (1 or batch_size, embedding_size) required embedding_b np . ndarray Embedding of shape (1 or batch_size, embedding_size) required Returns: Type Description np . ndarray Vector of distances (batch_size or 1,)","title":"metric()"},{"location":"inference_engine/api_classes/#npc_engine.services.tts.tts_base.TextToSpeechAPI","text":"Bases: BaseService Abstract base class for text-to-speech models.","title":"TextToSpeechAPI"},{"location":"inference_engine/api_classes/#npc_engine.services.tts.tts_base.TextToSpeechAPI.__init__","text":"Empty initialization method for API to be similar to other model base classes.","title":"__init__()"},{"location":"inference_engine/api_classes/#npc_engine.services.tts.tts_base.TextToSpeechAPI._chain_run","text":"Chain the run method to be used in the generator.","title":"_chain_run()"},{"location":"inference_engine/api_classes/#npc_engine.services.tts.tts_base.TextToSpeechAPI.get_api_name","text":"Get the API name.","title":"get_api_name()"},{"location":"inference_engine/api_classes/#npc_engine.services.tts.tts_base.TextToSpeechAPI.get_speaker_ids","text":"Get list of available speaker ids. Returns: Type Description List [ str ] The return value. True for success, False otherwise.","title":"get_speaker_ids()"},{"location":"inference_engine/api_classes/#npc_engine.services.tts.tts_base.TextToSpeechAPI.run","text":"Create a generator for iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Returns: Type Description Iterable [ np . ndarray ] Generator that yields next chunk of speech in the form of f32 ndarray.","title":"run()"},{"location":"inference_engine/api_classes/#npc_engine.services.tts.tts_base.TextToSpeechAPI.tts_get_results","text":"Retrieve the next chunk of generated speech. Returns: Type Description Iterable [ np . ndarray ] Next chunk of speech in the form of f32 ndarray.","title":"tts_get_results()"},{"location":"inference_engine/api_classes/#npc_engine.services.tts.tts_base.TextToSpeechAPI.tts_start","text":"Initiate iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required","title":"tts_start()"},{"location":"inference_engine/api_classes/#creating-new-apis","text":"You can use this dummy API example to create your own: from npc_engine.services.base_service import BaseService class EchoAPI(BaseService): API_METHODS: List[str] = [\"echo\"] def __init__(self, *args, **kwargs): pass def echo(self, text): return text Dont forget Import new API to npc-engine.services so that it is discovered. Models that are implemented for the API should appear there too.","title":"Creating new APIs"},{"location":"inference_engine/api_classes/#calling-other-apis","text":"There are a set of service clients that can be used to call other APIs that are running on the server. They will block other services so they should be used carefully. Module implementing the clients for services.","title":"Calling other APIs"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.chatbot_client","text":"Huggingface chatbot interface client implementation.","title":"chatbot_client"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.chatbot_client.ChatbotClient","text":"Bases: ServiceClient Json rpc client for chatbot service. Source code in npc_engine\\service_clients\\chatbot_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class ChatbotClient ( ServiceClient ): \"\"\"Json rpc client for chatbot service.\"\"\" def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str = \"ChatbotAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , service_id ) def generate_reply ( self , context : Dict [ str , Any ]) -> str : \"\"\"Send a chatbot request to the server. Args: context: A dictionary containing the chatbot request. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"generate_reply\" , \"id\" : 0 , \"params\" : [ context ], } reply = self . send_request ( request ) return reply def get_prompt_template ( self ) -> str : \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_prompt_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) def get_context_template ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_context_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) def get_special_tokens ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_special_tokens\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request )","title":"ChatbotClient"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.chatbot_client.ChatbotClient.__init__","text":"Connect to the server on the port. Source code in npc_engine\\service_clients\\chatbot_client.py 10 11 12 13 14 def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str = \"ChatbotAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , service_id )","title":"__init__()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.chatbot_client.ChatbotClient.generate_reply","text":"Send a chatbot request to the server. Parameters: Name Type Description Default context Dict [ str , Any ] A dictionary containing the chatbot request. required Source code in npc_engine\\service_clients\\chatbot_client.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def generate_reply ( self , context : Dict [ str , Any ]) -> str : \"\"\"Send a chatbot request to the server. Args: context: A dictionary containing the chatbot request. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"generate_reply\" , \"id\" : 0 , \"params\" : [ context ], } reply = self . send_request ( request ) return reply","title":"generate_reply()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.chatbot_client.ChatbotClient.get_context_template","text":"Send a chatbot request to the server. Source code in npc_engine\\service_clients\\chatbot_client.py 41 42 43 44 45 46 47 48 49 def get_context_template ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_context_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request )","title":"get_context_template()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.chatbot_client.ChatbotClient.get_prompt_template","text":"Send a chatbot request to the server. Source code in npc_engine\\service_clients\\chatbot_client.py 31 32 33 34 35 36 37 38 39 def get_prompt_template ( self ) -> str : \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_prompt_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request )","title":"get_prompt_template()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.chatbot_client.ChatbotClient.get_special_tokens","text":"Send a chatbot request to the server. Source code in npc_engine\\service_clients\\chatbot_client.py 51 52 53 54 55 56 57 58 59 def get_special_tokens ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_special_tokens\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request )","title":"get_special_tokens()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.control_client","text":"Control interface client implementation.","title":"control_client"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.control_client.ControlClient","text":"Bases: ServiceClient Json rpc client for control requests. Source code in npc_engine\\service_clients\\control_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 class ControlClient ( ServiceClient ): \"\"\"Json rpc client for control requests.\"\"\" def __init__ ( self , zmq_context : zmq . Context , port : str ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , \"control\" ) def start_service ( self , service_id ): \"\"\"Send a start service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"start_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) def stop_service ( self , service_id ): \"\"\"Send a stop service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"stop_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) def get_service_status ( self , service_id ) -> str : \"\"\"Send a get service status request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_service_status\" , \"id\" : 0 , \"params\" : [ service_id ], } return self . send_request ( request ) def restart_service ( self , service_id ): \"\"\"Send a restart service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"restart_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) def get_services_metadata ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Send a get services metadata request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_services_metadata\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request )","title":"ControlClient"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.control_client.ControlClient.__init__","text":"Connect to the server on the port. Source code in npc_engine\\service_clients\\control_client.py 10 11 12 def __init__ ( self , zmq_context : zmq . Context , port : str ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , \"control\" )","title":"__init__()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.control_client.ControlClient.get_service_status","text":"Send a get service status request to the server. Source code in npc_engine\\service_clients\\control_client.py 34 35 36 37 38 39 40 41 42 def get_service_status ( self , service_id ) -> str : \"\"\"Send a get service status request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_service_status\" , \"id\" : 0 , \"params\" : [ service_id ], } return self . send_request ( request )","title":"get_service_status()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.control_client.ControlClient.get_services_metadata","text":"Send a get services metadata request to the server. Source code in npc_engine\\service_clients\\control_client.py 54 55 56 57 58 59 60 61 62 def get_services_metadata ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Send a get services metadata request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_services_metadata\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request )","title":"get_services_metadata()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.control_client.ControlClient.restart_service","text":"Send a restart service request to the server. Source code in npc_engine\\service_clients\\control_client.py 44 45 46 47 48 49 50 51 52 def restart_service ( self , service_id ): \"\"\"Send a restart service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"restart_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request )","title":"restart_service()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.control_client.ControlClient.start_service","text":"Send a start service request to the server. Source code in npc_engine\\service_clients\\control_client.py 14 15 16 17 18 19 20 21 22 def start_service ( self , service_id ): \"\"\"Send a start service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"start_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request )","title":"start_service()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.control_client.ControlClient.stop_service","text":"Send a stop service request to the server. Source code in npc_engine\\service_clients\\control_client.py 24 25 26 27 28 29 30 31 32 def stop_service ( self , service_id ): \"\"\"Send a stop service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"stop_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request )","title":"stop_service()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.sequence_classifier_client","text":"Huggingface chatbot interface client implementation.","title":"sequence_classifier_client"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.sequence_classifier_client.SequenceClassifierClient","text":"Bases: ServiceClient Json rpc client for chatbot service. Source code in npc_engine\\service_clients\\sequence_classifier_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class SequenceClassifierClient ( ServiceClient ): \"\"\"Json rpc client for chatbot service.\"\"\" def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str = \"SequenceClassifierAPI\" , ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , service_id ) def classify ( self , texts : List [ Union [ str , Tuple [ str , str ]]]) -> str : \"\"\"Send a classify request to the SequenceClassifierAPI. Args: texts: List of texts to classify. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"classify\" , \"id\" : 0 , \"params\" : [ texts ], } reply = self . send_request ( request ) return reply","title":"SequenceClassifierClient"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.sequence_classifier_client.SequenceClassifierClient.__init__","text":"Connect to the server on the port. Source code in npc_engine\\service_clients\\sequence_classifier_client.py 10 11 12 13 14 15 16 17 def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str = \"SequenceClassifierAPI\" , ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , service_id )","title":"__init__()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.sequence_classifier_client.SequenceClassifierClient.classify","text":"Send a classify request to the SequenceClassifierAPI. Parameters: Name Type Description Default texts List [ Union [ str , Tuple [ str , str ]]] List of texts to classify. required Source code in npc_engine\\service_clients\\sequence_classifier_client.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def classify ( self , texts : List [ Union [ str , Tuple [ str , str ]]]) -> str : \"\"\"Send a classify request to the SequenceClassifierAPI. Args: texts: List of texts to classify. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"classify\" , \"id\" : 0 , \"params\" : [ texts ], } reply = self . send_request ( request ) return reply","title":"classify()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.service_client","text":"Module that implements ZMQ base client communication over JSON-RPC 2.0 (https://www.jsonrpc.org/specification).","title":"service_client"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.service_client.ServiceClient","text":"Base json rpc client. Source code in npc_engine\\service_clients\\service_client.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class ServiceClient : \"\"\"Base json rpc client.\"\"\" def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str ): \"\"\"Connect to the server on the port.\"\"\" self . context = zmq_context self . socket = self . context . socket ( zmq . REQ ) self . socket . setsockopt ( zmq . LINGER , 0 ) self . socket . setsockopt ( zmq . IDENTITY , service_id . encode ( \"utf-8\" )) self . socket . connect ( f \"tcp://localhost: { port } \" ) logger . info ( \"Connected to server\" ) def send_request ( self , request : Dict [ str , Any ]) -> Any : \"\"\"Send request to the server and return the response. Args: request: The request to send to the server. Returns: The result from the server. \"\"\" logger . trace ( f \"Sending request: { request } \" ) self . socket . send_json ( request ) response = self . socket . recv_json () logger . trace ( f \"Received response: { response } \" ) if \"result\" in response : return response [ \"result\" ] elif \"code\" in response : raise RuntimeError ( f \"code: { response [ 'code' ] } . { response [ 'message' ] } \" )","title":"ServiceClient"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.service_client.ServiceClient.__init__","text":"Connect to the server on the port. Source code in npc_engine\\service_clients\\service_client.py 12 13 14 15 16 17 18 19 def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str ): \"\"\"Connect to the server on the port.\"\"\" self . context = zmq_context self . socket = self . context . socket ( zmq . REQ ) self . socket . setsockopt ( zmq . LINGER , 0 ) self . socket . setsockopt ( zmq . IDENTITY , service_id . encode ( \"utf-8\" )) self . socket . connect ( f \"tcp://localhost: { port } \" ) logger . info ( \"Connected to server\" )","title":"__init__()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.service_client.ServiceClient.send_request","text":"Send request to the server and return the response. Parameters: Name Type Description Default request Dict [ str , Any ] The request to send to the server. required Returns: Type Description Any The result from the server. Source code in npc_engine\\service_clients\\service_client.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def send_request ( self , request : Dict [ str , Any ]) -> Any : \"\"\"Send request to the server and return the response. Args: request: The request to send to the server. Returns: The result from the server. \"\"\" logger . trace ( f \"Sending request: { request } \" ) self . socket . send_json ( request ) response = self . socket . recv_json () logger . trace ( f \"Received response: { response } \" ) if \"result\" in response : return response [ \"result\" ] elif \"code\" in response : raise RuntimeError ( f \"code: { response [ 'code' ] } . { response [ 'message' ] } \" )","title":"send_request()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.similarity_client","text":"Huggingface chatbot interface client implementation.","title":"similarity_client"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.similarity_client.SimilarityClient","text":"Bases: ServiceClient Json rpc client for chatbot service. Source code in npc_engine\\service_clients\\similarity_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class SimilarityClient ( ServiceClient ): \"\"\"Json rpc client for chatbot service.\"\"\" def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str = \"SimilarityAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , service_id ) def compare ( self , query : str , context : List [ str ]) -> str : \"\"\"Send a chatbot request to the server. Args: context: A dictionary containing the chatbot request. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"compare\" , \"id\" : 0 , \"params\" : [ query , context ], } reply = self . send_request ( request ) return reply","title":"SimilarityClient"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.similarity_client.SimilarityClient.__init__","text":"Connect to the server on the port. Source code in npc_engine\\service_clients\\similarity_client.py 10 11 12 13 14 def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str = \"SimilarityAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , service_id )","title":"__init__()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.similarity_client.SimilarityClient.compare","text":"Send a chatbot request to the server. Parameters: Name Type Description Default context List [ str ] A dictionary containing the chatbot request. required Source code in npc_engine\\service_clients\\similarity_client.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def compare ( self , query : str , context : List [ str ]) -> str : \"\"\"Send a chatbot request to the server. Args: context: A dictionary containing the chatbot request. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"compare\" , \"id\" : 0 , \"params\" : [ query , context ], } reply = self . send_request ( request ) return reply","title":"compare()"},{"location":"inference_engine/benchmarks/","text":"","title":"Benchmarks"},{"location":"inference_engine/building/","text":"Build on Windows Create virtualenv and activate it python3 -m venv npc-engine-venv .\\npc-engine-venv\\activate.bat Install dependencies pip install -e .[dev,dml] (Optional) Compile, build and install your custom ONNX python runtime\" Build instructions can be found here (Optional) Run tests tox -e py38 Compile to exe with pyinstaller --additional-hooks-dir hooks --exclude-module matplotlib --exclude-module jupyter --exclude-module torch --exclude-module torchvision .\\npc-engine\\cli.py --onedir","title":"Build From Source"},{"location":"inference_engine/building/#build-on-windows","text":"","title":"Build on Windows"},{"location":"inference_engine/building/#create-virtualenv-and-activate-it","text":"python3 -m venv npc-engine-venv .\\npc-engine-venv\\activate.bat","title":"Create virtualenv and activate it"},{"location":"inference_engine/building/#install-dependencies","text":"pip install -e .[dev,dml]","title":"Install dependencies"},{"location":"inference_engine/building/#optional-compile-build-and-install-your-custom-onnx-python-runtime","text":"Build instructions can be found here","title":"(Optional) Compile, build and install your custom ONNX python runtime\""},{"location":"inference_engine/building/#optional-run-tests","text":"tox -e py38","title":"(Optional) Run tests"},{"location":"inference_engine/building/#compile-to-exe-with","text":"pyinstaller --additional-hooks-dir hooks --exclude-module matplotlib --exclude-module jupyter --exclude-module torch --exclude-module torchvision .\\npc-engine\\cli.py --onedir","title":"Compile to exe with"},{"location":"inference_engine/exporting_models/","text":"It's possible to export models from popular libraries to npc-engine. Currently, npc-engine supports export from the following libraries/architectures: HuggingFace transformers Sequence classification Cosine similarity models Text generation To use this functionality you need to install a special extra called export pip install npc-engine[export] To export model you need to use export-model command. With Huggingface models you can use Huggingface Hub model id as well as path to the model. npc-engine export-model --models-path <path-to-models-folder> <model_id or folder> This will prompt you to select correct Exporter class and take you through filling in the required parameters. Create youre own exporter To define your own exporter you need to create child class of Exporter and import it into npc_engine.exporters module so that it's discovered by the CLI. This base class has five abstract methods to implement that cover all the functionality of the exporter.","title":"Exporting models"},{"location":"inference_engine/exporting_models/#create-youre-own-exporter","text":"To define your own exporter you need to create child class of Exporter and import it into npc_engine.exporters module so that it's discovered by the CLI. This base class has five abstract methods to implement that cover all the functionality of the exporter.","title":"Create youre own exporter"},{"location":"inference_engine/models/","text":"Service classes are specific implementations of API classes. They define the loading process in __init__ function and all the abstract methods required by the API class to function. How models are loaded? ServiceManager Scans the models folder. For each discovered subfolder ServiceManager validates the config.yml and creates descriptors with metadata for each service. The mandatory field of config.yml is type (or model_type ) that must contain correct service class that was discovered and registered by BaseService parent class. This service class will be instantiated with parsed dictionary as parameters on ServiceManager.start_service request to control service. How is their API exposed? When service is started ServiceManager starts a new process with BaseService message handling loop. BaseService handles ZMQ IPC requests to it's exposed functions from API's API_METHODS class variable to the main process, while main process handles routing requests to this service. Existing service classes npc_engine.services.sequence_classifier.hf_classifier.HfClassifier Bases: SequenceClassifierAPI Huggingface transformers sequence classification. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with sequence-classification feature. Also requires saved tokenizer with huggingface tokenizers. __init__ ( model_path , * args , ** kwargs ) Create and load biencoder model for semantic similarity. Parameters: Name Type Description Default model_path str A path where model config and weights are. required compute_scores_batch ( texts ) Compute scores. Parameters: Name Type Description Default texts List [ Union [ str , Tuple [ str , str ]]] Sentence to embed required Returns: Name Type Description scores np . ndarray Scores for each text npc_engine.services.chatbot.hf_chatbot.HfChatbot Bases: ChatbotAPI Chatbot that uses Huggingface transformer architectures. ONNX export of Huggingface transformer is required (see https://huggingface.co/docs/transformers/serialization). Features seq2seq-lm, causal-lm, seq2seq-lm-with-past, causal-lm-with-past are supported __init__ ( model_path , max_length = 100 , min_length = 2 , repetition_penalty = 1 , * args , ** kwargs ) Create the chatbot from config args and kwargs. Parameters: Name Type Description Default model_path path to scan for model files (weights and configs) required max_length stop generation at this number of tokens 100 min_length model can't stop generating text before it's atleast this long in tokens 2 repetition_penalty probability coef for same tokens to appear multiple times 1 create_starter_inputs ( prompt = '' ) Create starter inputs for the model. Parameters: Name Type Description Default prompt str Prompt to start generation from. '' Returns: Type Description Dict [ str , Any ] Dict of inputs to the model decode_logit ( logit , temperature , topk ) Decode logit to token. Parameters: Name Type Description Default logit np . ndarray Logit to decode of shape (vocab_size,) required Returns: Type Description int Decoded token of shape get_special_tokens () Retrun dict of special tokens to be renderable from template. run ( prompt , temperature = 1.0 , topk = None ) Run text generation from given prompt and parameters. Parameters: Name Type Description Default prompt str Fromatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1.0 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text update_inputs_with_results ( inputs , results , decoded_token ) Update inputs with results from model. Parameters: Name Type Description Default inputs Dict [ str , np . ndarray ] Inputs to the model required results Dict [ str , np . ndarray ] Results from the model required Returns: Type Description Dict [ str , np . ndarray ] Updated inputs Dict [ str , np . ndarray ] Finished generation npc_engine.services.chatbot.bart.BartChatbot Bases: ChatbotAPI BART based chatbot implementation class. This model class requires two ONNX models encoder_bart.onnx and decoder_bart.onnx that correspond to encoder and decoder from transformers EncoderDecoderModel and a tokenizer.json with huggingface tokenizers definition. encoder_bart.onnx spec: - inputs: `input_ids` - outputs: `encoder_hidden_state` decoder_bart.onnx spec: - inputs: `encoder_hidden_state` `decoder_input_ids` - outputs: `logits` __init__ ( model_path , max_steps = 100 , min_length = 2 , repetition_penalty = 1 , bos_token_id = 0 , eos_token_id = 2 , pad_token_id = 1 , sep_token_id = None , * args , ** kwargs ) Create the chatbot from config args and kwargs. Parameters: Name Type Description Default model_path path to scan for model files (weights and configs) required max_steps stop generation at this number of tokens 100 min_length model can't stop generating text before it's atleast this long in tokens 2 repetition_penalty probability coef for same tokens to appear multiple times 1 bos_token_id beginning of sequence token id 0 eos_token_id end of sequence token id 2 pad_token_id padding token id 1 sep_token_id token id for separating sequence into multiple parts None get_special_tokens () Retrun dict of special tokens to be renderable from template. run ( prompt , temperature = 1.0 , topk = None ) Run text generation from given prompt and parameters. Parameters: Name Type Description Default prompt str Fromatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1.0 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text npc_engine.services.similarity.similarity_transformers.TransformerSemanticSimilarity Bases: SimilarityAPI Huggingface transformers semantic similarity. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with biencoder architecture. Also requires a tokenizer.json with huggingface tokenizers definition. model.onnx spec: - inputs: `input_ids` of shape `(batch_size, sequence)` `attention_mask` of shape `(batch_size, sequence)` (Optional) `input_type_ids` of shape `(batch_size, sequence)` - outputs: `token_embeddings` of shape `(batch_size, sequence, hidden_size)` __init__ ( model_path , metric = 'dot' , * args , ** kwargs ) Create and load biencoder model for semantic similarity. Parameters: Name Type Description Default model_path str A path where model config and weights are required metric str distance to compute semantic similarity 'dot' compute_embedding ( line ) Compute sentence embedding. Parameters: Name Type Description Default line str Sentence to embed required Returns: Type Description np . ndarray Embedding of shape (1, embedding_size) compute_embedding_batch ( lines ) Compute line embeddings in batch. Parameters: Name Type Description Default lines List [ str ] List of sentences to embed required Returns: Type Description np . ndarray Embedding batch of shape (batch_size, embedding_size) metric ( embedding_a , embedding_b ) Similarity between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Parameters: Name Type Description Default embedding_a np . ndarray Embedding of shape (1 or batch_size, embedding_size) required embedding_b np . ndarray Embedding of shape (1 or batch_size, embedding_size) required Returns: Type Description np . ndarray Vector of distances (batch_size or 1,) npc_engine.services.tts.flowtron.FlowtronTTS Bases: TextToSpeechAPI Implements Flowtron architecture inference. Paper: arXiv:2005.05957 Code: https://github.com/NVIDIA/flowtron Onnx export script can be found in this fork https://github.com/npc-engine/flowtron. This model class requires four ONNX models encoder.onnx , backward_flow.onnx , forward_flow.onnx and vocoder.onnx where first three are layers from Flowtron architecture ( flow corresponding to one direction pass of affine coupling layers) and vocoder.onnx is neural vocoder. For detailed specs refer to https://github.com/npc-engine/flowtron. __init__ ( model_path , max_frames = 400 , gate_threshold = 0.5 , sigma = 0.8 , smoothing_window = 3 , smoothing_weight = 0.5 , * args , ** kwargs ) Create and load Flowtron and vocoder models. get_speaker_ids () Return available ids of different speakers. run ( speaker_id , text , n_chunks ) Create a generator for iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Returns: Type Description Iterator [ np . ndarray ] Generator that yields next chunk of speech in the form of f32 ndarray. Default Models Fantasy Chatbot BartChatbot trained on LIGHT Dataset . Model consumes both self, other personas and location dialogue is happening in. Semantic Similarity sentence-transformers/all-MiniLM-L6-v2 Onnx export of sentence-transformers/all-MiniLM-L6-v2 . FlowtronTTS with Waveglow vocoder Nvidia's FlowtronTTS architecture using Waveglow vocoder. Weights were published by the authors, this model uses Flowtron LibriTTS2K version. Speech to text NeMo models This model is still heavy WIP it is best to use your platform's of choice existing solutions e.g. UnityEngine.Windows.Speech.DictationRecognizer in Unity. This implementation uses several models exported from NeMo toolkit: QuartzNet15x5 for transcription. Punctuation BERT for applying punctuation. Custom transformer for recognizing end of response to the context initialized from all-MiniLM-L6-v2 Creating new Services You can use this dummy model example to create your own: from npc_engine.services.chatbot.chatbot_base import ChatbotAPI class EchoService(ChatbotAPI): def __init__(self, model_path:str, *args, **kwargs): print(\"model is in {model_path}\") def get_special_tokens(self): return {} def run(self, prompt, temperature=1, topk=None): return prompt Dont forget Import new model to npc-engine.services so that it is discovered.","title":"Models"},{"location":"inference_engine/models/#how-models-are-loaded","text":"ServiceManager Scans the models folder. For each discovered subfolder ServiceManager validates the config.yml and creates descriptors with metadata for each service. The mandatory field of config.yml is type (or model_type ) that must contain correct service class that was discovered and registered by BaseService parent class. This service class will be instantiated with parsed dictionary as parameters on ServiceManager.start_service request to control service.","title":"How models are loaded?"},{"location":"inference_engine/models/#how-is-their-api-exposed","text":"When service is started ServiceManager starts a new process with BaseService message handling loop. BaseService handles ZMQ IPC requests to it's exposed functions from API's API_METHODS class variable to the main process, while main process handles routing requests to this service.","title":"How is their API exposed?"},{"location":"inference_engine/models/#existing-service-classes","text":"","title":"Existing service classes"},{"location":"inference_engine/models/#npc_engine.services.sequence_classifier.hf_classifier.HfClassifier","text":"Bases: SequenceClassifierAPI Huggingface transformers sequence classification. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with sequence-classification feature. Also requires saved tokenizer with huggingface tokenizers.","title":"HfClassifier"},{"location":"inference_engine/models/#npc_engine.services.sequence_classifier.hf_classifier.HfClassifier.__init__","text":"Create and load biencoder model for semantic similarity. Parameters: Name Type Description Default model_path str A path where model config and weights are. required","title":"__init__()"},{"location":"inference_engine/models/#npc_engine.services.sequence_classifier.hf_classifier.HfClassifier.compute_scores_batch","text":"Compute scores. Parameters: Name Type Description Default texts List [ Union [ str , Tuple [ str , str ]]] Sentence to embed required Returns: Name Type Description scores np . ndarray Scores for each text","title":"compute_scores_batch()"},{"location":"inference_engine/models/#npc_engine.services.chatbot.hf_chatbot.HfChatbot","text":"Bases: ChatbotAPI Chatbot that uses Huggingface transformer architectures. ONNX export of Huggingface transformer is required (see https://huggingface.co/docs/transformers/serialization). Features seq2seq-lm, causal-lm, seq2seq-lm-with-past, causal-lm-with-past are supported","title":"HfChatbot"},{"location":"inference_engine/models/#npc_engine.services.chatbot.hf_chatbot.HfChatbot.__init__","text":"Create the chatbot from config args and kwargs. Parameters: Name Type Description Default model_path path to scan for model files (weights and configs) required max_length stop generation at this number of tokens 100 min_length model can't stop generating text before it's atleast this long in tokens 2 repetition_penalty probability coef for same tokens to appear multiple times 1","title":"__init__()"},{"location":"inference_engine/models/#npc_engine.services.chatbot.hf_chatbot.HfChatbot.create_starter_inputs","text":"Create starter inputs for the model. Parameters: Name Type Description Default prompt str Prompt to start generation from. '' Returns: Type Description Dict [ str , Any ] Dict of inputs to the model","title":"create_starter_inputs()"},{"location":"inference_engine/models/#npc_engine.services.chatbot.hf_chatbot.HfChatbot.decode_logit","text":"Decode logit to token. Parameters: Name Type Description Default logit np . ndarray Logit to decode of shape (vocab_size,) required Returns: Type Description int Decoded token of shape","title":"decode_logit()"},{"location":"inference_engine/models/#npc_engine.services.chatbot.hf_chatbot.HfChatbot.get_special_tokens","text":"Retrun dict of special tokens to be renderable from template.","title":"get_special_tokens()"},{"location":"inference_engine/models/#npc_engine.services.chatbot.hf_chatbot.HfChatbot.run","text":"Run text generation from given prompt and parameters. Parameters: Name Type Description Default prompt str Fromatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1.0 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text","title":"run()"},{"location":"inference_engine/models/#npc_engine.services.chatbot.hf_chatbot.HfChatbot.update_inputs_with_results","text":"Update inputs with results from model. Parameters: Name Type Description Default inputs Dict [ str , np . ndarray ] Inputs to the model required results Dict [ str , np . ndarray ] Results from the model required Returns: Type Description Dict [ str , np . ndarray ] Updated inputs Dict [ str , np . ndarray ] Finished generation","title":"update_inputs_with_results()"},{"location":"inference_engine/models/#npc_engine.services.chatbot.bart.BartChatbot","text":"Bases: ChatbotAPI BART based chatbot implementation class. This model class requires two ONNX models encoder_bart.onnx and decoder_bart.onnx that correspond to encoder and decoder from transformers EncoderDecoderModel and a tokenizer.json with huggingface tokenizers definition. encoder_bart.onnx spec: - inputs: `input_ids` - outputs: `encoder_hidden_state` decoder_bart.onnx spec: - inputs: `encoder_hidden_state` `decoder_input_ids` - outputs: `logits`","title":"BartChatbot"},{"location":"inference_engine/models/#npc_engine.services.chatbot.bart.BartChatbot.__init__","text":"Create the chatbot from config args and kwargs. Parameters: Name Type Description Default model_path path to scan for model files (weights and configs) required max_steps stop generation at this number of tokens 100 min_length model can't stop generating text before it's atleast this long in tokens 2 repetition_penalty probability coef for same tokens to appear multiple times 1 bos_token_id beginning of sequence token id 0 eos_token_id end of sequence token id 2 pad_token_id padding token id 1 sep_token_id token id for separating sequence into multiple parts None","title":"__init__()"},{"location":"inference_engine/models/#npc_engine.services.chatbot.bart.BartChatbot.get_special_tokens","text":"Retrun dict of special tokens to be renderable from template.","title":"get_special_tokens()"},{"location":"inference_engine/models/#npc_engine.services.chatbot.bart.BartChatbot.run","text":"Run text generation from given prompt and parameters. Parameters: Name Type Description Default prompt str Fromatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1.0 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text","title":"run()"},{"location":"inference_engine/models/#npc_engine.services.similarity.similarity_transformers.TransformerSemanticSimilarity","text":"Bases: SimilarityAPI Huggingface transformers semantic similarity. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with biencoder architecture. Also requires a tokenizer.json with huggingface tokenizers definition. model.onnx spec: - inputs: `input_ids` of shape `(batch_size, sequence)` `attention_mask` of shape `(batch_size, sequence)` (Optional) `input_type_ids` of shape `(batch_size, sequence)` - outputs: `token_embeddings` of shape `(batch_size, sequence, hidden_size)`","title":"TransformerSemanticSimilarity"},{"location":"inference_engine/models/#npc_engine.services.similarity.similarity_transformers.TransformerSemanticSimilarity.__init__","text":"Create and load biencoder model for semantic similarity. Parameters: Name Type Description Default model_path str A path where model config and weights are required metric str distance to compute semantic similarity 'dot'","title":"__init__()"},{"location":"inference_engine/models/#npc_engine.services.similarity.similarity_transformers.TransformerSemanticSimilarity.compute_embedding","text":"Compute sentence embedding. Parameters: Name Type Description Default line str Sentence to embed required Returns: Type Description np . ndarray Embedding of shape (1, embedding_size)","title":"compute_embedding()"},{"location":"inference_engine/models/#npc_engine.services.similarity.similarity_transformers.TransformerSemanticSimilarity.compute_embedding_batch","text":"Compute line embeddings in batch. Parameters: Name Type Description Default lines List [ str ] List of sentences to embed required Returns: Type Description np . ndarray Embedding batch of shape (batch_size, embedding_size)","title":"compute_embedding_batch()"},{"location":"inference_engine/models/#npc_engine.services.similarity.similarity_transformers.TransformerSemanticSimilarity.metric","text":"Similarity between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Parameters: Name Type Description Default embedding_a np . ndarray Embedding of shape (1 or batch_size, embedding_size) required embedding_b np . ndarray Embedding of shape (1 or batch_size, embedding_size) required Returns: Type Description np . ndarray Vector of distances (batch_size or 1,)","title":"metric()"},{"location":"inference_engine/models/#npc_engine.services.tts.flowtron.FlowtronTTS","text":"Bases: TextToSpeechAPI Implements Flowtron architecture inference. Paper: arXiv:2005.05957 Code: https://github.com/NVIDIA/flowtron Onnx export script can be found in this fork https://github.com/npc-engine/flowtron. This model class requires four ONNX models encoder.onnx , backward_flow.onnx , forward_flow.onnx and vocoder.onnx where first three are layers from Flowtron architecture ( flow corresponding to one direction pass of affine coupling layers) and vocoder.onnx is neural vocoder. For detailed specs refer to https://github.com/npc-engine/flowtron.","title":"FlowtronTTS"},{"location":"inference_engine/models/#npc_engine.services.tts.flowtron.FlowtronTTS.__init__","text":"Create and load Flowtron and vocoder models.","title":"__init__()"},{"location":"inference_engine/models/#npc_engine.services.tts.flowtron.FlowtronTTS.get_speaker_ids","text":"Return available ids of different speakers.","title":"get_speaker_ids()"},{"location":"inference_engine/models/#npc_engine.services.tts.flowtron.FlowtronTTS.run","text":"Create a generator for iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Returns: Type Description Iterator [ np . ndarray ] Generator that yields next chunk of speech in the form of f32 ndarray.","title":"run()"},{"location":"inference_engine/models/#default-models","text":"","title":"Default Models"},{"location":"inference_engine/models/#fantasy-chatbot","text":"BartChatbot trained on LIGHT Dataset . Model consumes both self, other personas and location dialogue is happening in.","title":"Fantasy Chatbot"},{"location":"inference_engine/models/#semantic-similarity-sentence-transformersall-minilm-l6-v2","text":"Onnx export of sentence-transformers/all-MiniLM-L6-v2 .","title":"Semantic Similarity sentence-transformers/all-MiniLM-L6-v2"},{"location":"inference_engine/models/#flowtrontts-with-waveglow-vocoder","text":"Nvidia's FlowtronTTS architecture using Waveglow vocoder. Weights were published by the authors, this model uses Flowtron LibriTTS2K version.","title":"FlowtronTTS with Waveglow vocoder"},{"location":"inference_engine/models/#speech-to-text-nemo-models","text":"This model is still heavy WIP it is best to use your platform's of choice existing solutions e.g. UnityEngine.Windows.Speech.DictationRecognizer in Unity. This implementation uses several models exported from NeMo toolkit: QuartzNet15x5 for transcription. Punctuation BERT for applying punctuation. Custom transformer for recognizing end of response to the context initialized from all-MiniLM-L6-v2","title":"Speech to text NeMo models"},{"location":"inference_engine/models/#creating-new-services","text":"You can use this dummy model example to create your own: from npc_engine.services.chatbot.chatbot_base import ChatbotAPI class EchoService(ChatbotAPI): def __init__(self, model_path:str, *args, **kwargs): print(\"model is in {model_path}\") def get_special_tokens(self): return {} def run(self, prompt, temperature=1, topk=None): return prompt Dont forget Import new model to npc-engine.services so that it is discovered.","title":"Creating new Services"},{"location":"inference_engine/overview/","text":"NPC Engine is a local python JSON-RPC server. It is using 0MQ TCP sockets and it can used with any programming language that has 0MQ bindings. The main goal of this server is to provide access to inference services that handle various functionalities required for storytelling and in-game AI. How does it work Each service is defined in it's own folder in provided --models-path . Service's folder name becomes it's unique Id. On start NPC Engine will expose a special service called control that allows you to get services metadata and to control their lifetime. Services are running in separate processes but handle requests sequentially (including situations when services call each-other). To route requests NPC Engine uses 0MQ identity . It can be defined only before connecting, therefore each connected socket will be refering to it's own service. Service resolution rules are simple: - If no identity provided, request is routed to the first service that implements the method called. - If identity is API name, request is routed to the first service that implements the API. - Same logic in case identity is a service class name. - Identity could be a service ID (folder name), then request is routed to the service with that ID. Service APIs are defined in API classes . Service exposes methods that are listed in API_METHODS class variable. You can find a description of default services available in Default Models section. The specifics of how model is loaded and how inference is done is defined in specific service classes Here is an example of JSON RPC request: request with identity \"some_model\" { \"method\": \"do_smth\", \"params\": [\"hello\"], \"jsonrpc\": \"2.0\", \"id\": 0 } will result in call to some_model.do_smth('hello') on the server, or will fail if the service wasn't started. Creating an integration A checklist for a new integration would be to: Create a class that manages npc-engine subprocess (starts, terminates, checks if it's alive). Create a connection class that talks to npc-engine. Review API classes and wrap JSON-RPC requests into the native functions.","title":"Overview"},{"location":"inference_engine/overview/#how-does-it-work","text":"Each service is defined in it's own folder in provided --models-path . Service's folder name becomes it's unique Id. On start NPC Engine will expose a special service called control that allows you to get services metadata and to control their lifetime. Services are running in separate processes but handle requests sequentially (including situations when services call each-other). To route requests NPC Engine uses 0MQ identity . It can be defined only before connecting, therefore each connected socket will be refering to it's own service. Service resolution rules are simple: - If no identity provided, request is routed to the first service that implements the method called. - If identity is API name, request is routed to the first service that implements the API. - Same logic in case identity is a service class name. - Identity could be a service ID (folder name), then request is routed to the service with that ID. Service APIs are defined in API classes . Service exposes methods that are listed in API_METHODS class variable. You can find a description of default services available in Default Models section. The specifics of how model is loaded and how inference is done is defined in specific service classes Here is an example of JSON RPC request: request with identity \"some_model\" { \"method\": \"do_smth\", \"params\": [\"hello\"], \"jsonrpc\": \"2.0\", \"id\": 0 } will result in call to some_model.do_smth('hello') on the server, or will fail if the service wasn't started.","title":"How does it work"},{"location":"inference_engine/overview/#creating-an-integration","text":"A checklist for a new integration would be to: Create a class that manages npc-engine subprocess (starts, terminates, checks if it's alive). Create a connection class that talks to npc-engine. Review API classes and wrap JSON-RPC requests into the native functions.","title":"Creating an integration"},{"location":"inference_engine/reference/","text":"Server for providing onnx runtime predictions for text generation and speech synthesis. Uses 0MQ REP/REQ sockets with JSONRPC 2.0 protocol. cli This is the entry point for the command-line interface that starts npc-engine server. cli ( verbose ) NPC engine JSON RPC server CLI. Source code in npc_engine\\cli.py 26 27 28 29 30 31 32 33 34 35 36 37 38 @click . group () @click . option ( \"--verbose/--silent\" , \"-v\" , default = False , help = \"Enable verbose output.\" ) def cli ( verbose : bool ): \"\"\"NPC engine JSON RPC server CLI.\"\"\" # Use the verbosity count to determine the logging level... logger . remove () if verbose : logger . add ( sys . stdout , format = \" {time} {level} {message} \" , level = \"INFO\" , enqueue = True ) click . echo ( click . style ( \"Verbose logging is enabled. (LEVEL=INFO)\" , fg = \"yellow\" ,) ) download_default_models ( models_path ) Download default models into the folder. Source code in npc_engine\\cli.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 @cli . command () @click . option ( \"--models-path\" , default = os . environ . get ( \"NPC_ENGINE_MODELS_PATH\" , \"./models\" ), ) def download_default_models ( models_path : str ): \"\"\"Download default models into the folder.\"\"\" model_names = [ \"npc-engine/exported-paraphrase-MiniLM-L6-v2\" , \"npc-engine/exported-bart-light-gail-chatbot\" , \"npc-engine/exported-nemo-quartznet-ctc-stt\" , \"npc-engine/exported-flowtron-waveglow-librispeech-tts\" , ] for model in model_names : logger . info ( \"Downloading model {} \" , model ) logger . info ( \"Downloading {} \" , model ) snapshot_download ( repo_id = model , revision = \"main\" , cache_dir = models_path ) download_model ( models_path , model_id ) Download the model. Source code in npc_engine\\cli.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 @cli . command () @click . option ( \"--models-path\" , default = os . environ . get ( \"NPC_ENGINE_MODELS_PATH\" , \"./models\" ), ) @click . argument ( \"model_id\" ) def download_model ( models_path : str , model_id : str ): \"\"\"Download the model.\"\"\" model_correct = validate_hub_model ( model_id ) if model_correct : logger . info ( \"Downloading model {} \" , model_id ) snapshot_download ( repo_id = model_id , revision = \"main\" , cache_dir = models_path ) else : if click . confirm ( click . style ( f \" { model_id } is not a valid npc-engine model.\" + \" \\n Do you want to export it?\" , fg = \"yellow\" , ) ): export_model ( models_path , model_id , True ) export_model ( models_path , model_id , remove_source = False ) Export the model. Source code in npc_engine\\cli.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 @cli . command () @click . option ( \"--models-path\" , default = os . environ . get ( \"NPC_ENGINE_MODELS_PATH\" , \"./models\" ), ) @click . argument ( \"model_id\" ) def export_model ( models_path : str , model_id : str , remove_source : bool = False ): \"\"\"Export the model.\"\"\" from npc_engine.exporters.base_exporter import Exporter logger . info ( \"Downloading source model {} \" , model_id ) if os . path . exists ( model_id ): source_path = model_id else : source_path = snapshot_download ( repo_id = model_id , revision = \"main\" , cache_dir = models_path ) remove_source = True export_path = os . path . join ( models_path , \"exported-\" + model_id . replace ( \" \\\\ \" , \"/\" ) . split ( \"/\" )[ - 1 ] ) os . makedirs ( export_path , exist_ok = True ) logger . info ( \"Exporting model {} to {} \" , model_id , export_path ) exporters = Exporter . get_exporters () click . echo ( \"Available exporters:\" ) for i , exporter in enumerate ( exporters ): click . echo ( f \" { i + 1 } . { exporter . description () } \" ) exporter_id = click . prompt ( \"Please select an exporter\" , type = int ) exporter = exporters [ exporter_id - 1 ] exporter . export ( source_path , export_path ) exporter . create_config ( export_path ) if remove_source : shutil . rmtree ( source_path ) list_models ( models_path ) List the models in the folder. Source code in npc_engine\\cli.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 @cli . command () @click . option ( \"--models-path\" , default = os . environ . get ( \"NPC_ENGINE_MODELS_PATH\" , \"./models\" ), ) def list_models ( models_path : str ): \"\"\"List the models in the folder.\"\"\" from npc_engine.service_manager.service_manager import ServiceManager model_manager = ServiceManager ( models_path ) metadata_list = model_manager . get_services_metadata () for metadata in metadata_list : click . echo ( metadata [ \"id\" ]) click . echo ( metadata [ \"type\" ]) click . echo ( \"Service description:\" ) click . echo ( metadata [ \"service_short_description\" ]) click . echo ( \"Model description:\" ) click . echo ( metadata [ \"readme\" ]) click . echo ( \"--------------------\" ) run ( models_path , port , start_all ) Load the models and start JSONRPC server. Source code in npc_engine\\cli.py 41 42 43 44 45 46 47 48 49 50 51 52 53 @cli . command () @click . option ( \"--models-path\" , default = \"./models\" ) @click . option ( \"--port\" , default = \"5555\" ) @click . option ( \"--start-all/--dont-start\" , default = True ) def run ( models_path : str , port : str , start_all : bool ): \"\"\"Load the models and start JSONRPC server.\"\"\" from npc_engine.service_manager.service_manager import ServiceManager from npc_engine.service_manager.server import Server context = zmq . asyncio . Context ( io_threads = 5 ) model_manager = ServiceManager ( context , models_path ) server = Server ( context , model_manager , port , start_all ) server . run () set_models_path ( models_path ) Set the models path. Source code in npc_engine\\cli.py 74 75 76 77 78 @cli . command () @click . option ( \"--models-path\" , type = click . Path ( exists = True )) def set_models_path ( models_path : str ): \"\"\"Set the models path.\"\"\" os . environ [ \"NPC_ENGINE_MODELS_PATH\" ] = models_path test_model ( models_path , model_id ) Send test request to the model and print reply. Source code in npc_engine\\cli.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 @cli . command () @click . option ( \"--models-path\" , default = os . environ . get ( \"NPC_ENGINE_MODELS_PATH\" , \"./models\" ), ) @click . argument ( \"model_id\" ) def test_model ( models_path : str , model_id : str ): \"\"\"Send test request to the model and print reply.\"\"\" from npc_engine.exporters.base_exporter import Exporter if not validate_local_model ( models_path , model_id ): click . echo ( click . style ( f \" { ( model_id ) } is not a valid npc-engine model.\" , fg = \"red\" ,) ) return 1 model_type = get_model_type_name ( models_path , model_id ) exporters = Exporter . get_exporters () for exporter in exporters : if exporter . get_model_name () == model_type : exporter . test_model ( models_path , model_id ) return 0 version () Get the npc engine version. Source code in npc_engine\\cli.py 180 181 182 183 @cli . command () def version (): \"\"\"Get the npc engine version.\"\"\" click . echo ( click . style ( f \" { __version__ } \" , bold = True )) exporters base_exporter Module with Exporters base class. Exporter Bases: ABC Abstract base class for exporter. Exporters are classes that handle converting models to be used with npc-engine. Source code in npc_engine\\exporters\\base_exporter.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class Exporter ( ABC ): \"\"\"Abstract base class for exporter. Exporters are classes that handle converting models to be used with npc-engine. \"\"\" exporters = {} def __init_subclass__ ( cls , ** kwargs ): \"\"\"Init subclass where model classes get registered to be loadable.\"\"\" super () . __init_subclass__ ( ** kwargs ) cls . exporters [ cls . __name__ ] = cls @classmethod def get_exporters ( cls ) -> List [ Any ]: \"\"\"Create all exporters.\"\"\" return [ cls . exporters [ name ]() for name in cls . exporters if not inspect . isabstract ( cls . exporters [ name ]) ] @classmethod def description ( cls ) -> str : \"\"\"Print the exporter.\"\"\" return cls . __name__ + \" \\n\\t \" + cls . __doc__ . split ( \" \\n\\n \" )[ 0 ] @abstractmethod def export ( self , model_path : str , export_path : str ): \"\"\"Export the model to the export path.\"\"\" pass @abstractmethod def create_config ( self , export_path : str ): \"\"\"Create the config for the model.\"\"\" pass @classmethod @abstractmethod def get_api ( cls ) -> str : \"\"\"Get the api for the exporter.\"\"\" pass @classmethod @abstractmethod def get_model_name ( cls ) -> str : \"\"\"Get the model name.\"\"\" pass def test_model ( self , models_path : str , model_id : str ): \"\"\"Test the model.\"\"\" start_server = click . confirm ( \"Start testing server? (It should be already running otherwise)\" ) if start_server : start_test_server ( \"5555\" , models_path ) self . test_model_impl ( models_path , model_id ) @abstractmethod def test_model_impl ( self , models_path : str , model_id : str ): \"\"\"Test the model.\"\"\" pass __init_subclass__ ( ** kwargs ) Init subclass where model classes get registered to be loadable. Source code in npc_engine\\exporters\\base_exporter.py 17 18 19 20 def __init_subclass__ ( cls , ** kwargs ): \"\"\"Init subclass where model classes get registered to be loadable.\"\"\" super () . __init_subclass__ ( ** kwargs ) cls . exporters [ cls . __name__ ] = cls create_config ( export_path ) abstractmethod Create the config for the model. Source code in npc_engine\\exporters\\base_exporter.py 41 42 43 44 @abstractmethod def create_config ( self , export_path : str ): \"\"\"Create the config for the model.\"\"\" pass description () classmethod Print the exporter. Source code in npc_engine\\exporters\\base_exporter.py 31 32 33 34 @classmethod def description ( cls ) -> str : \"\"\"Print the exporter.\"\"\" return cls . __name__ + \" \\n\\t \" + cls . __doc__ . split ( \" \\n\\n \" )[ 0 ] export ( model_path , export_path ) abstractmethod Export the model to the export path. Source code in npc_engine\\exporters\\base_exporter.py 36 37 38 39 @abstractmethod def export ( self , model_path : str , export_path : str ): \"\"\"Export the model to the export path.\"\"\" pass get_api () abstractmethod classmethod Get the api for the exporter. Source code in npc_engine\\exporters\\base_exporter.py 46 47 48 49 50 @classmethod @abstractmethod def get_api ( cls ) -> str : \"\"\"Get the api for the exporter.\"\"\" pass get_exporters () classmethod Create all exporters. Source code in npc_engine\\exporters\\base_exporter.py 22 23 24 25 26 27 28 29 @classmethod def get_exporters ( cls ) -> List [ Any ]: \"\"\"Create all exporters.\"\"\" return [ cls . exporters [ name ]() for name in cls . exporters if not inspect . isabstract ( cls . exporters [ name ]) ] get_model_name () abstractmethod classmethod Get the model name. Source code in npc_engine\\exporters\\base_exporter.py 52 53 54 55 56 @classmethod @abstractmethod def get_model_name ( cls ) -> str : \"\"\"Get the model name.\"\"\" pass test_model ( models_path , model_id ) Test the model. Source code in npc_engine\\exporters\\base_exporter.py 58 59 60 61 62 63 64 65 def test_model ( self , models_path : str , model_id : str ): \"\"\"Test the model.\"\"\" start_server = click . confirm ( \"Start testing server? (It should be already running otherwise)\" ) if start_server : start_test_server ( \"5555\" , models_path ) self . test_model_impl ( models_path , model_id ) test_model_impl ( models_path , model_id ) abstractmethod Test the model. Source code in npc_engine\\exporters\\base_exporter.py 67 68 69 70 @abstractmethod def test_model_impl ( self , models_path : str , model_id : str ): \"\"\"Test the model.\"\"\" pass base_hf_exporter Abstract class for huggingface exporters. BaseHfExporter Bases: Exporter Exporter for the Huggingface transformer models. Source code in npc_engine\\exporters\\base_hf_exporter.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class BaseHfExporter ( Exporter ): \"\"\"Exporter for the Huggingface transformer models.\"\"\" def export ( self , model_path : str , export_path : str ): \"\"\"Export the model to the export path.\"\"\" click . echo ( \"Exporting model to onnx\" ) sys . argv = [ \"onnx_converter\" , \"--model\" , model_path , \"--atol\" , \"0.0001\" , \"--feature\" , self . get_export_feature (), export_path , ] from transformers.onnx import __main__ __main__ . logger = logger __main__ . main () tokenizer = AutoTokenizer . from_pretrained ( model_path ) tokenizer . save_pretrained ( export_path ) @abstractmethod def get_export_feature ( self ) -> str : \"\"\"Get the transformers.onnx feature argument that describes what interface model should have. Possible values are: 'causal-lm', 'causal-lm-with-past', 'default', 'default-with-past', 'masked-lm', 'question-answering', 'seq2seq-lm', 'seq2seq-lm-with-past', 'sequence-classification', 'token-classification' \"\"\" pass export ( model_path , export_path ) Export the model to the export path. Source code in npc_engine\\exporters\\base_hf_exporter.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def export ( self , model_path : str , export_path : str ): \"\"\"Export the model to the export path.\"\"\" click . echo ( \"Exporting model to onnx\" ) sys . argv = [ \"onnx_converter\" , \"--model\" , model_path , \"--atol\" , \"0.0001\" , \"--feature\" , self . get_export_feature (), export_path , ] from transformers.onnx import __main__ __main__ . logger = logger __main__ . main () tokenizer = AutoTokenizer . from_pretrained ( model_path ) tokenizer . save_pretrained ( export_path ) get_export_feature () abstractmethod Get the transformers.onnx feature argument that describes what interface model should have. Possible values are 'causal-lm', 'causal-lm-with-past', 'default', 'default-with-past', 'masked-lm', 'question-answering', 'seq2seq-lm', 'seq2seq-lm-with-past', 'sequence-classification', 'token-classification' Source code in npc_engine\\exporters\\base_hf_exporter.py 34 35 36 37 38 39 40 41 42 43 @abstractmethod def get_export_feature ( self ) -> str : \"\"\"Get the transformers.onnx feature argument that describes what interface model should have. Possible values are: 'causal-lm', 'causal-lm-with-past', 'default', 'default-with-past', 'masked-lm', 'question-answering', 'seq2seq-lm', 'seq2seq-lm-with-past', 'sequence-classification', 'token-classification' \"\"\" pass hf_chatbot_exporter Exporter implementation for the Huggingface text generation models. HfChatbotExporter Bases: BaseHfExporter Base exporter for Huggingface transformer models to chatbot API. Source code in npc_engine\\exporters\\hf_chatbot_exporter.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 class HfChatbotExporter ( BaseHfExporter ): \"\"\"Base exporter for Huggingface transformer models to chatbot API.\"\"\" SUPPORTED_FEATURES = [ \"causal-lm\" , \"causal-lm-with-past\" , \"seq2seq-lm\" , \"seq2seq-lm-with-past\" , ] @classmethod def get_api ( cls ) -> str : \"\"\"Get the api for the exporter.\"\"\" return \"ChatbotAPI\" @classmethod def get_model_name ( cls ): \"\"\"Get the model name.\"\"\" return \"HfChatbot\" def create_config ( self , export_path : str ): \"\"\"Create the config for the model.\"\"\" config_dict = {} config_dict [ \"model_type\" ] = self . get_model_name () config_dict [ \"template_string\" ] = click . edit ( \"{# \\n \" + \"Please create a template that will map context fields to prompt \\n \" + \"Any context fields defined here must be then sent over the request via json as context arg \\n \" + \"See Jinja docs for template design https://jinja.palletsprojects.com/en/3.1.x/templates/# \\n \" + \"this example expects a list of strings named history \\n \" + \"#} \\n \" + \"{ % f or line in history %} \\n \" + \"{{ line }} \\n \" + \"{ % e ndfor -%}\" ) config_dict [ \"min_length\" ] = click . prompt ( \"Minimum generation length:\" , type = int , default = 3 ) config_dict [ \"max_length\" ] = click . prompt ( \"Maximum generation length (affects performance):\" , type = int , default = 128 ) config_dict [ \"repetition_penalty\" ] = click . prompt ( \"Repetition penalty (probability multiplier for repeated tokens):\" , type = float , default = 1.0 , ) yaml . dump ( config_dict , open ( os . path . join ( export_path , \"config.yml\" ), \"w\" )) def get_export_feature ( self ) -> str : \"\"\"Select and return hf feature for export.\"\"\" return click . prompt ( \"Please select one of the supported features (refer to huggingface docs for details):\" , type = click . Choice ( self . SUPPORTED_FEATURES ), show_default = False , ) def test_model_impl ( self , models_path : str , model_id : str ): \"\"\"Run test request. Args: models_path: path to models model_id: model id (directory name of the model) \"\"\" config_path = os . path . join ( models_path , model_id , \"config.yml\" ) with open ( config_path , \"r\" ) as f : config = yaml . load ( f , Loader = yaml . FullLoader ) use_default_text = click . confirm ( \"Use default text for variables?\" ) schema = to_json_schema ( infer ( config [ \"template_string\" ])) if use_default_text : def get_text ( _ ): return \"Hello world\" else : def get_text ( field_name ): click . prompt ( f \"Please enter { field_name } :\" ) context = schema_to_json ( schema , get_text ) print ( f \"Context: { context } \" ) zmq_context = zmq . Context () control_client = ControlClient ( zmq_context , \"5555\" ) chatbot_client = ChatbotClient ( zmq_context , \"5555\" , model_id ) control_client . start_service ( model_id ) time . sleep ( 1 ) response = None while response is None : try : response = chatbot_client . generate_reply ( context ) except RuntimeError as e : if \"is not running\" in str ( e ): print ( \"Model is not running, waiting for it to start..\" ) time . sleep ( 1 ) else : raise e click . echo ( click . style ( \"Request context:\" , fg = \"green\" )) click . echo ( json . dumps ( context , indent = 2 )) click . echo ( click . style ( \"Reply:\" , fg = \"green\" )) click . echo ( json . dumps ( response , indent = 2 )) create_config ( export_path ) Create the config for the model. Source code in npc_engine\\exporters\\hf_chatbot_exporter.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def create_config ( self , export_path : str ): \"\"\"Create the config for the model.\"\"\" config_dict = {} config_dict [ \"model_type\" ] = self . get_model_name () config_dict [ \"template_string\" ] = click . edit ( \"{# \\n \" + \"Please create a template that will map context fields to prompt \\n \" + \"Any context fields defined here must be then sent over the request via json as context arg \\n \" + \"See Jinja docs for template design https://jinja.palletsprojects.com/en/3.1.x/templates/# \\n \" + \"this example expects a list of strings named history \\n \" + \"#} \\n \" + \"{ % f or line in history %} \\n \" + \"{{ line }} \\n \" + \"{ % e ndfor -%}\" ) config_dict [ \"min_length\" ] = click . prompt ( \"Minimum generation length:\" , type = int , default = 3 ) config_dict [ \"max_length\" ] = click . prompt ( \"Maximum generation length (affects performance):\" , type = int , default = 128 ) config_dict [ \"repetition_penalty\" ] = click . prompt ( \"Repetition penalty (probability multiplier for repeated tokens):\" , type = float , default = 1.0 , ) yaml . dump ( config_dict , open ( os . path . join ( export_path , \"config.yml\" ), \"w\" )) get_api () classmethod Get the api for the exporter. Source code in npc_engine\\exporters\\hf_chatbot_exporter.py 25 26 27 28 @classmethod def get_api ( cls ) -> str : \"\"\"Get the api for the exporter.\"\"\" return \"ChatbotAPI\" get_export_feature () Select and return hf feature for export. Source code in npc_engine\\exporters\\hf_chatbot_exporter.py 63 64 65 66 67 68 69 def get_export_feature ( self ) -> str : \"\"\"Select and return hf feature for export.\"\"\" return click . prompt ( \"Please select one of the supported features (refer to huggingface docs for details):\" , type = click . Choice ( self . SUPPORTED_FEATURES ), show_default = False , ) get_model_name () classmethod Get the model name. Source code in npc_engine\\exporters\\hf_chatbot_exporter.py 30 31 32 33 @classmethod def get_model_name ( cls ): \"\"\"Get the model name.\"\"\" return \"HfChatbot\" test_model_impl ( models_path , model_id ) Run test request. Parameters: Name Type Description Default models_path str path to models required model_id str model id (directory name of the model) required Source code in npc_engine\\exporters\\hf_chatbot_exporter.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def test_model_impl ( self , models_path : str , model_id : str ): \"\"\"Run test request. Args: models_path: path to models model_id: model id (directory name of the model) \"\"\" config_path = os . path . join ( models_path , model_id , \"config.yml\" ) with open ( config_path , \"r\" ) as f : config = yaml . load ( f , Loader = yaml . FullLoader ) use_default_text = click . confirm ( \"Use default text for variables?\" ) schema = to_json_schema ( infer ( config [ \"template_string\" ])) if use_default_text : def get_text ( _ ): return \"Hello world\" else : def get_text ( field_name ): click . prompt ( f \"Please enter { field_name } :\" ) context = schema_to_json ( schema , get_text ) print ( f \"Context: { context } \" ) zmq_context = zmq . Context () control_client = ControlClient ( zmq_context , \"5555\" ) chatbot_client = ChatbotClient ( zmq_context , \"5555\" , model_id ) control_client . start_service ( model_id ) time . sleep ( 1 ) response = None while response is None : try : response = chatbot_client . generate_reply ( context ) except RuntimeError as e : if \"is not running\" in str ( e ): print ( \"Model is not running, waiting for it to start..\" ) time . sleep ( 1 ) else : raise e click . echo ( click . style ( \"Request context:\" , fg = \"green\" )) click . echo ( json . dumps ( context , indent = 2 )) click . echo ( click . style ( \"Reply:\" , fg = \"green\" )) click . echo ( json . dumps ( response , indent = 2 )) hf_classifier_exporter Exporter for huggingface sequence classification models. HfClassifierExporter Bases: BaseHfExporter Exporter for the Huggingface classification models. Source code in npc_engine\\exporters\\hf_classifier_exporter.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 class HfClassifierExporter ( BaseHfExporter ): \"\"\"Exporter for the Huggingface classification models.\"\"\" def get_api ( self ) -> str : \"\"\"Get the api for the exporter.\"\"\" return \"SequenceClassifierAPI\" @classmethod def get_model_name ( cls ): \"\"\"Get the model name.\"\"\" return \"HfClassifier\" def create_config ( self , export_path : str ): \"\"\"Create the config for the model.\"\"\" config_dict = {} config_dict [ \"type\" ] = self . get_model_name () config_dict [ \"cache_size\" ] = click . prompt ( \"Cache size\" , type = int ) with open ( os . path . join ( export_path , \"config.yml\" ), \"w\" ) as f : yaml . dump ( config_dict , f ) def get_export_feature ( self ) -> str : \"\"\"Create the config for the model.\"\"\" return \"sequence-classification\" def test_model_impl ( self , models_path : str , model_id : str ): \"\"\"Run test request. Args: models_path: path to models model_id: model id (directory name of the model) \"\"\" use_default_text = click . confirm ( \"Use default text for variables?\" ) if use_default_text : texts = [[ \"Hello\" , \"world\" ], \"Hello world\" ] else : texts = [ click . prompt ( \"Text 1\" ), click . prompt ( \"Text 2\" )] zmq_context = zmq . Context () control_client = ControlClient ( zmq_context , \"5555\" ) service_client = SequenceClassifierClient ( zmq_context , \"5555\" , model_id ) control_client . start_service ( model_id ) ready = False while not ready : time . sleep ( 1 ) status = control_client . get_service_status ( model_id ) ready = status == \"running\" if status == \"error\" : raise Exception ( \"Error while loading model\" ) if not ready : print ( \"Waiting for service to start...\" ) scores = service_client . classify ( texts ) click . echo ( click . style ( \"Scores:\" , fg = \"green\" )) click . echo ( json . dumps ( scores , indent = 2 )) click . echo ( click . style ( \"Argmax classes:\" , fg = \"green\" )) click . echo ( json . dumps ([ classes . index ( max ( classes )) for classes in scores ], indent = 2 ) ) create_config ( export_path ) Create the config for the model. Source code in npc_engine\\exporters\\hf_classifier_exporter.py 24 25 26 27 28 29 30 def create_config ( self , export_path : str ): \"\"\"Create the config for the model.\"\"\" config_dict = {} config_dict [ \"type\" ] = self . get_model_name () config_dict [ \"cache_size\" ] = click . prompt ( \"Cache size\" , type = int ) with open ( os . path . join ( export_path , \"config.yml\" ), \"w\" ) as f : yaml . dump ( config_dict , f ) get_api () Get the api for the exporter. Source code in npc_engine\\exporters\\hf_classifier_exporter.py 15 16 17 def get_api ( self ) -> str : \"\"\"Get the api for the exporter.\"\"\" return \"SequenceClassifierAPI\" get_export_feature () Create the config for the model. Source code in npc_engine\\exporters\\hf_classifier_exporter.py 32 33 34 def get_export_feature ( self ) -> str : \"\"\"Create the config for the model.\"\"\" return \"sequence-classification\" get_model_name () classmethod Get the model name. Source code in npc_engine\\exporters\\hf_classifier_exporter.py 19 20 21 22 @classmethod def get_model_name ( cls ): \"\"\"Get the model name.\"\"\" return \"HfClassifier\" test_model_impl ( models_path , model_id ) Run test request. Parameters: Name Type Description Default models_path str path to models required model_id str model id (directory name of the model) required Source code in npc_engine\\exporters\\hf_classifier_exporter.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def test_model_impl ( self , models_path : str , model_id : str ): \"\"\"Run test request. Args: models_path: path to models model_id: model id (directory name of the model) \"\"\" use_default_text = click . confirm ( \"Use default text for variables?\" ) if use_default_text : texts = [[ \"Hello\" , \"world\" ], \"Hello world\" ] else : texts = [ click . prompt ( \"Text 1\" ), click . prompt ( \"Text 2\" )] zmq_context = zmq . Context () control_client = ControlClient ( zmq_context , \"5555\" ) service_client = SequenceClassifierClient ( zmq_context , \"5555\" , model_id ) control_client . start_service ( model_id ) ready = False while not ready : time . sleep ( 1 ) status = control_client . get_service_status ( model_id ) ready = status == \"running\" if status == \"error\" : raise Exception ( \"Error while loading model\" ) if not ready : print ( \"Waiting for service to start...\" ) scores = service_client . classify ( texts ) click . echo ( click . style ( \"Scores:\" , fg = \"green\" )) click . echo ( json . dumps ( scores , indent = 2 )) click . echo ( click . style ( \"Argmax classes:\" , fg = \"green\" )) click . echo ( json . dumps ([ classes . index ( max ( classes )) for classes in scores ], indent = 2 ) ) hf_similarity_exporter Exporter implementation for the Huggingface semantic similarity models. HfSimilarityExporter Bases: BaseHfExporter Exporter for the Huggingface transformer models. Source code in npc_engine\\exporters\\hf_similarity_exporter.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class HfSimilarityExporter ( BaseHfExporter ): \"\"\"Exporter for the Huggingface transformer models.\"\"\" def get_api ( self ) -> str : \"\"\"Get the api for the exporter.\"\"\" return \"SimilarityAPI\" @classmethod def get_model_name ( cls ): \"\"\"Get the model name.\"\"\" return \"TransformerSemanticSimilarity\" def create_config ( self , export_path : str ): \"\"\"Create the config for the model.\"\"\" config_dict = {} config_dict [ \"type\" ] = self . get_model_name ( export_path ) config_dict [ \"cache_size\" ] = click . prompt ( \"Cache size\" , type = int ) with open ( os . path . join ( export_path , \"config.yml\" ), \"w\" ) as f : yaml . dump ( config_dict , f ) def get_export_feature ( self ) -> str : \"\"\"Create the config for the model.\"\"\" return \"default\" def test_model_impl ( self , models_path : str , model_id : str ): \"\"\"Run test request. Args: models_path: path to models model_id: model id (directory name of the model) \"\"\" use_default_text = click . confirm ( \"Use default text for variables?\" ) if use_default_text : query = \"Hello world\" context = [ \"Whats up world\" ] else : query = click . prompt ( \"Query\" ) context = [ click . prompt ( \"Context\" )] zmq_context = zmq . Context () control_client = ControlClient ( zmq_context , \"5555\" ) service_client = SimilarityClient ( zmq_context , \"5555\" , model_id ) control_client . start_service ( model_id ) ready = False while not ready : time . sleep ( 1 ) status = control_client . get_service_status ( model_id ) ready = status == \"running\" if status == \"error\" : raise Exception ( \"Error while loading model\" ) if not ready : print ( \"Waiting for service to start...\" ) scores = service_client . compare ( query , context ) click . echo ( click . style ( \"Scores:\" , fg = \"green\" )) click . echo ( json . dumps ( scores , indent = 2 )) click . echo ( click . style ( \"Argmax classes:\" , fg = \"green\" )) click . echo ( json . dumps ([ classes . index ( max ( classes )) for classes in scores ], indent = 2 ) ) create_config ( export_path ) Create the config for the model. Source code in npc_engine\\exporters\\hf_similarity_exporter.py 24 25 26 27 28 29 30 def create_config ( self , export_path : str ): \"\"\"Create the config for the model.\"\"\" config_dict = {} config_dict [ \"type\" ] = self . get_model_name ( export_path ) config_dict [ \"cache_size\" ] = click . prompt ( \"Cache size\" , type = int ) with open ( os . path . join ( export_path , \"config.yml\" ), \"w\" ) as f : yaml . dump ( config_dict , f ) get_api () Get the api for the exporter. Source code in npc_engine\\exporters\\hf_similarity_exporter.py 15 16 17 def get_api ( self ) -> str : \"\"\"Get the api for the exporter.\"\"\" return \"SimilarityAPI\" get_export_feature () Create the config for the model. Source code in npc_engine\\exporters\\hf_similarity_exporter.py 32 33 34 def get_export_feature ( self ) -> str : \"\"\"Create the config for the model.\"\"\" return \"default\" get_model_name () classmethod Get the model name. Source code in npc_engine\\exporters\\hf_similarity_exporter.py 19 20 21 22 @classmethod def get_model_name ( cls ): \"\"\"Get the model name.\"\"\" return \"TransformerSemanticSimilarity\" test_model_impl ( models_path , model_id ) Run test request. Parameters: Name Type Description Default models_path str path to models required model_id str model id (directory name of the model) required Source code in npc_engine\\exporters\\hf_similarity_exporter.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def test_model_impl ( self , models_path : str , model_id : str ): \"\"\"Run test request. Args: models_path: path to models model_id: model id (directory name of the model) \"\"\" use_default_text = click . confirm ( \"Use default text for variables?\" ) if use_default_text : query = \"Hello world\" context = [ \"Whats up world\" ] else : query = click . prompt ( \"Query\" ) context = [ click . prompt ( \"Context\" )] zmq_context = zmq . Context () control_client = ControlClient ( zmq_context , \"5555\" ) service_client = SimilarityClient ( zmq_context , \"5555\" , model_id ) control_client . start_service ( model_id ) ready = False while not ready : time . sleep ( 1 ) status = control_client . get_service_status ( model_id ) ready = status == \"running\" if status == \"error\" : raise Exception ( \"Error while loading model\" ) if not ready : print ( \"Waiting for service to start...\" ) scores = service_client . compare ( query , context ) click . echo ( click . style ( \"Scores:\" , fg = \"green\" )) click . echo ( json . dumps ( scores , indent = 2 )) click . echo ( click . style ( \"Argmax classes:\" , fg = \"green\" )) click . echo ( json . dumps ([ classes . index ( max ( classes )) for classes in scores ], indent = 2 ) ) service_clients Module implementing the clients for services. chatbot_client Huggingface chatbot interface client implementation. ChatbotClient Bases: ServiceClient Json rpc client for chatbot service. Source code in npc_engine\\service_clients\\chatbot_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class ChatbotClient ( ServiceClient ): \"\"\"Json rpc client for chatbot service.\"\"\" def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str = \"ChatbotAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , service_id ) def generate_reply ( self , context : Dict [ str , Any ]) -> str : \"\"\"Send a chatbot request to the server. Args: context: A dictionary containing the chatbot request. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"generate_reply\" , \"id\" : 0 , \"params\" : [ context ], } reply = self . send_request ( request ) return reply def get_prompt_template ( self ) -> str : \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_prompt_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) def get_context_template ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_context_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) def get_special_tokens ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_special_tokens\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) __init__ ( zmq_context , port , service_id = 'ChatbotAPI' ) Connect to the server on the port. Source code in npc_engine\\service_clients\\chatbot_client.py 10 11 12 13 14 def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str = \"ChatbotAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , service_id ) generate_reply ( context ) Send a chatbot request to the server. Parameters: Name Type Description Default context Dict [ str , Any ] A dictionary containing the chatbot request. required Source code in npc_engine\\service_clients\\chatbot_client.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def generate_reply ( self , context : Dict [ str , Any ]) -> str : \"\"\"Send a chatbot request to the server. Args: context: A dictionary containing the chatbot request. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"generate_reply\" , \"id\" : 0 , \"params\" : [ context ], } reply = self . send_request ( request ) return reply get_context_template () Send a chatbot request to the server. Source code in npc_engine\\service_clients\\chatbot_client.py 41 42 43 44 45 46 47 48 49 def get_context_template ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_context_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) get_prompt_template () Send a chatbot request to the server. Source code in npc_engine\\service_clients\\chatbot_client.py 31 32 33 34 35 36 37 38 39 def get_prompt_template ( self ) -> str : \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_prompt_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) get_special_tokens () Send a chatbot request to the server. Source code in npc_engine\\service_clients\\chatbot_client.py 51 52 53 54 55 56 57 58 59 def get_special_tokens ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_special_tokens\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) control_client Control interface client implementation. ControlClient Bases: ServiceClient Json rpc client for control requests. Source code in npc_engine\\service_clients\\control_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 class ControlClient ( ServiceClient ): \"\"\"Json rpc client for control requests.\"\"\" def __init__ ( self , zmq_context : zmq . Context , port : str ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , \"control\" ) def start_service ( self , service_id ): \"\"\"Send a start service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"start_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) def stop_service ( self , service_id ): \"\"\"Send a stop service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"stop_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) def get_service_status ( self , service_id ) -> str : \"\"\"Send a get service status request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_service_status\" , \"id\" : 0 , \"params\" : [ service_id ], } return self . send_request ( request ) def restart_service ( self , service_id ): \"\"\"Send a restart service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"restart_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) def get_services_metadata ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Send a get services metadata request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_services_metadata\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) __init__ ( zmq_context , port ) Connect to the server on the port. Source code in npc_engine\\service_clients\\control_client.py 10 11 12 def __init__ ( self , zmq_context : zmq . Context , port : str ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , \"control\" ) get_service_status ( service_id ) Send a get service status request to the server. Source code in npc_engine\\service_clients\\control_client.py 34 35 36 37 38 39 40 41 42 def get_service_status ( self , service_id ) -> str : \"\"\"Send a get service status request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_service_status\" , \"id\" : 0 , \"params\" : [ service_id ], } return self . send_request ( request ) get_services_metadata () Send a get services metadata request to the server. Source code in npc_engine\\service_clients\\control_client.py 54 55 56 57 58 59 60 61 62 def get_services_metadata ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Send a get services metadata request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_services_metadata\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) restart_service ( service_id ) Send a restart service request to the server. Source code in npc_engine\\service_clients\\control_client.py 44 45 46 47 48 49 50 51 52 def restart_service ( self , service_id ): \"\"\"Send a restart service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"restart_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) start_service ( service_id ) Send a start service request to the server. Source code in npc_engine\\service_clients\\control_client.py 14 15 16 17 18 19 20 21 22 def start_service ( self , service_id ): \"\"\"Send a start service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"start_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) stop_service ( service_id ) Send a stop service request to the server. Source code in npc_engine\\service_clients\\control_client.py 24 25 26 27 28 29 30 31 32 def stop_service ( self , service_id ): \"\"\"Send a stop service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"stop_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) sequence_classifier_client Huggingface chatbot interface client implementation. SequenceClassifierClient Bases: ServiceClient Json rpc client for chatbot service. Source code in npc_engine\\service_clients\\sequence_classifier_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class SequenceClassifierClient ( ServiceClient ): \"\"\"Json rpc client for chatbot service.\"\"\" def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str = \"SequenceClassifierAPI\" , ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , service_id ) def classify ( self , texts : List [ Union [ str , Tuple [ str , str ]]]) -> str : \"\"\"Send a classify request to the SequenceClassifierAPI. Args: texts: List of texts to classify. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"classify\" , \"id\" : 0 , \"params\" : [ texts ], } reply = self . send_request ( request ) return reply __init__ ( zmq_context , port , service_id = 'SequenceClassifierAPI' ) Connect to the server on the port. Source code in npc_engine\\service_clients\\sequence_classifier_client.py 10 11 12 13 14 15 16 17 def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str = \"SequenceClassifierAPI\" , ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , service_id ) classify ( texts ) Send a classify request to the SequenceClassifierAPI. Parameters: Name Type Description Default texts List [ Union [ str , Tuple [ str , str ]]] List of texts to classify. required Source code in npc_engine\\service_clients\\sequence_classifier_client.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def classify ( self , texts : List [ Union [ str , Tuple [ str , str ]]]) -> str : \"\"\"Send a classify request to the SequenceClassifierAPI. Args: texts: List of texts to classify. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"classify\" , \"id\" : 0 , \"params\" : [ texts ], } reply = self . send_request ( request ) return reply service_client Module that implements ZMQ base client communication over JSON-RPC 2.0 (https://www.jsonrpc.org/specification). ServiceClient Base json rpc client. Source code in npc_engine\\service_clients\\service_client.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class ServiceClient : \"\"\"Base json rpc client.\"\"\" def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str ): \"\"\"Connect to the server on the port.\"\"\" self . context = zmq_context self . socket = self . context . socket ( zmq . REQ ) self . socket . setsockopt ( zmq . LINGER , 0 ) self . socket . setsockopt ( zmq . IDENTITY , service_id . encode ( \"utf-8\" )) self . socket . connect ( f \"tcp://localhost: { port } \" ) logger . info ( \"Connected to server\" ) def send_request ( self , request : Dict [ str , Any ]) -> Any : \"\"\"Send request to the server and return the response. Args: request: The request to send to the server. Returns: The result from the server. \"\"\" logger . trace ( f \"Sending request: { request } \" ) self . socket . send_json ( request ) response = self . socket . recv_json () logger . trace ( f \"Received response: { response } \" ) if \"result\" in response : return response [ \"result\" ] elif \"code\" in response : raise RuntimeError ( f \"code: { response [ 'code' ] } . { response [ 'message' ] } \" ) __init__ ( zmq_context , port , service_id ) Connect to the server on the port. Source code in npc_engine\\service_clients\\service_client.py 12 13 14 15 16 17 18 19 def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str ): \"\"\"Connect to the server on the port.\"\"\" self . context = zmq_context self . socket = self . context . socket ( zmq . REQ ) self . socket . setsockopt ( zmq . LINGER , 0 ) self . socket . setsockopt ( zmq . IDENTITY , service_id . encode ( \"utf-8\" )) self . socket . connect ( f \"tcp://localhost: { port } \" ) logger . info ( \"Connected to server\" ) send_request ( request ) Send request to the server and return the response. Parameters: Name Type Description Default request Dict [ str , Any ] The request to send to the server. required Returns: Type Description Any The result from the server. Source code in npc_engine\\service_clients\\service_client.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def send_request ( self , request : Dict [ str , Any ]) -> Any : \"\"\"Send request to the server and return the response. Args: request: The request to send to the server. Returns: The result from the server. \"\"\" logger . trace ( f \"Sending request: { request } \" ) self . socket . send_json ( request ) response = self . socket . recv_json () logger . trace ( f \"Received response: { response } \" ) if \"result\" in response : return response [ \"result\" ] elif \"code\" in response : raise RuntimeError ( f \"code: { response [ 'code' ] } . { response [ 'message' ] } \" ) similarity_client Huggingface chatbot interface client implementation. SimilarityClient Bases: ServiceClient Json rpc client for chatbot service. Source code in npc_engine\\service_clients\\similarity_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class SimilarityClient ( ServiceClient ): \"\"\"Json rpc client for chatbot service.\"\"\" def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str = \"SimilarityAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , service_id ) def compare ( self , query : str , context : List [ str ]) -> str : \"\"\"Send a chatbot request to the server. Args: context: A dictionary containing the chatbot request. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"compare\" , \"id\" : 0 , \"params\" : [ query , context ], } reply = self . send_request ( request ) return reply __init__ ( zmq_context , port , service_id = 'SimilarityAPI' ) Connect to the server on the port. Source code in npc_engine\\service_clients\\similarity_client.py 10 11 12 13 14 def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str = \"SimilarityAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , service_id ) compare ( query , context ) Send a chatbot request to the server. Parameters: Name Type Description Default context List [ str ] A dictionary containing the chatbot request. required Source code in npc_engine\\service_clients\\similarity_client.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def compare ( self , query : str , context : List [ str ]) -> str : \"\"\"Send a chatbot request to the server. Args: context: A dictionary containing the chatbot request. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"compare\" , \"id\" : 0 , \"params\" : [ query , context ], } reply = self . send_request ( request ) return reply service_manager Module with RPC related functionality implementation. server Module that implements ZMQ server communication over JSON-RPC 2.0 (https://www.jsonrpc.org/specification). Server Json rpc server over zmq. Source code in npc_engine\\service_manager\\server.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 class Server : \"\"\"Json rpc server over zmq.\"\"\" def __init__ ( self , zmq_context : zmq . asyncio . Context , service_manager : ServiceManager , port : str , start_services : bool = True , ): \"\"\"Create a server on the port.\"\"\" self . context = zmq_context self . socket = self . context . socket ( zmq . ROUTER ) self . socket . setsockopt ( zmq . LINGER , 0 ) self . socket . bind ( f \"tcp://*: { port } \" ) self . service_manager = service_manager self . start_services = start_services def run ( self ): \"\"\"Run an npc-engine json rpc server and start listening.\"\"\" logger . info ( \"Starting server\" ) if sys . platform == \"win32\" : asyncio . set_event_loop_policy ( asyncio . WindowsSelectorEventLoopPolicy ()) asyncio . get_event_loop () . run_until_complete ( self . loop ()) async def loop ( self ): \"\"\"Run the server loop.\"\"\" await asyncio . gather ( self . msg_loop (), self . interrupt_loop ()) async def interrupt_loop ( self ): \"\"\"Handle interrupts loop.\"\"\" while True : await asyncio . sleep ( 1 ) async def msg_loop ( self ): \"\"\"Asynchoriniously handle a request and reply.\"\"\" try : logger . info ( \"Starting services\" ) if self . start_services : for service in self . service_manager . services : self . service_manager . start_service ( service ) logger . info ( \"Starting message loop\" ) while True : address = await self . socket . recv () _ = await self . socket . recv () message = await self . socket . recv_string () logger . info ( f \"Received request to { address } : { message } \" ) asyncio . create_task ( self . handle_reply ( address , message )) except Exception as e : logger . error ( f \"Error in message loop: { e } \" ) logger . error ( tb . format_exc ()) raise e finally : logger . info ( \"Closing message loop\" ) self . socket . close () self . context . destroy () async def handle_reply ( self , address : str , message : str ): \"\"\"Handle message and reply.\"\"\" logging . info ( \"Handling reply\" ) start = time . time () try : address_str = address . decode ( \"utf-8\" ) except UnicodeDecodeError : address_str = address . hex () try : response = await self . service_manager . handle_request ( address_str , message ) except Exception as e : response = { \"code\" : - 32000 , \"message\" : f \"Internal error: { type ( e ) } { e } \" , \"data\" : tb . extract_tb ( e . __traceback__ ) . format () if hasattr ( e , \"__traceback__\" ) else None , } response = json . dumps ( response ) end = time . time () logger . info ( \"Handle message time: %d \" % ( end - start )) logger . info ( \"Message reply: %s \" % ( response )) # Send reply back to client await self . socket . send ( address , zmq . SNDMORE ) await self . socket . send_string ( \"\" , zmq . SNDMORE ) await self . socket . send_string ( response ) __init__ ( zmq_context , service_manager , port , start_services = True ) Create a server on the port. Source code in npc_engine\\service_manager\\server.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def __init__ ( self , zmq_context : zmq . asyncio . Context , service_manager : ServiceManager , port : str , start_services : bool = True , ): \"\"\"Create a server on the port.\"\"\" self . context = zmq_context self . socket = self . context . socket ( zmq . ROUTER ) self . socket . setsockopt ( zmq . LINGER , 0 ) self . socket . bind ( f \"tcp://*: { port } \" ) self . service_manager = service_manager self . start_services = start_services handle_reply ( address , message ) async Handle message and reply. Source code in npc_engine\\service_manager\\server.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 async def handle_reply ( self , address : str , message : str ): \"\"\"Handle message and reply.\"\"\" logging . info ( \"Handling reply\" ) start = time . time () try : address_str = address . decode ( \"utf-8\" ) except UnicodeDecodeError : address_str = address . hex () try : response = await self . service_manager . handle_request ( address_str , message ) except Exception as e : response = { \"code\" : - 32000 , \"message\" : f \"Internal error: { type ( e ) } { e } \" , \"data\" : tb . extract_tb ( e . __traceback__ ) . format () if hasattr ( e , \"__traceback__\" ) else None , } response = json . dumps ( response ) end = time . time () logger . info ( \"Handle message time: %d \" % ( end - start )) logger . info ( \"Message reply: %s \" % ( response )) # Send reply back to client await self . socket . send ( address , zmq . SNDMORE ) await self . socket . send_string ( \"\" , zmq . SNDMORE ) await self . socket . send_string ( response ) interrupt_loop () async Handle interrupts loop. Source code in npc_engine\\service_manager\\server.py 44 45 46 47 async def interrupt_loop ( self ): \"\"\"Handle interrupts loop.\"\"\" while True : await asyncio . sleep ( 1 ) loop () async Run the server loop. Source code in npc_engine\\service_manager\\server.py 40 41 42 async def loop ( self ): \"\"\"Run the server loop.\"\"\" await asyncio . gather ( self . msg_loop (), self . interrupt_loop ()) msg_loop () async Asynchoriniously handle a request and reply. Source code in npc_engine\\service_manager\\server.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 async def msg_loop ( self ): \"\"\"Asynchoriniously handle a request and reply.\"\"\" try : logger . info ( \"Starting services\" ) if self . start_services : for service in self . service_manager . services : self . service_manager . start_service ( service ) logger . info ( \"Starting message loop\" ) while True : address = await self . socket . recv () _ = await self . socket . recv () message = await self . socket . recv_string () logger . info ( f \"Received request to { address } : { message } \" ) asyncio . create_task ( self . handle_reply ( address , message )) except Exception as e : logger . error ( f \"Error in message loop: { e } \" ) logger . error ( tb . format_exc ()) raise e finally : logger . info ( \"Closing message loop\" ) self . socket . close () self . context . destroy () run () Run an npc-engine json rpc server and start listening. Source code in npc_engine\\service_manager\\server.py 33 34 35 36 37 38 def run ( self ): \"\"\"Run an npc-engine json rpc server and start listening.\"\"\" logger . info ( \"Starting server\" ) if sys . platform == \"win32\" : asyncio . set_event_loop_policy ( asyncio . WindowsSelectorEventLoopPolicy ()) asyncio . get_event_loop () . run_until_complete ( self . loop ()) service_manager Module that implements lifetime and discoverability of the services. ServiceManager Object for managing lifetime and discoverability of the services. Source code in npc_engine\\service_manager\\service_manager.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 class ServiceManager : \"\"\"Object for managing lifetime and discoverability of the services.\"\"\" def __init__ ( self , zmq_context : zmq . asyncio . Context , path ): \"\"\"Create model manager and load models from the given path.\"\"\" self . services = self . _scan_path ( path ) self . control_dispatcher = Dispatcher () self . control_dispatcher . update ( { \"get_services_metadata\" : self . get_services_metadata , \"get_service_status\" : self . get_service_status , \"start_service\" : self . start_service , \"stop_service\" : self . stop_service , \"restart_service\" : self . restart_service , } ) self . zmq_context = zmq_context os . makedirs ( \".npc_engine_tmp\" , exist_ok = True ) def __del__ ( self ): \"\"\"Stop all services.\"\"\" if hasattr ( self , \"services\" ): for service_id , service in self . services . items (): if service . process_data [ \"state\" ] == ServiceState . RUNNING : self . stop_service ( service_id ) async def handle_request ( self , address : str , request : str ) -> str : \"\"\"Parse request string and route request to correct service. Args: address (str): address of the service (either model name or class name) request (str): jsonRPC string Returns: str: jsonRPC response \"\"\" request_dict = json . loads ( request ) service_id = self . resolve_and_check_service ( address , request_dict [ \"method\" ]) logger . info ( f \"Request from { address } \\n Request: { request } \" ) if service_id == \"control\" : return JSONRPCResponseManager . handle ( request , self . control_dispatcher ) . json else : if self . services [ service_id ] . process_data [ \"state\" ] != ServiceState . RUNNING : raise ValueError ( f \"Service { service_id } is not running\" ) else : socket = self . services [ service_id ] . process_data [ \"socket\" ] await socket . send_string ( request ) response = await socket . recv_string () return response def resolve_and_check_service ( self , id_or_type , method = None ): \"\"\"Resolve service id or type to service id.\"\"\" service_id = None if id_or_type == \"control\" : service_id = \"control\" else : if id_or_type in self . services : service_id = id_or_type if service_id is None : for service_key , service in self . services . items (): if service . type == id_or_type or service . api_name == id_or_type : service_id = service_key break if service_id is None and method is not None : service_id = self . resolve_by_method ( method ) elif service_id is None : raise ValueError ( f \"Service { id_or_type } not found\" ) if ( self . services [ service_id ] . process_data [ \"state\" ] == ServiceState . RUNNING or self . services [ service_id ] . process_data [ \"state\" ] == ServiceState . STARTING or self . services [ service_id ] . process_data [ \"state\" ] == ServiceState . AWAITING ) and not self . services [ service_id ] . process_data [ \"process\" ] . is_alive (): self . services [ service_id ] . process_data [ \"state\" ] = ServiceState . ERROR raise ValueError ( f \"Error in service { service_id } . Process is not alive.\" ) return service_id def resolve_by_method ( self , method_name ): \"\"\"Resolve service id by method name.\"\"\" for service_id , service in self . services . items (): if service . process_data [ \"state\" ] == ServiceState . RUNNING : if method_name in service . api_methods : return service_id raise ValueError ( f \"Runnning service with method { method_name } not found\" ) def get_services_metadata ( self ): \"\"\"List the models in the folder.\"\"\" return [ services . BaseService . get_metadata ( descriptor . path ) for descriptor in self . services . values () ] def get_service_status ( self , service_id ): \"\"\"Get the status of the service.\"\"\" service_id = self . resolve_and_check_service ( service_id , None ) return self . services [ service_id ] . process_data [ \"state\" ] def start_service ( self , service_id ): \"\"\"Start the service.\"\"\" service_id = self . resolve_and_check_service ( service_id , None ) if service_id not in self . services : raise ValueError ( f \"Service { service_id } not found\" ) if self . services [ service_id ] . process_data [ \"state\" ] == ServiceState . RUNNING : raise ValueError ( f \"Service { service_id } is already running\" ) process = Process ( target = service_process , args = ( self . services [ service_id ] . path , self . services [ service_id ] . uri ), daemon = True , ) process . start () self . services [ service_id ] . process_data [ \"process\" ] = process self . services [ service_id ] . process_data [ \"state\" ] = ServiceState . STARTING self . services [ service_id ] . process_data [ \"socket\" ] = self . zmq_context . socket ( zmq . REQ ) self . services [ service_id ] . process_data [ \"socket\" ] . setsockopt ( zmq . LINGER , 0 ) self . services [ service_id ] . process_data [ \"socket\" ] . setsockopt ( zmq . RCVTIMEO , 10000 ) self . services [ service_id ] . process_data [ \"socket\" ] . connect ( self . services [ service_id ] . uri ) try : asyncio . create_task ( self . confirm_state_coroutine ( service_id )) except RuntimeError : logger . warning ( \"Create task to confirm service state failed.\" + \" Probably asyncio loop is not running.\" + \" Trying to execute it via asyncio.run()\" ) asyncio . run ( self . confirm_state_coroutine ( service_id )) async def confirm_state_coroutine ( self , service_id ): \"\"\"Confirm the state of the service.\"\"\" request = json . dumps ({ \"jsonrpc\" : \"2.0\" , \"method\" : \"status\" , \"id\" : 1 }) socket = self . services [ service_id ] . process_data [ \"socket\" ] await socket . send_string ( request ) response = await socket . recv_string () resp_dict = json . loads ( response ) if resp_dict [ \"result\" ] == ServiceState . RUNNING : self . services [ service_id ] . process_data [ \"state\" ] = ServiceState . RUNNING elif resp_dict [ \"result\" ] == ServiceState . STARTING : logger . info ( f \"Service { service_id } responds but still starting\" ) await asyncio . sleep ( 1 ) await self . confirm_state_coroutine ( service_id ) else : logger . warning ( f \"Service { service_id } failed to start and returned incorrect state.\" ) self . services [ service_id ] . process_data [ \"state\" ] = ServiceState . ERROR def stop_service ( self , service_id ): \"\"\"Stop the service.\"\"\" service_id = self . resolve_and_check_service ( service_id , None ) if service_id not in self . services : raise ValueError ( f \"Service { service_id } not found\" ) if self . services [ service_id ] . process_data [ \"state\" ] != ServiceState . RUNNING : raise ValueError ( f \"Service { service_id } is not running\" ) self . services [ service_id ] . process_data [ \"socket\" ] . close () self . services [ service_id ] . process_data [ \"socket\" ] = None self . services [ service_id ] . process_data [ \"process\" ] . terminate () self . services [ service_id ] . process_data [ \"process\" ] = None self . services [ service_id ] . process_data [ \"state\" ] = ServiceState . STOPPED def restart_service ( self , service_id ): \"\"\"Restart the service.\"\"\" self . stop_service ( service_id ) self . start_service ( service_id ) def _scan_path ( self , path : str ) -> Dict [ str , ServiceDescriptor ]: \"\"\"Scan services defined in the given path.\"\"\" norm_path = ntpath . normpath ( path ) . replace ( \" \\\\ \" , os . path . sep ) paths = [ f . path for f in os . scandir ( norm_path ) if f . is_dir () and os . path . exists ( os . path . join ( f , \"config.yml\" )) ] svcs = {} for path in paths : with open ( os . path . join ( path , \"config.yml\" )) as f : config_dict = yaml . safe_load ( f ) uri = f \"ipc:// { os . path . join ( user_cache_dir ( 'npc-engine' , 'NpcEngine' ), os . path . basename ( path )) } \" cls = getattr ( services , config_dict . get ( \"model_type\" , config_dict . get ( \"type\" , None )), ) svcs [ os . path . basename ( path )] = ServiceDescriptor ( os . path . basename ( path ), config_dict . get ( \"model_type\" , config_dict . get ( \"type\" , None )), path , uri , cls . get_api_name (), cls . API_METHODS , { \"process\" : None , \"socket\" : None , \"state\" : ServiceState . STOPPED }, ) return svcs __del__ () Stop all services. Source code in npc_engine\\service_manager\\service_manager.py 71 72 73 74 75 76 def __del__ ( self ): \"\"\"Stop all services.\"\"\" if hasattr ( self , \"services\" ): for service_id , service in self . services . items (): if service . process_data [ \"state\" ] == ServiceState . RUNNING : self . stop_service ( service_id ) __init__ ( zmq_context , path ) Create model manager and load models from the given path. Source code in npc_engine\\service_manager\\service_manager.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def __init__ ( self , zmq_context : zmq . asyncio . Context , path ): \"\"\"Create model manager and load models from the given path.\"\"\" self . services = self . _scan_path ( path ) self . control_dispatcher = Dispatcher () self . control_dispatcher . update ( { \"get_services_metadata\" : self . get_services_metadata , \"get_service_status\" : self . get_service_status , \"start_service\" : self . start_service , \"stop_service\" : self . stop_service , \"restart_service\" : self . restart_service , } ) self . zmq_context = zmq_context os . makedirs ( \".npc_engine_tmp\" , exist_ok = True ) _scan_path ( path ) Scan services defined in the given path. Source code in npc_engine\\service_manager\\service_manager.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 def _scan_path ( self , path : str ) -> Dict [ str , ServiceDescriptor ]: \"\"\"Scan services defined in the given path.\"\"\" norm_path = ntpath . normpath ( path ) . replace ( \" \\\\ \" , os . path . sep ) paths = [ f . path for f in os . scandir ( norm_path ) if f . is_dir () and os . path . exists ( os . path . join ( f , \"config.yml\" )) ] svcs = {} for path in paths : with open ( os . path . join ( path , \"config.yml\" )) as f : config_dict = yaml . safe_load ( f ) uri = f \"ipc:// { os . path . join ( user_cache_dir ( 'npc-engine' , 'NpcEngine' ), os . path . basename ( path )) } \" cls = getattr ( services , config_dict . get ( \"model_type\" , config_dict . get ( \"type\" , None )), ) svcs [ os . path . basename ( path )] = ServiceDescriptor ( os . path . basename ( path ), config_dict . get ( \"model_type\" , config_dict . get ( \"type\" , None )), path , uri , cls . get_api_name (), cls . API_METHODS , { \"process\" : None , \"socket\" : None , \"state\" : ServiceState . STOPPED }, ) return svcs confirm_state_coroutine ( service_id ) async Confirm the state of the service. Source code in npc_engine\\service_manager\\service_manager.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 async def confirm_state_coroutine ( self , service_id ): \"\"\"Confirm the state of the service.\"\"\" request = json . dumps ({ \"jsonrpc\" : \"2.0\" , \"method\" : \"status\" , \"id\" : 1 }) socket = self . services [ service_id ] . process_data [ \"socket\" ] await socket . send_string ( request ) response = await socket . recv_string () resp_dict = json . loads ( response ) if resp_dict [ \"result\" ] == ServiceState . RUNNING : self . services [ service_id ] . process_data [ \"state\" ] = ServiceState . RUNNING elif resp_dict [ \"result\" ] == ServiceState . STARTING : logger . info ( f \"Service { service_id } responds but still starting\" ) await asyncio . sleep ( 1 ) await self . confirm_state_coroutine ( service_id ) else : logger . warning ( f \"Service { service_id } failed to start and returned incorrect state.\" ) self . services [ service_id ] . process_data [ \"state\" ] = ServiceState . ERROR get_service_status ( service_id ) Get the status of the service. Source code in npc_engine\\service_manager\\service_manager.py 148 149 150 151 def get_service_status ( self , service_id ): \"\"\"Get the status of the service.\"\"\" service_id = self . resolve_and_check_service ( service_id , None ) return self . services [ service_id ] . process_data [ \"state\" ] get_services_metadata () List the models in the folder. Source code in npc_engine\\service_manager\\service_manager.py 141 142 143 144 145 146 def get_services_metadata ( self ): \"\"\"List the models in the folder.\"\"\" return [ services . BaseService . get_metadata ( descriptor . path ) for descriptor in self . services . values () ] handle_request ( address , request ) async Parse request string and route request to correct service. Parameters: Name Type Description Default address str address of the service (either model name or class name) required request str jsonRPC string required Returns: Name Type Description str str jsonRPC response Source code in npc_engine\\service_manager\\service_manager.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 async def handle_request ( self , address : str , request : str ) -> str : \"\"\"Parse request string and route request to correct service. Args: address (str): address of the service (either model name or class name) request (str): jsonRPC string Returns: str: jsonRPC response \"\"\" request_dict = json . loads ( request ) service_id = self . resolve_and_check_service ( address , request_dict [ \"method\" ]) logger . info ( f \"Request from { address } \\n Request: { request } \" ) if service_id == \"control\" : return JSONRPCResponseManager . handle ( request , self . control_dispatcher ) . json else : if self . services [ service_id ] . process_data [ \"state\" ] != ServiceState . RUNNING : raise ValueError ( f \"Service { service_id } is not running\" ) else : socket = self . services [ service_id ] . process_data [ \"socket\" ] await socket . send_string ( request ) response = await socket . recv_string () return response resolve_and_check_service ( id_or_type , method = None ) Resolve service id or type to service id. Source code in npc_engine\\service_manager\\service_manager.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def resolve_and_check_service ( self , id_or_type , method = None ): \"\"\"Resolve service id or type to service id.\"\"\" service_id = None if id_or_type == \"control\" : service_id = \"control\" else : if id_or_type in self . services : service_id = id_or_type if service_id is None : for service_key , service in self . services . items (): if service . type == id_or_type or service . api_name == id_or_type : service_id = service_key break if service_id is None and method is not None : service_id = self . resolve_by_method ( method ) elif service_id is None : raise ValueError ( f \"Service { id_or_type } not found\" ) if ( self . services [ service_id ] . process_data [ \"state\" ] == ServiceState . RUNNING or self . services [ service_id ] . process_data [ \"state\" ] == ServiceState . STARTING or self . services [ service_id ] . process_data [ \"state\" ] == ServiceState . AWAITING ) and not self . services [ service_id ] . process_data [ \"process\" ] . is_alive (): self . services [ service_id ] . process_data [ \"state\" ] = ServiceState . ERROR raise ValueError ( f \"Error in service { service_id } . Process is not alive.\" ) return service_id resolve_by_method ( method_name ) Resolve service id by method name. Source code in npc_engine\\service_manager\\service_manager.py 133 134 135 136 137 138 139 def resolve_by_method ( self , method_name ): \"\"\"Resolve service id by method name.\"\"\" for service_id , service in self . services . items (): if service . process_data [ \"state\" ] == ServiceState . RUNNING : if method_name in service . api_methods : return service_id raise ValueError ( f \"Runnning service with method { method_name } not found\" ) restart_service ( service_id ) Restart the service. Source code in npc_engine\\service_manager\\service_manager.py 219 220 221 222 def restart_service ( self , service_id ): \"\"\"Restart the service.\"\"\" self . stop_service ( service_id ) self . start_service ( service_id ) start_service ( service_id ) Start the service. Source code in npc_engine\\service_manager\\service_manager.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def start_service ( self , service_id ): \"\"\"Start the service.\"\"\" service_id = self . resolve_and_check_service ( service_id , None ) if service_id not in self . services : raise ValueError ( f \"Service { service_id } not found\" ) if self . services [ service_id ] . process_data [ \"state\" ] == ServiceState . RUNNING : raise ValueError ( f \"Service { service_id } is already running\" ) process = Process ( target = service_process , args = ( self . services [ service_id ] . path , self . services [ service_id ] . uri ), daemon = True , ) process . start () self . services [ service_id ] . process_data [ \"process\" ] = process self . services [ service_id ] . process_data [ \"state\" ] = ServiceState . STARTING self . services [ service_id ] . process_data [ \"socket\" ] = self . zmq_context . socket ( zmq . REQ ) self . services [ service_id ] . process_data [ \"socket\" ] . setsockopt ( zmq . LINGER , 0 ) self . services [ service_id ] . process_data [ \"socket\" ] . setsockopt ( zmq . RCVTIMEO , 10000 ) self . services [ service_id ] . process_data [ \"socket\" ] . connect ( self . services [ service_id ] . uri ) try : asyncio . create_task ( self . confirm_state_coroutine ( service_id )) except RuntimeError : logger . warning ( \"Create task to confirm service state failed.\" + \" Probably asyncio loop is not running.\" + \" Trying to execute it via asyncio.run()\" ) asyncio . run ( self . confirm_state_coroutine ( service_id )) stop_service ( service_id ) Stop the service. Source code in npc_engine\\service_manager\\service_manager.py 206 207 208 209 210 211 212 213 214 215 216 217 def stop_service ( self , service_id ): \"\"\"Stop the service.\"\"\" service_id = self . resolve_and_check_service ( service_id , None ) if service_id not in self . services : raise ValueError ( f \"Service { service_id } not found\" ) if self . services [ service_id ] . process_data [ \"state\" ] != ServiceState . RUNNING : raise ValueError ( f \"Service { service_id } is not running\" ) self . services [ service_id ] . process_data [ \"socket\" ] . close () self . services [ service_id ] . process_data [ \"socket\" ] = None self . services [ service_id ] . process_data [ \"process\" ] . terminate () self . services [ service_id ] . process_data [ \"process\" ] = None self . services [ service_id ] . process_data [ \"state\" ] = ServiceState . STOPPED ServiceState Enum for the state of the service. Source code in npc_engine\\service_manager\\service_manager.py 19 20 21 22 23 24 25 26 27 class ServiceState : \"\"\"Enum for the state of the service.\"\"\" STARTING = \"starting\" RUNNING = \"running\" STOPPED = \"stopped\" AWAITING = \"awaiting\" TIMEOUT = \"timeout\" ERROR = \"error\" service_process ( service_path , uri ) Service subprocess function. Starts the service and runs it's loop. Source code in npc_engine\\service_manager\\service_manager.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def service_process ( service_path : str , uri : str ) -> None : \"\"\"Service subprocess function. Starts the service and runs it's loop. \"\"\" logger . remove () logger . add ( os . path . join ( \"logs\" , f \" { service_path . split ( os . path . sep )[ - 1 ] } .log\" ), rotation = \"10 MB\" , enqueue = True , ) context = zmq . Context () service = services . BaseService . create ( context , service_path , uri ) service . start () utils Utility functions for RPC communication. schema_to_json ( s , fill_value = lambda _ : '' ) Iterate the schema and return simplified dictionary. Source code in npc_engine\\service_manager\\utils.py 6 7 8 9 10 11 12 13 14 15 16 17 def schema_to_json ( s : Dict [ str , Any ], fill_value : Callable [[ str ], Any ] = lambda _ : \"\" ) -> Dict [ str , Any ]: \"\"\"Iterate the schema and return simplified dictionary.\"\"\" if \"type\" not in s and \"anyOf\" in s : return fill_value ( s [ \"title\" ]) elif \"type\" in s and s [ \"type\" ] == \"object\" : return { k : schema_to_json ( v ) for k , v in s [ \"properties\" ] . items ()} elif \"type\" in s and s [ \"type\" ] == \"array\" : return [ schema_to_json ( s [ \"items\" ])] else : raise ValueError ( f \"Unknown schema type: { s } \" ) start_test_server ( port , models_path ) Start the test server. Parameters: Name Type Description Default port str The port to start the server on. required models_path str The path to the models. required Source code in npc_engine\\service_manager\\utils.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def start_test_server ( port : str , models_path : str ): # pragma: no cover \"\"\"Start the test server. Args: port: The port to start the server on. models_path: The path to the models. \"\"\" subprocess . Popen ( [ \"npc-engine\" , \"--verbose\" , \"run\" , \"--port\" , port , \"--models-path\" , models_path , ], creationflags = subprocess . CREATE_NEW_CONSOLE , ) services Module that contains everything related to deep learning models. For your model API to be discovered it must be imported here base_service Module with Model base class. BaseService Bases: ABC Abstract base class for managed services. Source code in npc_engine\\services\\base_service.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 class BaseService ( ABC ): \"\"\"Abstract base class for managed services.\"\"\" models = {} def __init_subclass__ ( cls , ** kwargs ): \"\"\"Init subclass where service classes get registered to be discovered.\"\"\" super () . __init_subclass__ ( ** kwargs ) cls . models [ cls . __name__ ] = cls def __init__ ( self , context : zmq . Context , uri : str , * args , ** kwargs ): \"\"\"Initialize the service.\"\"\" self . zmq_context = context self . socket = context . socket ( zmq . REP ) self . socket . setsockopt ( zmq . LINGER , 0 ) print ( uri ) if uri . startswith ( \"ipc://\" ): os . makedirs ( Path ( uri . replace ( \"ipc://\" , \"\" )) . parent , exist_ok = True ) os . chmod ( Path ( uri . replace ( \"ipc://\" , \"\" )) . parent , 777 ) self . socket . bind ( uri ) @classmethod @abstractmethod def get_api_name ( cls ): \"\"\"Return the name of the API.\"\"\" pass @classmethod def create ( cls , context : zmq . Context , path : str , uri : str ): \"\"\"Create a service from the path.\"\"\" config_path = os . path . join ( path , \"config.yml\" ) with open ( config_path ) as f : config_dict = yaml . load ( f , Loader = yaml . Loader ) config_dict [ \"model_path\" ] = path config_dict [ \"uri\" ] = uri model_cls = cls . models [ get_type_from_dict ( config_dict )] return model_cls ( ** config_dict , context = context ) def start ( self ): \"\"\"Run service main loop that accepts json rpc.\"\"\" try : dispatcher = Dispatcher () dispatcher . update ( self . build_api_dict ()) dispatcher . update ({ \"status\" : self . status }) while True : request = self . socket . recv_string () response = JSONRPCResponseManager . handle ( request , dispatcher ) self . socket . send_string ( response . json ) except Exception as e : logger . exception ( e ) raise e finally : self . socket . close () self . zmq_context . destroy () def status ( self ): \"\"\"Return status of the service.\"\"\" from npc_engine.service_manager.service_manager import ServiceState return ServiceState . RUNNING def build_api_dict ( self ): \"\"\"Build api dict. Returns: dict(str,str): Mapping \"method_name\" -> callable that will be exposed to API \"\"\" api_dict = {} for method in type ( self ) . API_METHODS : logger . info ( f \"Registering method { method } for model { type ( self ) . __name__ } \" ) # TODO api_dict [ method ] = getattr ( self , method ) return api_dict @classmethod def get_metadata ( cls , path : str ) -> Dict [ str , str ]: \"\"\"Print the model from the path.\"\"\" path = path . replace ( \" \\\\ \" , os . path . sep ) model_id = path . split ( os . path . sep )[ - 1 ] config_path = os . path . join ( path , \"config.yml\" ) readme_path = os . path . join ( path , \"README.md\" ) with open ( config_path ) as f : config_dict = yaml . load ( f , Loader = yaml . Loader ) try : with open ( readme_path ) as f : readme = f . read () . split ( \"---\" )[ - 1 ] except FileNotFoundError : readme = \"\" return { \"id\" : model_id , \"service\" : get_type_from_dict ( config_dict ), \"path\" : path , \"service_short_description\" : cls . models [ get_type_from_dict ( config_dict ) ] . __doc__ . split ( \" \\n\\n \" )[ 0 ], \"service_description\" : cls . models [ get_type_from_dict ( config_dict )] . __doc__ , \"readme\" : readme , } __init__ ( context , uri , * args , ** kwargs ) Initialize the service. Source code in npc_engine\\services\\base_service.py 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , context : zmq . Context , uri : str , * args , ** kwargs ): \"\"\"Initialize the service.\"\"\" self . zmq_context = context self . socket = context . socket ( zmq . REP ) self . socket . setsockopt ( zmq . LINGER , 0 ) print ( uri ) if uri . startswith ( \"ipc://\" ): os . makedirs ( Path ( uri . replace ( \"ipc://\" , \"\" )) . parent , exist_ok = True ) os . chmod ( Path ( uri . replace ( \"ipc://\" , \"\" )) . parent , 777 ) self . socket . bind ( uri ) __init_subclass__ ( ** kwargs ) Init subclass where service classes get registered to be discovered. Source code in npc_engine\\services\\base_service.py 19 20 21 22 def __init_subclass__ ( cls , ** kwargs ): \"\"\"Init subclass where service classes get registered to be discovered.\"\"\" super () . __init_subclass__ ( ** kwargs ) cls . models [ cls . __name__ ] = cls build_api_dict () Build api dict. Returns: Name Type Description dict str , str Mapping \"method_name\" -> callable that will be exposed to API Source code in npc_engine\\services\\base_service.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def build_api_dict ( self ): \"\"\"Build api dict. Returns: dict(str,str): Mapping \"method_name\" -> callable that will be exposed to API \"\"\" api_dict = {} for method in type ( self ) . API_METHODS : logger . info ( f \"Registering method { method } for model { type ( self ) . __name__ } \" ) # TODO api_dict [ method ] = getattr ( self , method ) return api_dict create ( context , path , uri ) classmethod Create a service from the path. Source code in npc_engine\\services\\base_service.py 41 42 43 44 45 46 47 48 49 50 @classmethod def create ( cls , context : zmq . Context , path : str , uri : str ): \"\"\"Create a service from the path.\"\"\" config_path = os . path . join ( path , \"config.yml\" ) with open ( config_path ) as f : config_dict = yaml . load ( f , Loader = yaml . Loader ) config_dict [ \"model_path\" ] = path config_dict [ \"uri\" ] = uri model_cls = cls . models [ get_type_from_dict ( config_dict )] return model_cls ( ** config_dict , context = context ) get_api_name () abstractmethod classmethod Return the name of the API. Source code in npc_engine\\services\\base_service.py 35 36 37 38 39 @classmethod @abstractmethod def get_api_name ( cls ): \"\"\"Return the name of the API.\"\"\" pass get_metadata ( path ) classmethod Print the model from the path. Source code in npc_engine\\services\\base_service.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 @classmethod def get_metadata ( cls , path : str ) -> Dict [ str , str ]: \"\"\"Print the model from the path.\"\"\" path = path . replace ( \" \\\\ \" , os . path . sep ) model_id = path . split ( os . path . sep )[ - 1 ] config_path = os . path . join ( path , \"config.yml\" ) readme_path = os . path . join ( path , \"README.md\" ) with open ( config_path ) as f : config_dict = yaml . load ( f , Loader = yaml . Loader ) try : with open ( readme_path ) as f : readme = f . read () . split ( \"---\" )[ - 1 ] except FileNotFoundError : readme = \"\" return { \"id\" : model_id , \"service\" : get_type_from_dict ( config_dict ), \"path\" : path , \"service_short_description\" : cls . models [ get_type_from_dict ( config_dict ) ] . __doc__ . split ( \" \\n\\n \" )[ 0 ], \"service_description\" : cls . models [ get_type_from_dict ( config_dict )] . __doc__ , \"readme\" : readme , } start () Run service main loop that accepts json rpc. Source code in npc_engine\\services\\base_service.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def start ( self ): \"\"\"Run service main loop that accepts json rpc.\"\"\" try : dispatcher = Dispatcher () dispatcher . update ( self . build_api_dict ()) dispatcher . update ({ \"status\" : self . status }) while True : request = self . socket . recv_string () response = JSONRPCResponseManager . handle ( request , dispatcher ) self . socket . send_string ( response . json ) except Exception as e : logger . exception ( e ) raise e finally : self . socket . close () self . zmq_context . destroy () status () Return status of the service. Source code in npc_engine\\services\\base_service.py 69 70 71 72 73 def status ( self ): \"\"\"Return status of the service.\"\"\" from npc_engine.service_manager.service_manager import ServiceState return ServiceState . RUNNING chatbot Chatbot model implementations. This module implements specific models and wraps them under the common interface for loading and inference. Example from npc_engine.models.chatbot import ChatbotAPI model = ChatbotAPI.load(\"path/to/model_dir\") model.generate_reply(context, temperature=0.8, topk=None,) bart BART based chatbot implementation. BartChatbot Bases: ChatbotAPI BART based chatbot implementation class. This model class requires two ONNX models encoder_bart.onnx and decoder_bart.onnx that correspond to encoder and decoder from transformers EncoderDecoderModel and a tokenizer.json with huggingface tokenizers definition. encoder_bart.onnx spec: - inputs: `input_ids` - outputs: `encoder_hidden_state` decoder_bart.onnx spec: - inputs: `encoder_hidden_state` `decoder_input_ids` - outputs: `logits` Source code in npc_engine\\services\\chatbot\\bart.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 class BartChatbot ( ChatbotAPI ): \"\"\"BART based chatbot implementation class. This model class requires two ONNX models `encoder_bart.onnx` and `decoder_bart.onnx` that correspond to encoder and decoder from transformers [EncoderDecoderModel](https://huggingface.co/transformers/model_doc/encoderdecoder.html) and a tokenizer.json with huggingface tokenizers definition. encoder_bart.onnx spec: - inputs: `input_ids` - outputs: `encoder_hidden_state` decoder_bart.onnx spec: - inputs: `encoder_hidden_state` `decoder_input_ids` - outputs: `logits` \"\"\" def __init__ ( self , model_path , max_steps = 100 , min_length = 2 , repetition_penalty = 1 , bos_token_id = 0 , eos_token_id = 2 , pad_token_id = 1 , sep_token_id = None , * args , ** kwargs , ): \"\"\"Create the chatbot from config args and kwargs. Args: model_path: path to scan for model files (weights and configs) max_steps: stop generation at this number of tokens min_length: model can't stop generating text before it's atleast this long in tokens repetition_penalty: probability coef for same tokens to appear multiple times bos_token_id: beginning of sequence token id eos_token_id: end of sequence token id pad_token_id: padding token id sep_token_id: token id for separating sequence into multiple parts \"\"\" super () . __init__ ( * args , ** kwargs ) self . bos_token_id = bos_token_id self . eos_token_id = eos_token_id self . sep_token_id = eos_token_id if sep_token_id is None else sep_token_id self . pad_token_id = pad_token_id sess_options = rt . SessionOptions () sess_options . graph_optimization_level = ( rt . GraphOptimizationLevel . ORT_DISABLE_ALL ) self . encoder_model = rt . InferenceSession ( os . path . join ( model_path , \"encoder_bart.onnx\" ), providers = [ rt . get_available_providers ()[ 0 ]], sess_options = sess_options , ) self . decoder_model = rt . InferenceSession ( os . path . join ( model_path , \"decoder_bart.onnx\" ), providers = [ rt . get_available_providers ()[ 0 ]], sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) added_tokens_path = os . path . join ( model_path , \"added_tokens.txt\" ) if os . path . exists ( added_tokens_path ): with open ( added_tokens_path ) as f : added_tokens = json . load ( f ) added_tokens = [ key for key , _ in sorted ( list ( added_tokens . items ()), key = lambda x : x [ 1 ]) ] self . tokenizer . add_tokens ( added_tokens ) self . special_tokens = { \"bos_token\" : self . tokenizer . decode ( [ bos_token_id ], skip_special_tokens = False ), \"eos_token\" : self . tokenizer . decode ( [ eos_token_id ], skip_special_tokens = False ), \"sep_token\" : self . tokenizer . decode ( [ self . sep_token_id ], skip_special_tokens = False ), \"pad_token\" : self . tokenizer . decode ( [ pad_token_id ], skip_special_tokens = False ), ** { f \"added_token { self . tokenizer . token_to_id ( token ) } \" : token for token in added_tokens }, } self . max_steps = max_steps self . min_length = min_length self . repetition_penalty = repetition_penalty def run ( self , prompt : str , temperature : float = 1.0 , topk : int = None ) -> str : \"\"\"Run text generation from given prompt and parameters. Args: prompt: Fromatted prompt. temperature: Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness topk: If not none selects top n of predictions to sample from during generation. Returns: Generated text \"\"\" tokens = self . tokenizer . encode ( prompt ) total = np . asarray ( tokens . ids , dtype = np . int64 ) . reshape ([ 1 , - 1 ]) total_enc = self . encoder_model . run ( None , { \"input_ids\" : total })[ 0 ] utterance = np . asarray ([ self . eos_token_id ], dtype = np . int64 ) . reshape ([ 1 , 1 ]) for i in range ( self . max_steps ): o = self . decoder_model . run ( None , { \"encoder_hidden_state\" : total_enc , \"decoder_input_ids\" : utterance }, ) logits = o [ 0 ][ 0 , - 1 , :] if i < self . min_length : logits [ self . eos_token_id ] = float ( \"-inf\" ) if topk is not None : ind = np . argpartition ( logits , - topk )[ - topk :] new_logits = np . zeros ( logits . shape ) new_logits [ ind ] = logits [ ind ] logits = new_logits probs = scp . softmax ( logits / temperature , axis = 0 ) token = np . random . choice ( np . arange ( probs . shape [ 0 ]), p = probs ) token = token . reshape ([ 1 , 1 ]) utterance = np . concatenate ([ utterance , token ], axis = 1 ) if token [ 0 , 0 ] == self . eos_token_id : break return self . tokenizer . decode ( utterance [ 0 , :] . tolist (), skip_special_tokens = True ) def get_special_tokens ( self ) -> Dict [ str , str ]: \"\"\"Retrun dict of special tokens to be renderable from template.\"\"\" return self . special_tokens __init__ ( model_path , max_steps = 100 , min_length = 2 , repetition_penalty = 1 , bos_token_id = 0 , eos_token_id = 2 , pad_token_id = 1 , sep_token_id = None , * args , ** kwargs ) Create the chatbot from config args and kwargs. Parameters: Name Type Description Default model_path path to scan for model files (weights and configs) required max_steps stop generation at this number of tokens 100 min_length model can't stop generating text before it's atleast this long in tokens 2 repetition_penalty probability coef for same tokens to appear multiple times 1 bos_token_id beginning of sequence token id 0 eos_token_id end of sequence token id 2 pad_token_id padding token id 1 sep_token_id token id for separating sequence into multiple parts None Source code in npc_engine\\services\\chatbot\\bart.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def __init__ ( self , model_path , max_steps = 100 , min_length = 2 , repetition_penalty = 1 , bos_token_id = 0 , eos_token_id = 2 , pad_token_id = 1 , sep_token_id = None , * args , ** kwargs , ): \"\"\"Create the chatbot from config args and kwargs. Args: model_path: path to scan for model files (weights and configs) max_steps: stop generation at this number of tokens min_length: model can't stop generating text before it's atleast this long in tokens repetition_penalty: probability coef for same tokens to appear multiple times bos_token_id: beginning of sequence token id eos_token_id: end of sequence token id pad_token_id: padding token id sep_token_id: token id for separating sequence into multiple parts \"\"\" super () . __init__ ( * args , ** kwargs ) self . bos_token_id = bos_token_id self . eos_token_id = eos_token_id self . sep_token_id = eos_token_id if sep_token_id is None else sep_token_id self . pad_token_id = pad_token_id sess_options = rt . SessionOptions () sess_options . graph_optimization_level = ( rt . GraphOptimizationLevel . ORT_DISABLE_ALL ) self . encoder_model = rt . InferenceSession ( os . path . join ( model_path , \"encoder_bart.onnx\" ), providers = [ rt . get_available_providers ()[ 0 ]], sess_options = sess_options , ) self . decoder_model = rt . InferenceSession ( os . path . join ( model_path , \"decoder_bart.onnx\" ), providers = [ rt . get_available_providers ()[ 0 ]], sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) added_tokens_path = os . path . join ( model_path , \"added_tokens.txt\" ) if os . path . exists ( added_tokens_path ): with open ( added_tokens_path ) as f : added_tokens = json . load ( f ) added_tokens = [ key for key , _ in sorted ( list ( added_tokens . items ()), key = lambda x : x [ 1 ]) ] self . tokenizer . add_tokens ( added_tokens ) self . special_tokens = { \"bos_token\" : self . tokenizer . decode ( [ bos_token_id ], skip_special_tokens = False ), \"eos_token\" : self . tokenizer . decode ( [ eos_token_id ], skip_special_tokens = False ), \"sep_token\" : self . tokenizer . decode ( [ self . sep_token_id ], skip_special_tokens = False ), \"pad_token\" : self . tokenizer . decode ( [ pad_token_id ], skip_special_tokens = False ), ** { f \"added_token { self . tokenizer . token_to_id ( token ) } \" : token for token in added_tokens }, } self . max_steps = max_steps self . min_length = min_length self . repetition_penalty = repetition_penalty get_special_tokens () Retrun dict of special tokens to be renderable from template. Source code in npc_engine\\services\\chatbot\\bart.py 157 158 159 def get_special_tokens ( self ) -> Dict [ str , str ]: \"\"\"Retrun dict of special tokens to be renderable from template.\"\"\" return self . special_tokens run ( prompt , temperature = 1.0 , topk = None ) Run text generation from given prompt and parameters. Parameters: Name Type Description Default prompt str Fromatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1.0 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text Source code in npc_engine\\services\\chatbot\\bart.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def run ( self , prompt : str , temperature : float = 1.0 , topk : int = None ) -> str : \"\"\"Run text generation from given prompt and parameters. Args: prompt: Fromatted prompt. temperature: Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness topk: If not none selects top n of predictions to sample from during generation. Returns: Generated text \"\"\" tokens = self . tokenizer . encode ( prompt ) total = np . asarray ( tokens . ids , dtype = np . int64 ) . reshape ([ 1 , - 1 ]) total_enc = self . encoder_model . run ( None , { \"input_ids\" : total })[ 0 ] utterance = np . asarray ([ self . eos_token_id ], dtype = np . int64 ) . reshape ([ 1 , 1 ]) for i in range ( self . max_steps ): o = self . decoder_model . run ( None , { \"encoder_hidden_state\" : total_enc , \"decoder_input_ids\" : utterance }, ) logits = o [ 0 ][ 0 , - 1 , :] if i < self . min_length : logits [ self . eos_token_id ] = float ( \"-inf\" ) if topk is not None : ind = np . argpartition ( logits , - topk )[ - topk :] new_logits = np . zeros ( logits . shape ) new_logits [ ind ] = logits [ ind ] logits = new_logits probs = scp . softmax ( logits / temperature , axis = 0 ) token = np . random . choice ( np . arange ( probs . shape [ 0 ]), p = probs ) token = token . reshape ([ 1 , 1 ]) utterance = np . concatenate ([ utterance , token ], axis = 1 ) if token [ 0 , 0 ] == self . eos_token_id : break return self . tokenizer . decode ( utterance [ 0 , :] . tolist (), skip_special_tokens = True ) chatbot_base Module that implements chatbot model API. ChatbotAPI Bases: BaseService Abstract base class for Chatbot models. Source code in npc_engine\\services\\chatbot\\chatbot_base.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 class ChatbotAPI ( BaseService ): \"\"\"Abstract base class for Chatbot models.\"\"\" API_METHODS : List [ str ] = [ \"generate_reply\" , \"get_prompt_template\" , \"get_special_tokens\" , \"get_context_template\" , ] def __init__ ( self , template_string : str , * args , ** kwargs ): \"\"\"Initialize prompt formatting variables. Args: template_string: Template string to be rendered as prompt. \"\"\" super () . __init__ ( * args , ** kwargs ) self . template_string = template_string self . template = Template ( template_string ) self . initialized = True @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"ChatbotAPI\" def generate_reply ( self , context : Dict [ str , Any ], * args , ** kwargs ) -> str : \"\"\"Format the model prompt and generate response. Args: context: Prompt context. *args **kwargs Returns: Text response to a prompt. \"\"\" if not self . initialized : raise AssertionError ( \"Can not generate replies before base Chatbot class was initialized\" ) prompt = self . template . render ( ** context , ** self . get_special_tokens ()) return self . run ( prompt , * args , ** kwargs ) @abstractmethod def run ( self , prompt : str , temperature : float = 1 , topk : int = None ) -> str : \"\"\"Abstract method for concrete implementation of generation. Args: prompt: Fromatted prompt. temperature: Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness topk: If not none selects top n of predictions to sample from during generation. Returns: Generated text \"\"\" return None @abstractmethod def get_special_tokens ( self ) -> Dict [ str , str ]: \"\"\"Return dictionary mapping for special tokens. To be implemented by child class. Can then be used in template string as fields Returns: Dictionary of special tokens \"\"\" return None def get_prompt_template ( self ) -> str : \"\"\"Return prompt template string used to render model prompt. Returns: A template string. \"\"\" return self . template_string def get_context_template ( self ) -> Dict [ str , Any ]: \"\"\"Return context template. Returns: Example context \"\"\" return schema_to_json ( to_json_schema ( infer ( self . template_string ))) __init__ ( template_string , * args , ** kwargs ) Initialize prompt formatting variables. Parameters: Name Type Description Default template_string str Template string to be rendered as prompt. required Source code in npc_engine\\services\\chatbot\\chatbot_base.py 22 23 24 25 26 27 28 29 30 31 def __init__ ( self , template_string : str , * args , ** kwargs ): \"\"\"Initialize prompt formatting variables. Args: template_string: Template string to be rendered as prompt. \"\"\" super () . __init__ ( * args , ** kwargs ) self . template_string = template_string self . template = Template ( template_string ) self . initialized = True generate_reply ( context , * args , ** kwargs ) Format the model prompt and generate response. Parameters: Name Type Description Default context Dict [ str , Any ] Prompt context. required Returns: Type Description str Text response to a prompt. Source code in npc_engine\\services\\chatbot\\chatbot_base.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def generate_reply ( self , context : Dict [ str , Any ], * args , ** kwargs ) -> str : \"\"\"Format the model prompt and generate response. Args: context: Prompt context. *args **kwargs Returns: Text response to a prompt. \"\"\" if not self . initialized : raise AssertionError ( \"Can not generate replies before base Chatbot class was initialized\" ) prompt = self . template . render ( ** context , ** self . get_special_tokens ()) return self . run ( prompt , * args , ** kwargs ) get_api_name () classmethod Get the API name. Source code in npc_engine\\services\\chatbot\\chatbot_base.py 33 34 35 36 @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"ChatbotAPI\" get_context_template () Return context template. Returns: Type Description Dict [ str , Any ] Example context Source code in npc_engine\\services\\chatbot\\chatbot_base.py 90 91 92 93 94 95 96 def get_context_template ( self ) -> Dict [ str , Any ]: \"\"\"Return context template. Returns: Example context \"\"\" return schema_to_json ( to_json_schema ( infer ( self . template_string ))) get_prompt_template () Return prompt template string used to render model prompt. Returns: Type Description str A template string. Source code in npc_engine\\services\\chatbot\\chatbot_base.py 82 83 84 85 86 87 88 def get_prompt_template ( self ) -> str : \"\"\"Return prompt template string used to render model prompt. Returns: A template string. \"\"\" return self . template_string get_special_tokens () abstractmethod Return dictionary mapping for special tokens. To be implemented by child class. Can then be used in template string as fields Returns: Type Description Dict [ str , str ] Dictionary of special tokens Source code in npc_engine\\services\\chatbot\\chatbot_base.py 71 72 73 74 75 76 77 78 79 80 @abstractmethod def get_special_tokens ( self ) -> Dict [ str , str ]: \"\"\"Return dictionary mapping for special tokens. To be implemented by child class. Can then be used in template string as fields Returns: Dictionary of special tokens \"\"\" return None run ( prompt , temperature = 1 , topk = None ) abstractmethod Abstract method for concrete implementation of generation. Parameters: Name Type Description Default prompt str Fromatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text Source code in npc_engine\\services\\chatbot\\chatbot_base.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 @abstractmethod def run ( self , prompt : str , temperature : float = 1 , topk : int = None ) -> str : \"\"\"Abstract method for concrete implementation of generation. Args: prompt: Fromatted prompt. temperature: Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness topk: If not none selects top n of predictions to sample from during generation. Returns: Generated text \"\"\" return None hf_chatbot BART based chatbot implementation. HfChatbot Bases: ChatbotAPI Chatbot that uses Huggingface transformer architectures. ONNX export of Huggingface transformer is required (see https://huggingface.co/docs/transformers/serialization). Features seq2seq-lm, causal-lm, seq2seq-lm-with-past, causal-lm-with-past are supported Source code in npc_engine\\services\\chatbot\\hf_chatbot.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 class HfChatbot ( ChatbotAPI ): \"\"\"Chatbot that uses Huggingface transformer architectures. ONNX export of Huggingface transformer is required (see https://huggingface.co/docs/transformers/serialization). Features seq2seq-lm, causal-lm, seq2seq-lm-with-past, causal-lm-with-past are supported \"\"\" def __init__ ( self , model_path , max_length = 100 , min_length = 2 , repetition_penalty = 1 , * args , ** kwargs , ): \"\"\"Create the chatbot from config args and kwargs. Args: model_path: path to scan for model files (weights and configs) max_length: stop generation at this number of tokens min_length: model can't stop generating text before it's atleast this long in tokens repetition_penalty: probability coef for same tokens to appear multiple times \"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = rt . GraphOptimizationLevel . ORT_ENABLE_ALL self . model = rt . InferenceSession ( os . path . join ( model_path , \"model.onnx\" ), providers = [ rt . get_available_providers ()[ 0 ]], sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) self . max_steps = max_length self . min_length = min_length self . repetition_penalty = repetition_penalty special_tokens_map_path = os . path . join ( model_path , \"special_tokens_map.json\" ) with open ( special_tokens_map_path , \"r\" ) as f : self . special_tokens = json . load ( f ) self . eos_token_id = self . tokenizer . encode ( self . special_tokens [ \"eos_token\" ]) . ids [ 0 ] self . model_inputs = self . model . get_inputs () self . is_encdec = ( len ([ i . name for i in self . model_inputs if \"decoder\" in i . name ]) > 0 ) self . with_past = ( len ([ i . name for i in self . model_inputs if \"past_key_values\" in i . name ]) > 0 ) self . shape_dict = { \"batch\" : 1 , \"past_encoder_sequence\" : 0 , \"past_decoder_sequence\" : 0 , \"past_sequence + sequence\" : 0 , } self . dtypes = { i . name : DTYPE_MAP [ i . type ] for i in self . model_inputs } def run ( self , prompt : str , temperature : float = 1.0 , topk : int = None ) -> str : \"\"\"Run text generation from given prompt and parameters. Args: prompt: Fromatted prompt. temperature: Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness topk: If not none selects top n of predictions to sample from during generation. Returns: Generated text \"\"\" inputs = self . create_starter_inputs ( prompt ) utterance = [] for i in range ( self . max_steps ): o = self . model . run ( None , inputs ,) logit = o [ 0 ][ 0 , - 1 , :] if i < self . min_length : logit [ self . eos_token_id ] = float ( \"-inf\" ) token = self . decode_logit ( logit , temperature , topk ) utterance . append ( token ) result_dict = { outp . name : o [ i ] for i , outp in enumerate ( self . model . get_outputs ()) } inputs = self . update_inputs_with_results ( inputs , result_dict , token ) if token == self . eos_token_id : break return self . tokenizer . decode ( utterance , skip_special_tokens = True ) def create_starter_inputs ( self , prompt : str = \"\" ) -> Dict [ str , Any ]: \"\"\"Create starter inputs for the model. Args: prompt: Prompt to start generation from. Returns: Dict of inputs to the model \"\"\" tokens = self . tokenizer . encode ( prompt ) . ids inputs = {} if self . is_encdec : prompt_start = tokens [ - 1 :] inputs [ \"input_ids\" ] = np . asarray ( tokens [: - 1 ], dtype = self . dtypes [ \"input_ids\" ] ) . reshape ([ 1 , - 1 ]) inputs [ \"decoder_input_ids\" ] = np . asarray ( prompt_start , dtype = self . dtypes [ \"decoder_input_ids\" ] ) . reshape ([ 1 , - 1 ]) inputs [ \"attention_mask\" ] = np . ones ( [ 1 , 6 ], dtype = self . dtypes [ \"attention_mask\" ] ) inputs [ \"decoder_attention_mask\" ] = np . ones ( [ 1 , 3 ], dtype = self . dtypes [ \"decoder_attention_mask\" ] ) else : inputs [ \"input_ids\" ] = np . asarray ( tokens , dtype = self . dtypes [ \"input_ids\" ] ) . reshape ([ 1 , - 1 ]) inputs [ \"attention_mask\" ] = np . ones_like ( inputs [ \"input_ids\" ], dtype = self . dtypes [ \"attention_mask\" ] ) if self . with_past : for i in self . model_inputs : if \"past_key_values\" in i . name : shape_tuple = [ self . shape_dict . get ( dim , dim ) for dim in i . shape ] inputs [ i . name ] = np . empty ( shape_tuple , dtype = self . dtypes [ i . name ]) return inputs def decode_logit ( self , logit : np . ndarray , temperature : float , topk : int ) -> int : \"\"\"Decode logit to token. Args: logit: Logit to decode of shape (vocab_size,) Returns: Decoded token of shape \"\"\" if topk is not None : ind = np . argpartition ( logit , - topk )[ - topk :] new_logits = np . zeros ( logit . shape ) new_logits [ ind ] = logit [ ind ] logit = new_logits probs = scp . softmax ( logit / temperature , axis = 0 ) token = np . random . choice ( np . arange ( probs . shape [ 0 ]), p = probs ) token = token . ravel ()[ 0 ] return token def update_inputs_with_results ( self , inputs : Dict [ str , np . ndarray ], results : Dict [ str , np . ndarray ], decoded_token : int , ) -> Dict [ str , np . ndarray ]: \"\"\"Update inputs with results from model. Args: inputs: Inputs to the model results: Results from the model Returns: Updated inputs Finished generation \"\"\" ids_name = \"decoder_input_ids\" if self . is_encdec else \"input_ids\" att_mask_name = \"decoder_attention_mask\" if self . is_encdec else \"attention_mask\" if self . with_past : inputs [ ids_name ] = np . asarray ( [ decoded_token ], dtype = self . dtypes [ ids_name ] ) . reshape ([ 1 , - 1 ]) inputs [ att_mask_name ] = np . ones ( [ 1 , inputs [ att_mask_name ] . shape [ - 1 ] + 1 ], dtype = self . dtypes [ att_mask_name ], ) for inp in self . model_inputs : if \"past_key_values\" in inp . name : inputs [ inp . name ] = results [ inp . name . replace ( \"past_key_values\" , \"present\" ) ] if self . is_encdec : inputs . pop ( \"input_ids\" , None ) else : decoder_input_ids = inputs [ ids_name ] decoder_attention_mask = inputs [ att_mask_name ] decoder_input_ids = np . concatenate ( [ decoder_input_ids , np . asarray ([[ decoded_token ]], dtype = np . int32 )], axis = 1 , ) decoder_attention_mask = np . ones_like ( decoder_input_ids , dtype = self . dtypes [ att_mask_name ] ) inputs [ ids_name ] = decoder_input_ids inputs [ att_mask_name ] = decoder_attention_mask return inputs def get_special_tokens ( self ) -> Dict [ str , str ]: \"\"\"Retrun dict of special tokens to be renderable from template.\"\"\" return self . special_tokens __init__ ( model_path , max_length = 100 , min_length = 2 , repetition_penalty = 1 , * args , ** kwargs ) Create the chatbot from config args and kwargs. Parameters: Name Type Description Default model_path path to scan for model files (weights and configs) required max_length stop generation at this number of tokens 100 min_length model can't stop generating text before it's atleast this long in tokens 2 repetition_penalty probability coef for same tokens to appear multiple times 1 Source code in npc_engine\\services\\chatbot\\hf_chatbot.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def __init__ ( self , model_path , max_length = 100 , min_length = 2 , repetition_penalty = 1 , * args , ** kwargs , ): \"\"\"Create the chatbot from config args and kwargs. Args: model_path: path to scan for model files (weights and configs) max_length: stop generation at this number of tokens min_length: model can't stop generating text before it's atleast this long in tokens repetition_penalty: probability coef for same tokens to appear multiple times \"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = rt . GraphOptimizationLevel . ORT_ENABLE_ALL self . model = rt . InferenceSession ( os . path . join ( model_path , \"model.onnx\" ), providers = [ rt . get_available_providers ()[ 0 ]], sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) self . max_steps = max_length self . min_length = min_length self . repetition_penalty = repetition_penalty special_tokens_map_path = os . path . join ( model_path , \"special_tokens_map.json\" ) with open ( special_tokens_map_path , \"r\" ) as f : self . special_tokens = json . load ( f ) self . eos_token_id = self . tokenizer . encode ( self . special_tokens [ \"eos_token\" ]) . ids [ 0 ] self . model_inputs = self . model . get_inputs () self . is_encdec = ( len ([ i . name for i in self . model_inputs if \"decoder\" in i . name ]) > 0 ) self . with_past = ( len ([ i . name for i in self . model_inputs if \"past_key_values\" in i . name ]) > 0 ) self . shape_dict = { \"batch\" : 1 , \"past_encoder_sequence\" : 0 , \"past_decoder_sequence\" : 0 , \"past_sequence + sequence\" : 0 , } self . dtypes = { i . name : DTYPE_MAP [ i . type ] for i in self . model_inputs } create_starter_inputs ( prompt = '' ) Create starter inputs for the model. Parameters: Name Type Description Default prompt str Prompt to start generation from. '' Returns: Type Description Dict [ str , Any ] Dict of inputs to the model Source code in npc_engine\\services\\chatbot\\hf_chatbot.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def create_starter_inputs ( self , prompt : str = \"\" ) -> Dict [ str , Any ]: \"\"\"Create starter inputs for the model. Args: prompt: Prompt to start generation from. Returns: Dict of inputs to the model \"\"\" tokens = self . tokenizer . encode ( prompt ) . ids inputs = {} if self . is_encdec : prompt_start = tokens [ - 1 :] inputs [ \"input_ids\" ] = np . asarray ( tokens [: - 1 ], dtype = self . dtypes [ \"input_ids\" ] ) . reshape ([ 1 , - 1 ]) inputs [ \"decoder_input_ids\" ] = np . asarray ( prompt_start , dtype = self . dtypes [ \"decoder_input_ids\" ] ) . reshape ([ 1 , - 1 ]) inputs [ \"attention_mask\" ] = np . ones ( [ 1 , 6 ], dtype = self . dtypes [ \"attention_mask\" ] ) inputs [ \"decoder_attention_mask\" ] = np . ones ( [ 1 , 3 ], dtype = self . dtypes [ \"decoder_attention_mask\" ] ) else : inputs [ \"input_ids\" ] = np . asarray ( tokens , dtype = self . dtypes [ \"input_ids\" ] ) . reshape ([ 1 , - 1 ]) inputs [ \"attention_mask\" ] = np . ones_like ( inputs [ \"input_ids\" ], dtype = self . dtypes [ \"attention_mask\" ] ) if self . with_past : for i in self . model_inputs : if \"past_key_values\" in i . name : shape_tuple = [ self . shape_dict . get ( dim , dim ) for dim in i . shape ] inputs [ i . name ] = np . empty ( shape_tuple , dtype = self . dtypes [ i . name ]) return inputs decode_logit ( logit , temperature , topk ) Decode logit to token. Parameters: Name Type Description Default logit np . ndarray Logit to decode of shape (vocab_size,) required Returns: Type Description int Decoded token of shape Source code in npc_engine\\services\\chatbot\\hf_chatbot.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def decode_logit ( self , logit : np . ndarray , temperature : float , topk : int ) -> int : \"\"\"Decode logit to token. Args: logit: Logit to decode of shape (vocab_size,) Returns: Decoded token of shape \"\"\" if topk is not None : ind = np . argpartition ( logit , - topk )[ - topk :] new_logits = np . zeros ( logit . shape ) new_logits [ ind ] = logit [ ind ] logit = new_logits probs = scp . softmax ( logit / temperature , axis = 0 ) token = np . random . choice ( np . arange ( probs . shape [ 0 ]), p = probs ) token = token . ravel ()[ 0 ] return token get_special_tokens () Retrun dict of special tokens to be renderable from template. Source code in npc_engine\\services\\chatbot\\hf_chatbot.py 213 214 215 def get_special_tokens ( self ) -> Dict [ str , str ]: \"\"\"Retrun dict of special tokens to be renderable from template.\"\"\" return self . special_tokens run ( prompt , temperature = 1.0 , topk = None ) Run text generation from given prompt and parameters. Parameters: Name Type Description Default prompt str Fromatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1.0 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text Source code in npc_engine\\services\\chatbot\\hf_chatbot.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def run ( self , prompt : str , temperature : float = 1.0 , topk : int = None ) -> str : \"\"\"Run text generation from given prompt and parameters. Args: prompt: Fromatted prompt. temperature: Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness topk: If not none selects top n of predictions to sample from during generation. Returns: Generated text \"\"\" inputs = self . create_starter_inputs ( prompt ) utterance = [] for i in range ( self . max_steps ): o = self . model . run ( None , inputs ,) logit = o [ 0 ][ 0 , - 1 , :] if i < self . min_length : logit [ self . eos_token_id ] = float ( \"-inf\" ) token = self . decode_logit ( logit , temperature , topk ) utterance . append ( token ) result_dict = { outp . name : o [ i ] for i , outp in enumerate ( self . model . get_outputs ()) } inputs = self . update_inputs_with_results ( inputs , result_dict , token ) if token == self . eos_token_id : break return self . tokenizer . decode ( utterance , skip_special_tokens = True ) update_inputs_with_results ( inputs , results , decoded_token ) Update inputs with results from model. Parameters: Name Type Description Default inputs Dict [ str , np . ndarray ] Inputs to the model required results Dict [ str , np . ndarray ] Results from the model required Returns: Type Description Dict [ str , np . ndarray ] Updated inputs Dict [ str , np . ndarray ] Finished generation Source code in npc_engine\\services\\chatbot\\hf_chatbot.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def update_inputs_with_results ( self , inputs : Dict [ str , np . ndarray ], results : Dict [ str , np . ndarray ], decoded_token : int , ) -> Dict [ str , np . ndarray ]: \"\"\"Update inputs with results from model. Args: inputs: Inputs to the model results: Results from the model Returns: Updated inputs Finished generation \"\"\" ids_name = \"decoder_input_ids\" if self . is_encdec else \"input_ids\" att_mask_name = \"decoder_attention_mask\" if self . is_encdec else \"attention_mask\" if self . with_past : inputs [ ids_name ] = np . asarray ( [ decoded_token ], dtype = self . dtypes [ ids_name ] ) . reshape ([ 1 , - 1 ]) inputs [ att_mask_name ] = np . ones ( [ 1 , inputs [ att_mask_name ] . shape [ - 1 ] + 1 ], dtype = self . dtypes [ att_mask_name ], ) for inp in self . model_inputs : if \"past_key_values\" in inp . name : inputs [ inp . name ] = results [ inp . name . replace ( \"past_key_values\" , \"present\" ) ] if self . is_encdec : inputs . pop ( \"input_ids\" , None ) else : decoder_input_ids = inputs [ ids_name ] decoder_attention_mask = inputs [ att_mask_name ] decoder_input_ids = np . concatenate ( [ decoder_input_ids , np . asarray ([[ decoded_token ]], dtype = np . int32 )], axis = 1 , ) decoder_attention_mask = np . ones_like ( decoder_input_ids , dtype = self . dtypes [ att_mask_name ] ) inputs [ ids_name ] = decoder_input_ids inputs [ att_mask_name ] = decoder_attention_mask return inputs sequence_classifier Sequence classification API module. hf_classifier Module that implements Huggingface transformers classification. HfClassifier Bases: SequenceClassifierAPI Huggingface transformers sequence classification. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with sequence-classification feature. Also requires saved tokenizer with huggingface tokenizers. Source code in npc_engine\\services\\sequence_classifier\\hf_classifier.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class HfClassifier ( SequenceClassifierAPI ): \"\"\"Huggingface transformers sequence classification. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with sequence-classification feature. Also requires saved tokenizer with huggingface tokenizers. \"\"\" def __init__ ( self , model_path : str , * args , ** kwargs ): \"\"\"Create and load biencoder model for semantic similarity. Args: model_path: A path where model config and weights are. \"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_ALL self . model = rt . InferenceSession ( os . path . join ( model_path , \"model.onnx\" ), providers = [ rt . get_available_providers ()[ 0 ]], sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) input_names = [ inp . name for inp in self . model . get_inputs ()] self . token_type_support = \"token_type_ids\" in input_names special_tokens_map_path = os . path . join ( model_path , \"special_tokens_map.json\" ) with open ( special_tokens_map_path , \"r\" ) as f : self . special_tokens = json . load ( f ) self . pad_token_id = self . tokenizer . encode ( self . special_tokens [ \"pad_token\" ]) . ids [ 0 ] self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) self . tokenizer . enable_padding ( direction = \"right\" , pad_id = self . pad_token_id , pad_type_id = 0 , pad_token = self . tokenizer . decode ( [ self . pad_token_id ], skip_special_tokens = False ), length = None , pad_to_multiple_of = None , ) self . tests = {} def compute_scores_batch ( self , texts : List [ Union [ str , Tuple [ str , str ]]] ) -> np . ndarray : \"\"\"Compute scores. Args: texts: Sentence to embed Returns: scores: Scores for each text \"\"\" tokenized = self . tokenizer . encode_batch ( texts ) ids = np . stack ([ np . asarray ( encoding . ids ) for encoding in tokenized ]) . astype ( np . int64 ) attention_mask = np . stack ( [ np . asarray ( encoding . attention_mask ) for encoding in tokenized ] ) . astype ( np . int64 ) if not self . token_type_support : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask } else : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : np . stack ( [ np . asarray ( encoding . type_ids ) for encoding in tokenized ] ) . astype ( np . int64 ), } outp = self . model . run ( None , input_dict ) return outp [ 0 ] __init__ ( model_path , * args , ** kwargs ) Create and load biencoder model for semantic similarity. Parameters: Name Type Description Default model_path str A path where model config and weights are. required Source code in npc_engine\\services\\sequence_classifier\\hf_classifier.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , model_path : str , * args , ** kwargs ): \"\"\"Create and load biencoder model for semantic similarity. Args: model_path: A path where model config and weights are. \"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_ALL self . model = rt . InferenceSession ( os . path . join ( model_path , \"model.onnx\" ), providers = [ rt . get_available_providers ()[ 0 ]], sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) input_names = [ inp . name for inp in self . model . get_inputs ()] self . token_type_support = \"token_type_ids\" in input_names special_tokens_map_path = os . path . join ( model_path , \"special_tokens_map.json\" ) with open ( special_tokens_map_path , \"r\" ) as f : self . special_tokens = json . load ( f ) self . pad_token_id = self . tokenizer . encode ( self . special_tokens [ \"pad_token\" ]) . ids [ 0 ] self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) self . tokenizer . enable_padding ( direction = \"right\" , pad_id = self . pad_token_id , pad_type_id = 0 , pad_token = self . tokenizer . decode ( [ self . pad_token_id ], skip_special_tokens = False ), length = None , pad_to_multiple_of = None , ) self . tests = {} compute_scores_batch ( texts ) Compute scores. Parameters: Name Type Description Default texts List [ Union [ str , Tuple [ str , str ]]] Sentence to embed required Returns: Name Type Description scores np . ndarray Scores for each text Source code in npc_engine\\services\\sequence_classifier\\hf_classifier.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def compute_scores_batch ( self , texts : List [ Union [ str , Tuple [ str , str ]]] ) -> np . ndarray : \"\"\"Compute scores. Args: texts: Sentence to embed Returns: scores: Scores for each text \"\"\" tokenized = self . tokenizer . encode_batch ( texts ) ids = np . stack ([ np . asarray ( encoding . ids ) for encoding in tokenized ]) . astype ( np . int64 ) attention_mask = np . stack ( [ np . asarray ( encoding . attention_mask ) for encoding in tokenized ] ) . astype ( np . int64 ) if not self . token_type_support : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask } else : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : np . stack ( [ np . asarray ( encoding . type_ids ) for encoding in tokenized ] ) . astype ( np . int64 ), } outp = self . model . run ( None , input_dict ) return outp [ 0 ] sequence_classifier_base Module that implements sequence classification API. SequenceClassifierAPI Bases: BaseService Abstract base class for text classification models. Source code in npc_engine\\services\\sequence_classifier\\sequence_classifier_base.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class SequenceClassifierAPI ( BaseService ): \"\"\"Abstract base class for text classification models.\"\"\" API_METHODS : List [ str ] = [ \"classify\" ] def __init__ ( self , cache_size = 0 , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True self . cache = NumpyLRUCache ( cache_size ) @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"SequenceClassifierAPI\" def classify ( self , texts : List [ str ]) -> List [ List [ float ]]: \"\"\"Classify a list of texts. Args: texts: A list of texts to classify. Returns: List of scores for each text. \"\"\" texts = [ row if isinstance ( row , str ) else tuple ( row ) for row in texts ] scores = self . cache . cache_compute ( texts , lambda values : self . compute_scores_batch ( values ) ) return scores . tolist () @abstractmethod def compute_scores_batch ( self , texts : List [ str ]) -> np . ndarray : \"\"\"Compute scores for a list of texts. Args: texts: A list of texts to compute scores for. Returns: List of scores for each text. \"\"\" pass __init__ ( cache_size = 0 , * args , ** kwargs ) Empty initialization method for API to be similar to other model base classes. Source code in npc_engine\\services\\sequence_classifier\\sequence_classifier_base.py 15 16 17 18 19 def __init__ ( self , cache_size = 0 , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True self . cache = NumpyLRUCache ( cache_size ) classify ( texts ) Classify a list of texts. Parameters: Name Type Description Default texts List [ str ] A list of texts to classify. required Returns: Type Description List [ List [ float ]] List of scores for each text. Source code in npc_engine\\services\\sequence_classifier\\sequence_classifier_base.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def classify ( self , texts : List [ str ]) -> List [ List [ float ]]: \"\"\"Classify a list of texts. Args: texts: A list of texts to classify. Returns: List of scores for each text. \"\"\" texts = [ row if isinstance ( row , str ) else tuple ( row ) for row in texts ] scores = self . cache . cache_compute ( texts , lambda values : self . compute_scores_batch ( values ) ) return scores . tolist () compute_scores_batch ( texts ) abstractmethod Compute scores for a list of texts. Parameters: Name Type Description Default texts List [ str ] A list of texts to compute scores for. required Returns: Type Description np . ndarray List of scores for each text. Source code in npc_engine\\services\\sequence_classifier\\sequence_classifier_base.py 41 42 43 44 45 46 47 48 49 50 51 @abstractmethod def compute_scores_batch ( self , texts : List [ str ]) -> np . ndarray : \"\"\"Compute scores for a list of texts. Args: texts: A list of texts to compute scores for. Returns: List of scores for each text. \"\"\" pass get_api_name () classmethod Get the API name. Source code in npc_engine\\services\\sequence_classifier\\sequence_classifier_base.py 21 22 23 24 @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"SequenceClassifierAPI\" similarity Similarity model implementations. This module implements specific models and wraps them under the common interface for loading and inference. Example from npc_engine.models.similarity import SimilarityAPI model = SimilarityAPI.load(\"path/to/model_dir\") model.compare(\"hello\", [\"Hello, world!\"]) similarity_base Module that implements semantic similarity model API. SimilarityAPI Bases: BaseService Abstract base class for text similarity models. Source code in npc_engine\\services\\similarity\\similarity_base.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class SimilarityAPI ( BaseService ): \"\"\"Abstract base class for text similarity models.\"\"\" API_METHODS : List [ str ] = [ \"compare\" , \"cache\" ] def __init__ ( self , cache_size = 0 , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True self . lru_cache = NumpyLRUCache ( cache_size ) @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"SimilarityAPI\" def compare ( self , query : str , context : List [ str ]) -> List [ float ]: \"\"\"Compare a query to the context. Args: query: A sentence to compare. context: A list of sentences to compare to. This will be cached if caching is enabled Returns: List of similarities \"\"\" embedding_a = self . compute_embedding ( query ) embedding_b = self . lru_cache . cache_compute ( context , lambda values : self . compute_embedding_batch ( values ) ) similarities = self . metric ( embedding_a , embedding_b ) return similarities . tolist () def cache ( self , context : List [ str ]): \"\"\"Cache embeddings of given sequences. Args: context: A list of sentences to cache. \"\"\" self . lru_cache . cache_compute ( context , lambda values : self . compute_embedding_batch ( values ) ) @abstractmethod def compute_embedding_batch ( self , lines : List [ str ]) -> np . ndarray : \"\"\"Compute line embeddings in batch. Args: lines: List of sentences to embed Returns: Embedding batch of shape (batch_size, embedding_size) \"\"\" return None @abstractmethod def compute_embedding ( self , line : str ) -> np . ndarray : \"\"\"Compute sentence embedding. Args: line: Sentence to embed Returns: Embedding of shape (1, embedding_size) \"\"\" return None @abstractmethod def metric ( self , embedding_a : np . ndarray , embedding_b : np . ndarray ) -> np . ndarray : \"\"\"Compute distance between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Args: embedding_a: Embedding of shape (1 or batch_size, embedding_size) embedding_b: Embedding of shape (1 or batch_size, embedding_size) Returns: Vector of distances (batch_size or 1,) \"\"\" return None __init__ ( cache_size = 0 , * args , ** kwargs ) Empty initialization method for API to be similar to other model base classes. Source code in npc_engine\\services\\similarity\\similarity_base.py 15 16 17 18 19 def __init__ ( self , cache_size = 0 , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True self . lru_cache = NumpyLRUCache ( cache_size ) cache ( context ) Cache embeddings of given sequences. Parameters: Name Type Description Default context List [ str ] A list of sentences to cache. required Source code in npc_engine\\services\\similarity\\similarity_base.py 43 44 45 46 47 48 49 50 51 def cache ( self , context : List [ str ]): \"\"\"Cache embeddings of given sequences. Args: context: A list of sentences to cache. \"\"\" self . lru_cache . cache_compute ( context , lambda values : self . compute_embedding_batch ( values ) ) compare ( query , context ) Compare a query to the context. Parameters: Name Type Description Default query str A sentence to compare. required context List [ str ] A list of sentences to compare to. This will be cached if caching is enabled required Returns: Type Description List [ float ] List of similarities Source code in npc_engine\\services\\similarity\\similarity_base.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def compare ( self , query : str , context : List [ str ]) -> List [ float ]: \"\"\"Compare a query to the context. Args: query: A sentence to compare. context: A list of sentences to compare to. This will be cached if caching is enabled Returns: List of similarities \"\"\" embedding_a = self . compute_embedding ( query ) embedding_b = self . lru_cache . cache_compute ( context , lambda values : self . compute_embedding_batch ( values ) ) similarities = self . metric ( embedding_a , embedding_b ) return similarities . tolist () compute_embedding ( line ) abstractmethod Compute sentence embedding. Parameters: Name Type Description Default line str Sentence to embed required Returns: Type Description np . ndarray Embedding of shape (1, embedding_size) Source code in npc_engine\\services\\similarity\\similarity_base.py 65 66 67 68 69 70 71 72 73 74 75 @abstractmethod def compute_embedding ( self , line : str ) -> np . ndarray : \"\"\"Compute sentence embedding. Args: line: Sentence to embed Returns: Embedding of shape (1, embedding_size) \"\"\" return None compute_embedding_batch ( lines ) abstractmethod Compute line embeddings in batch. Parameters: Name Type Description Default lines List [ str ] List of sentences to embed required Returns: Type Description np . ndarray Embedding batch of shape (batch_size, embedding_size) Source code in npc_engine\\services\\similarity\\similarity_base.py 53 54 55 56 57 58 59 60 61 62 63 @abstractmethod def compute_embedding_batch ( self , lines : List [ str ]) -> np . ndarray : \"\"\"Compute line embeddings in batch. Args: lines: List of sentences to embed Returns: Embedding batch of shape (batch_size, embedding_size) \"\"\" return None get_api_name () classmethod Get the API name. Source code in npc_engine\\services\\similarity\\similarity_base.py 21 22 23 24 @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"SimilarityAPI\" metric ( embedding_a , embedding_b ) abstractmethod Compute distance between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Parameters: Name Type Description Default embedding_a np . ndarray Embedding of shape (1 or batch_size, embedding_size) required embedding_b np . ndarray Embedding of shape (1 or batch_size, embedding_size) required Returns: Type Description np . ndarray Vector of distances (batch_size or 1,) Source code in npc_engine\\services\\similarity\\similarity_base.py 77 78 79 80 81 82 83 84 85 86 87 88 89 @abstractmethod def metric ( self , embedding_a : np . ndarray , embedding_b : np . ndarray ) -> np . ndarray : \"\"\"Compute distance between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Args: embedding_a: Embedding of shape (1 or batch_size, embedding_size) embedding_b: Embedding of shape (1 or batch_size, embedding_size) Returns: Vector of distances (batch_size or 1,) \"\"\" return None similarity_transformers Module that implements Huggingface transformers semantic similarity. TransformerSemanticSimilarity Bases: SimilarityAPI Huggingface transformers semantic similarity. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with biencoder architecture. Also requires a tokenizer.json with huggingface tokenizers definition. model.onnx spec: - inputs: `input_ids` of shape `(batch_size, sequence)` `attention_mask` of shape `(batch_size, sequence)` (Optional) `input_type_ids` of shape `(batch_size, sequence)` - outputs: `token_embeddings` of shape `(batch_size, sequence, hidden_size)` Source code in npc_engine\\services\\similarity\\similarity_transformers.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 class TransformerSemanticSimilarity ( SimilarityAPI ): \"\"\"Huggingface transformers semantic similarity. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with biencoder architecture. Also requires a tokenizer.json with huggingface tokenizers definition. model.onnx spec: - inputs: `input_ids` of shape `(batch_size, sequence)` `attention_mask` of shape `(batch_size, sequence)` (Optional) `input_type_ids` of shape `(batch_size, sequence)` - outputs: `token_embeddings` of shape `(batch_size, sequence, hidden_size)` \"\"\" def __init__ ( self , model_path : str , metric : str = \"dot\" , * args , ** kwargs ): \"\"\"Create and load biencoder model for semantic similarity. Args: model_path: A path where model config and weights are metric: distance to compute semantic similarity \"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_ALL self . model = rt . InferenceSession ( os . path . join ( model_path , \"model.onnx\" ), providers = [ rt . get_available_providers ()[ 0 ]], sess_options = sess_options , ) input_names = [ inp . name for inp in self . model . get_inputs ()] self . token_type_support = \"token_type_ids\" in input_names special_tokens_map_path = os . path . join ( model_path , \"special_tokens_map.json\" ) with open ( special_tokens_map_path , \"r\" ) as f : self . special_tokens = json . load ( f ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) self . pad_token_id = self . tokenizer . encode ( self . special_tokens [ \"pad_token\" ]) . ids [ 0 ] self . tokenizer . enable_padding ( direction = \"right\" , pad_id = self . pad_token_id , pad_type_id = 0 , pad_token = self . tokenizer . decode ( [ self . pad_token_id ], skip_special_tokens = False ), length = None , pad_to_multiple_of = None , ) self . tests = {} self . metric_type = metric def compute_embedding ( self , line : str ) -> np . ndarray : \"\"\"Compute sentence embedding. Args: line: Sentence to embed Returns: Embedding of shape (1, embedding_size) \"\"\" ids = ( np . asarray ( self . tokenizer . encode ( line ) . ids ) . reshape ([ 1 , - 1 ]) . astype ( np . int64 ) ) attention_mask = np . ones_like ( ids ) if not self . token_type_support : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask } else : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : np . zeros_like ( ids ), } outp = self . model . run ( None , input_dict ) return self . _mean_pooling ( outp , attention_mask ) def compute_embedding_batch ( self , lines : List [ str ]) -> np . ndarray : \"\"\"Compute line embeddings in batch. Args: lines: List of sentences to embed Returns: Embedding batch of shape (batch_size, embedding_size) \"\"\" tokenized = self . tokenizer . encode_batch ( lines ) ids = np . stack ([ np . asarray ( encoding . ids ) for encoding in tokenized ]) . astype ( np . int64 ) attention_mask = np . stack ( [ np . asarray ( encoding . attention_mask ) for encoding in tokenized ] ) . astype ( np . int64 ) if not self . token_type_support : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask } else : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : np . stack ( [ np . asarray ( encoding . type_ids ) for encoding in tokenized ] ) . astype ( np . int64 ), } outp = self . model . run ( None , input_dict ) return self . _mean_pooling ( outp , attention_mask ) def _mean_pooling ( self , model_output , attention_mask ): token_embeddings = model_output [ 0 ] attention_mask = np . expand_dims ( attention_mask , - 1 ) sum_embeddings = np . sum ( token_embeddings * attention_mask , 1 ) sum_mask = np . clip ( attention_mask . sum ( 1 ), a_min = 1e-9 , a_max = None ) return sum_embeddings / sum_mask def metric ( self , embedding_a : np . ndarray , embedding_b : np . ndarray ) -> np . ndarray : \"\"\"Similarity between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Args: embedding_a: Embedding of shape (1 or batch_size, embedding_size) embedding_b: Embedding of shape (1 or batch_size, embedding_size) Returns: Vector of distances (batch_size or 1,) \"\"\" if self . metric_type == \"dot\" : return - np . dot ( embedding_a , embedding_b . T ) . squeeze ( 0 ) elif self . metric_type == \"cosine\" : return 1 - cdist ( embedding_a , embedding_b , metric = \"cosine\" ) . squeeze ( 0 ) __init__ ( model_path , metric = 'dot' , * args , ** kwargs ) Create and load biencoder model for semantic similarity. Parameters: Name Type Description Default model_path str A path where model config and weights are required metric str distance to compute semantic similarity 'dot' Source code in npc_engine\\services\\similarity\\similarity_transformers.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , model_path : str , metric : str = \"dot\" , * args , ** kwargs ): \"\"\"Create and load biencoder model for semantic similarity. Args: model_path: A path where model config and weights are metric: distance to compute semantic similarity \"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_ALL self . model = rt . InferenceSession ( os . path . join ( model_path , \"model.onnx\" ), providers = [ rt . get_available_providers ()[ 0 ]], sess_options = sess_options , ) input_names = [ inp . name for inp in self . model . get_inputs ()] self . token_type_support = \"token_type_ids\" in input_names special_tokens_map_path = os . path . join ( model_path , \"special_tokens_map.json\" ) with open ( special_tokens_map_path , \"r\" ) as f : self . special_tokens = json . load ( f ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) self . pad_token_id = self . tokenizer . encode ( self . special_tokens [ \"pad_token\" ]) . ids [ 0 ] self . tokenizer . enable_padding ( direction = \"right\" , pad_id = self . pad_token_id , pad_type_id = 0 , pad_token = self . tokenizer . decode ( [ self . pad_token_id ], skip_special_tokens = False ), length = None , pad_to_multiple_of = None , ) self . tests = {} self . metric_type = metric compute_embedding ( line ) Compute sentence embedding. Parameters: Name Type Description Default line str Sentence to embed required Returns: Type Description np . ndarray Embedding of shape (1, embedding_size) Source code in npc_engine\\services\\similarity\\similarity_transformers.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def compute_embedding ( self , line : str ) -> np . ndarray : \"\"\"Compute sentence embedding. Args: line: Sentence to embed Returns: Embedding of shape (1, embedding_size) \"\"\" ids = ( np . asarray ( self . tokenizer . encode ( line ) . ids ) . reshape ([ 1 , - 1 ]) . astype ( np . int64 ) ) attention_mask = np . ones_like ( ids ) if not self . token_type_support : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask } else : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : np . zeros_like ( ids ), } outp = self . model . run ( None , input_dict ) return self . _mean_pooling ( outp , attention_mask ) compute_embedding_batch ( lines ) Compute line embeddings in batch. Parameters: Name Type Description Default lines List [ str ] List of sentences to embed required Returns: Type Description np . ndarray Embedding batch of shape (batch_size, embedding_size) Source code in npc_engine\\services\\similarity\\similarity_transformers.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def compute_embedding_batch ( self , lines : List [ str ]) -> np . ndarray : \"\"\"Compute line embeddings in batch. Args: lines: List of sentences to embed Returns: Embedding batch of shape (batch_size, embedding_size) \"\"\" tokenized = self . tokenizer . encode_batch ( lines ) ids = np . stack ([ np . asarray ( encoding . ids ) for encoding in tokenized ]) . astype ( np . int64 ) attention_mask = np . stack ( [ np . asarray ( encoding . attention_mask ) for encoding in tokenized ] ) . astype ( np . int64 ) if not self . token_type_support : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask } else : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : np . stack ( [ np . asarray ( encoding . type_ids ) for encoding in tokenized ] ) . astype ( np . int64 ), } outp = self . model . run ( None , input_dict ) return self . _mean_pooling ( outp , attention_mask ) metric ( embedding_a , embedding_b ) Similarity between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Parameters: Name Type Description Default embedding_a np . ndarray Embedding of shape (1 or batch_size, embedding_size) required embedding_b np . ndarray Embedding of shape (1 or batch_size, embedding_size) required Returns: Type Description np . ndarray Vector of distances (batch_size or 1,) Source code in npc_engine\\services\\similarity\\similarity_transformers.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def metric ( self , embedding_a : np . ndarray , embedding_b : np . ndarray ) -> np . ndarray : \"\"\"Similarity between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Args: embedding_a: Embedding of shape (1 or batch_size, embedding_size) embedding_b: Embedding of shape (1 or batch_size, embedding_size) Returns: Vector of distances (batch_size or 1,) \"\"\" if self . metric_type == \"dot\" : return - np . dot ( embedding_a , embedding_b . T ) . squeeze ( 0 ) elif self . metric_type == \"cosine\" : return 1 - cdist ( embedding_a , embedding_b , metric = \"cosine\" ) . squeeze ( 0 ) stt Speech to text API. This module implements specific models and wraps them under the common interface for loading and inference. Example from npc_engine.models.stt import SpeechToTextAPI model = SpeechToTextAPI.load(\"path/to/model_dir\") text = model.listen() # Say something nemo_stt Module that implements Huggingface transformers semantic similarity. NemoSTT Bases: SpeechToTextAPI Text to speech pipeline based on Nemo toolkit. Uses ONNX export of EncDecCTCModel from Nemo toolkit. Punctuation distillbert model from Nemo toolkit. (requires tokenizer.json as well) Huggingface transformers model for predicting that sentence is finished (Cropped sentence -> 0 label, finished sentence -> 1 label). OpenSLR Librispeech 3-gram model converted to lowercase https://www.openslr.org/11/ References https://github.com/NVIDIA/NeMo https://catalog.ngc.nvidia.com/orgs/nvidia/models/quartznet15x5 https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/punctuation_and_capitalization.html ctc.onnx spec: - inputs: `audio_signal` mel spectogram of shape `(batch_size, 64, mel_sequence)` - outputs: `tokens` of shape `(batch_size, token_sequence, logits)` punctuation.onnx spec: - inputs: `input_ids` mel spectogram of shape `(batch_size, sequence)` `attention_mask` mel spectogram of shape `(batch_size, sequence)` - outputs: `punctuation` of shape `(batch_size, sequence, 4)` `capitalization` of shape `(batch_size, sequence, 2)` Source code in npc_engine\\services\\stt\\nemo_stt.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 class NemoSTT ( SpeechToTextAPI ): \"\"\"Text to speech pipeline based on Nemo toolkit. Uses: - ONNX export of EncDecCTCModel from Nemo toolkit. - Punctuation distillbert model from Nemo toolkit. (requires tokenizer.json as well) - Huggingface transformers model for predicting that sentence is finished (Cropped sentence -> 0 label, finished sentence -> 1 label). - OpenSLR Librispeech 3-gram model converted to lowercase https://www.openslr.org/11/ References: https://github.com/NVIDIA/NeMo https://catalog.ngc.nvidia.com/orgs/nvidia/models/quartznet15x5 https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/punctuation_and_capitalization.html ctc.onnx spec: - inputs: `audio_signal` mel spectogram of shape `(batch_size, 64, mel_sequence)` - outputs: `tokens` of shape `(batch_size, token_sequence, logits)` punctuation.onnx spec: - inputs: `input_ids` mel spectogram of shape `(batch_size, sequence)` `attention_mask` mel spectogram of shape `(batch_size, sequence)` - outputs: `punctuation` of shape `(batch_size, sequence, 4)` `capitalization` of shape `(batch_size, sequence, 2)` \"\"\" def __init__ ( self , model_path : str , frame_size : int = 1000 , sample_rate : int = 16000 , predict_punctuation : bool = False , * args , ** kwargs , ): \"\"\"Create and load biencoder model for semantic similarity. Args: model_path: A path where model config and weights are stored. frame_size: Size of the audio frame in milliseconds. sample_rate: Sample rate of the audio. predict_punctuation: Whether to predict punctuation and capitalization. \"\"\" super () . __init__ ( sample_rate = sample_rate , model_path = model_path , frame_size = frame_size , * args , ** kwargs , ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_BASIC provider = rt . get_available_providers ()[ 0 ] logger . info ( f \"STT uses { provider } provider\" ) self . stft_filterbanks = librosa . filters . mel ( 16000 , 512 , n_mels = 64 , fmin = 0 , fmax = 8000 ) self . stft_window = librosa . filters . get_window ( \"hann\" , 320 , fftbins = False ) self . mel_mean , self . mel_std = self . _fixed_normalization () self . asr_model = rt . InferenceSession ( path . join ( model_path , \"ctc.onnx\" ), providers = [ provider ], sess_options = sess_options , ) self . predict_punctuation = predict_punctuation if self . predict_punctuation : self . punctuation = rt . InferenceSession ( path . join ( model_path , \"punctuation.onnx\" ), providers = [ provider ], sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( path . join ( model_path , \"tokenizer.json\" ) ) self . asr_vocab = \" abcdefghijklmnopqrstuvwxyz'\" self . asr_vocab = list ( self . asr_vocab ) self . asr_vocab . append ( \"\" ) self . punct_labels = \"O,.?\" self . capit_labels = \"OU\" self . decoder = build_ctcdecoder ( self . asr_vocab ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_ALL self . sentence_model = rt . InferenceSession ( path . join ( model_path , \"sentence_prediction.onnx\" ), providers = [ rt . get_available_providers ()[ 0 ]], sess_options = sess_options , ) self . sentence_tokenizer = Tokenizer . from_file ( path . join ( model_path , \"sentence_tokenizer.json\" ) ) logger . info ( f \"Sentence classifier uses { rt . get_available_providers ()[ 0 ] } provider\" ) def transcribe ( self , audio : List [ float ]) -> np . ndarray : \"\"\"Transcribe audio usign this pipeline. Args: audio: ndarray of int16 of shape (samples,). Returns: Transcribed text from the audio. \"\"\" logits = self . _predict ( np . asarray ( audio , dtype = np . float32 )) return logits def decode ( self , logits : np . ndarray ) -> str : \"\"\"Decode logits into text. Args: logits: ndarray of float32 of shape (timesteps, vocab_size). Returns: Decoded string. \"\"\" return self . decoder . decode ( logits ) def decide_finished ( self , context : str , text : str ) -> bool : \"\"\"Decide if audio transcription should be finished. Args: context: Text context of the speech recognized (e.g. a question to which speech recognized is a reply to). text: Recognized speech so far Returns: Decision to stop recognition and finalize results. \"\"\" decision = self . _decide_sentence_finished ( context , text ) done = bool ( decision ) return done def postprocess ( self , text : str ) -> str : \"\"\"Add punctuation and capitalization. Args: text: audio transcription. Returns: Postprocessed text transcribtion. \"\"\" if self . predict_punctuation : enc = self . tokenizer . encode ( text ) punct , capit = self . punctuation . run ( None , { \"input_ids\" : np . asarray ( enc . ids , np . int64 ) . reshape ([ 1 , - 1 ]), \"attention_mask\" : np . asarray ( enc . attention_mask , np . int64 ) . reshape ( [ 1 , - 1 ] ), }, ) punct = punct . argmax ( 2 ) capit = capit . argmax ( 2 ) punctuated_capitalized = self . _apply_punct_capit_predictions ( text , punct [ 0 ][ 1 : - 1 ] . tolist (), capit [ 0 ][ 1 : - 1 ] . tolist () ) return punctuated_capitalized else : return text def _decide_sentence_finished ( self , context , text ): tokenized = self . sentence_tokenizer . encode ( context , text ) ids = np . asarray ( tokenized . ids ) . reshape ([ 1 , - 1 ]) . astype ( np . int64 ) type_ids = np . asarray ( tokenized . type_ids ) . reshape ([ 1 , - 1 ]) . astype ( np . int64 ) attention_mask = np . ones_like ( ids ) input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : type_ids , } logits = self . sentence_model . run ( None , input_dict )[ 0 ] return logits . argmax ( - 1 )[ 0 ] def _predict ( self , audio : np . ndarray ) -> np . ndarray : signal = audio . reshape ([ 1 , - 1 ]) audio_signal = self . _preprocess_signal ( signal ) . astype ( np . float32 ) return self . asr_model . run ( None , { \"audio_signal\" : audio_signal })[ 0 ][ 0 ] def _preprocess_signal ( self , signal ): audio_signal = signal . reshape ([ 1 , - 1 ]) # audio_signal += np.random.rand(*audio_signal.shape) * 1e-5 audio_signal = np . concatenate ( ( audio_signal [:, 0 ] . reshape ([ - 1 , 1 ]), audio_signal [:, 1 :] - 0.97 * audio_signal [:, : - 1 ], ), axis = 1 , ) audio_signal = audio_signal . reshape ([ - 1 ]) spectogram = librosa . stft ( audio_signal , n_fft = 512 , hop_length = 160 , win_length = 320 , window = self . stft_window , center = True , ) spectogram = np . stack ([ spectogram . real , spectogram . imag ], - 1 ) spectogram = np . sqrt (( spectogram ** 2 ) . sum ( - 1 )) spectogram = spectogram ** 2 spectogram = np . dot ( self . stft_filterbanks , spectogram ) spectogram = np . expand_dims ( spectogram , 0 ) spectogram = np . log ( spectogram + ( 2 ** - 24 )) spectogram = spectogram - np . asarray ( self . mel_mean ) . reshape ([ 1 , 64 , 1 ]) spectogram = spectogram / np . asarray ( self . mel_std ) . reshape ([ 1 , 64 , 1 ]) return spectogram def _apply_punct_capit_predictions ( self , query : str , punct_preds , capit_preds ) -> str : \"\"\"Restores punctuation and capitalization in ``query``. Args: query: a string without punctuation and capitalization punct_preds: ids of predicted punctuation labels capit_preds: ids of predicted capitalization labels Returns: a query with restored punctuation and capitalization \"\"\" query = query . strip () . split () query_with_punct_and_capit = \"\" skip = 0 for j , word in enumerate ( query ): word_enc = self . tokenizer . encode ( word , add_special_tokens = False ) . ids skip += len ( word_enc ) - 1 punct_label = self . punct_labels [ punct_preds [ j + skip ]] capit_label = self . capit_labels [ capit_preds [ j + skip ]] if capit_label != \"O\" : word = word . capitalize () query_with_punct_and_capit += word if punct_label != \"O\" : query_with_punct_and_capit += punct_label query_with_punct_and_capit += \" \" return query_with_punct_and_capit [: - 1 ] def _fixed_normalization ( self ): \"\"\"From https://github.com/NVIDIA/NeMo/blob/stable/tutorials/asr/Online_ASR_Microphone_Demo.ipynb . For whatever reason they use these values to normalize the mel spectogram. \"\"\" mel_fixed_mean = [ - 14.95827016 , - 12.71798736 , - 11.76067913 , - 10.83311182 , - 10.6746914 , - 10.15163465 , - 10.05378331 , - 9.53918999 , - 9.41858904 , - 9.23382904 , - 9.46470918 , - 9.56037 , - 9.57434245 , - 9.47498732 , - 9.7635205 , - 10.08113074 , - 10.05454561 , - 9.81112681 , - 9.68673603 , - 9.83652977 , - 9.90046248 , - 9.85404766 , - 9.92560366 , - 9.95440354 , - 10.17162966 , - 9.90102482 , - 9.47471025 , - 9.54416855 , - 10.07109475 , - 9.98249912 , - 9.74359465 , - 9.55632283 , - 9.23399915 , - 9.36487649 , - 9.81791084 , - 9.56799225 , - 9.70630899 , - 9.85148006 , - 9.8594418 , - 10.01378735 , - 9.98505315 , - 9.62016094 , - 10.342285 , - 10.41070709 , - 10.10687659 , - 10.14536695 , - 10.30828702 , - 10.23542833 , - 10.88546868 , - 11.31723646 , - 11.46087382 , - 11.54877829 , - 11.62400934 , - 11.92190509 , - 12.14063815 , - 11.65130117 , - 11.58308531 , - 12.22214663 , - 12.42927197 , - 12.58039805 , - 13.10098969 , - 13.14345864 , - 13.31835645 , - 14.47345634 , ] mel_fixed_std = [ 3.81402054 , 4.12647781 , 4.05007065 , 3.87790987 , 3.74721178 , 3.68377423 , 3.69344 , 3.54001005 , 3.59530412 , 3.63752368 , 3.62826417 , 3.56488469 , 3.53740577 , 3.68313898 , 3.67138151 , 3.55707266 , 3.54919572 , 3.55721289 , 3.56723346 , 3.46029304 , 3.44119672 , 3.49030548 , 3.39328435 , 3.28244406 , 3.28001423 , 3.26744937 , 3.46692348 , 3.35378948 , 2.96330901 , 2.97663111 , 3.04575148 , 2.89717604 , 2.95659301 , 2.90181116 , 2.7111687 , 2.93041291 , 2.86647897 , 2.73473181 , 2.71495654 , 2.75543763 , 2.79174615 , 2.96076456 , 2.57376336 , 2.68789782 , 2.90930817 , 2.90412004 , 2.76187531 , 2.89905006 , 2.65896173 , 2.81032176 , 2.87769857 , 2.84665271 , 2.80863137 , 2.80707634 , 2.83752184 , 3.01914511 , 2.92046439 , 2.78461139 , 2.90034605 , 2.94599508 , 2.99099718 , 3.0167554 , 3.04649716 , 2.94116777 , ] return mel_fixed_mean , mel_fixed_std __init__ ( model_path , frame_size = 1000 , sample_rate = 16000 , predict_punctuation = False , * args , ** kwargs ) Create and load biencoder model for semantic similarity. Parameters: Name Type Description Default model_path str A path where model config and weights are stored. required frame_size int Size of the audio frame in milliseconds. 1000 sample_rate int Sample rate of the audio. 16000 predict_punctuation bool Whether to predict punctuation and capitalization. False Source code in npc_engine\\services\\stt\\nemo_stt.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def __init__ ( self , model_path : str , frame_size : int = 1000 , sample_rate : int = 16000 , predict_punctuation : bool = False , * args , ** kwargs , ): \"\"\"Create and load biencoder model for semantic similarity. Args: model_path: A path where model config and weights are stored. frame_size: Size of the audio frame in milliseconds. sample_rate: Sample rate of the audio. predict_punctuation: Whether to predict punctuation and capitalization. \"\"\" super () . __init__ ( sample_rate = sample_rate , model_path = model_path , frame_size = frame_size , * args , ** kwargs , ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_BASIC provider = rt . get_available_providers ()[ 0 ] logger . info ( f \"STT uses { provider } provider\" ) self . stft_filterbanks = librosa . filters . mel ( 16000 , 512 , n_mels = 64 , fmin = 0 , fmax = 8000 ) self . stft_window = librosa . filters . get_window ( \"hann\" , 320 , fftbins = False ) self . mel_mean , self . mel_std = self . _fixed_normalization () self . asr_model = rt . InferenceSession ( path . join ( model_path , \"ctc.onnx\" ), providers = [ provider ], sess_options = sess_options , ) self . predict_punctuation = predict_punctuation if self . predict_punctuation : self . punctuation = rt . InferenceSession ( path . join ( model_path , \"punctuation.onnx\" ), providers = [ provider ], sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( path . join ( model_path , \"tokenizer.json\" ) ) self . asr_vocab = \" abcdefghijklmnopqrstuvwxyz'\" self . asr_vocab = list ( self . asr_vocab ) self . asr_vocab . append ( \"\" ) self . punct_labels = \"O,.?\" self . capit_labels = \"OU\" self . decoder = build_ctcdecoder ( self . asr_vocab ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_ALL self . sentence_model = rt . InferenceSession ( path . join ( model_path , \"sentence_prediction.onnx\" ), providers = [ rt . get_available_providers ()[ 0 ]], sess_options = sess_options , ) self . sentence_tokenizer = Tokenizer . from_file ( path . join ( model_path , \"sentence_tokenizer.json\" ) ) logger . info ( f \"Sentence classifier uses { rt . get_available_providers ()[ 0 ] } provider\" ) _apply_punct_capit_predictions ( query , punct_preds , capit_preds ) Restores punctuation and capitalization in query . Parameters: Name Type Description Default query str a string without punctuation and capitalization required punct_preds ids of predicted punctuation labels required capit_preds ids of predicted capitalization labels required Returns: Type Description str a query with restored punctuation and capitalization Source code in npc_engine\\services\\stt\\nemo_stt.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 def _apply_punct_capit_predictions ( self , query : str , punct_preds , capit_preds ) -> str : \"\"\"Restores punctuation and capitalization in ``query``. Args: query: a string without punctuation and capitalization punct_preds: ids of predicted punctuation labels capit_preds: ids of predicted capitalization labels Returns: a query with restored punctuation and capitalization \"\"\" query = query . strip () . split () query_with_punct_and_capit = \"\" skip = 0 for j , word in enumerate ( query ): word_enc = self . tokenizer . encode ( word , add_special_tokens = False ) . ids skip += len ( word_enc ) - 1 punct_label = self . punct_labels [ punct_preds [ j + skip ]] capit_label = self . capit_labels [ capit_preds [ j + skip ]] if capit_label != \"O\" : word = word . capitalize () query_with_punct_and_capit += word if punct_label != \"O\" : query_with_punct_and_capit += punct_label query_with_punct_and_capit += \" \" return query_with_punct_and_capit [: - 1 ] _fixed_normalization () From https://github.com/NVIDIA/NeMo/blob/stable/tutorials/asr/Online_ASR_Microphone_Demo.ipynb . For whatever reason they use these values to normalize the mel spectogram. Source code in npc_engine\\services\\stt\\nemo_stt.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 def _fixed_normalization ( self ): \"\"\"From https://github.com/NVIDIA/NeMo/blob/stable/tutorials/asr/Online_ASR_Microphone_Demo.ipynb . For whatever reason they use these values to normalize the mel spectogram. \"\"\" mel_fixed_mean = [ - 14.95827016 , - 12.71798736 , - 11.76067913 , - 10.83311182 , - 10.6746914 , - 10.15163465 , - 10.05378331 , - 9.53918999 , - 9.41858904 , - 9.23382904 , - 9.46470918 , - 9.56037 , - 9.57434245 , - 9.47498732 , - 9.7635205 , - 10.08113074 , - 10.05454561 , - 9.81112681 , - 9.68673603 , - 9.83652977 , - 9.90046248 , - 9.85404766 , - 9.92560366 , - 9.95440354 , - 10.17162966 , - 9.90102482 , - 9.47471025 , - 9.54416855 , - 10.07109475 , - 9.98249912 , - 9.74359465 , - 9.55632283 , - 9.23399915 , - 9.36487649 , - 9.81791084 , - 9.56799225 , - 9.70630899 , - 9.85148006 , - 9.8594418 , - 10.01378735 , - 9.98505315 , - 9.62016094 , - 10.342285 , - 10.41070709 , - 10.10687659 , - 10.14536695 , - 10.30828702 , - 10.23542833 , - 10.88546868 , - 11.31723646 , - 11.46087382 , - 11.54877829 , - 11.62400934 , - 11.92190509 , - 12.14063815 , - 11.65130117 , - 11.58308531 , - 12.22214663 , - 12.42927197 , - 12.58039805 , - 13.10098969 , - 13.14345864 , - 13.31835645 , - 14.47345634 , ] mel_fixed_std = [ 3.81402054 , 4.12647781 , 4.05007065 , 3.87790987 , 3.74721178 , 3.68377423 , 3.69344 , 3.54001005 , 3.59530412 , 3.63752368 , 3.62826417 , 3.56488469 , 3.53740577 , 3.68313898 , 3.67138151 , 3.55707266 , 3.54919572 , 3.55721289 , 3.56723346 , 3.46029304 , 3.44119672 , 3.49030548 , 3.39328435 , 3.28244406 , 3.28001423 , 3.26744937 , 3.46692348 , 3.35378948 , 2.96330901 , 2.97663111 , 3.04575148 , 2.89717604 , 2.95659301 , 2.90181116 , 2.7111687 , 2.93041291 , 2.86647897 , 2.73473181 , 2.71495654 , 2.75543763 , 2.79174615 , 2.96076456 , 2.57376336 , 2.68789782 , 2.90930817 , 2.90412004 , 2.76187531 , 2.89905006 , 2.65896173 , 2.81032176 , 2.87769857 , 2.84665271 , 2.80863137 , 2.80707634 , 2.83752184 , 3.01914511 , 2.92046439 , 2.78461139 , 2.90034605 , 2.94599508 , 2.99099718 , 3.0167554 , 3.04649716 , 2.94116777 , ] return mel_fixed_mean , mel_fixed_std decide_finished ( context , text ) Decide if audio transcription should be finished. Parameters: Name Type Description Default context str Text context of the speech recognized (e.g. a question to which speech recognized is a reply to). required text str Recognized speech so far required Returns: Type Description bool Decision to stop recognition and finalize results. Source code in npc_engine\\services\\stt\\nemo_stt.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def decide_finished ( self , context : str , text : str ) -> bool : \"\"\"Decide if audio transcription should be finished. Args: context: Text context of the speech recognized (e.g. a question to which speech recognized is a reply to). text: Recognized speech so far Returns: Decision to stop recognition and finalize results. \"\"\" decision = self . _decide_sentence_finished ( context , text ) done = bool ( decision ) return done decode ( logits ) Decode logits into text. Parameters: Name Type Description Default logits np . ndarray ndarray of float32 of shape (timesteps, vocab_size). required Returns: Type Description str Decoded string. Source code in npc_engine\\services\\stt\\nemo_stt.py 136 137 138 139 140 141 142 143 144 145 def decode ( self , logits : np . ndarray ) -> str : \"\"\"Decode logits into text. Args: logits: ndarray of float32 of shape (timesteps, vocab_size). Returns: Decoded string. \"\"\" return self . decoder . decode ( logits ) postprocess ( text ) Add punctuation and capitalization. Parameters: Name Type Description Default text str audio transcription. required Returns: Type Description str Postprocessed text transcribtion. Source code in npc_engine\\services\\stt\\nemo_stt.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def postprocess ( self , text : str ) -> str : \"\"\"Add punctuation and capitalization. Args: text: audio transcription. Returns: Postprocessed text transcribtion. \"\"\" if self . predict_punctuation : enc = self . tokenizer . encode ( text ) punct , capit = self . punctuation . run ( None , { \"input_ids\" : np . asarray ( enc . ids , np . int64 ) . reshape ([ 1 , - 1 ]), \"attention_mask\" : np . asarray ( enc . attention_mask , np . int64 ) . reshape ( [ 1 , - 1 ] ), }, ) punct = punct . argmax ( 2 ) capit = capit . argmax ( 2 ) punctuated_capitalized = self . _apply_punct_capit_predictions ( text , punct [ 0 ][ 1 : - 1 ] . tolist (), capit [ 0 ][ 1 : - 1 ] . tolist () ) return punctuated_capitalized else : return text transcribe ( audio ) Transcribe audio usign this pipeline. Parameters: Name Type Description Default audio List [ float ] ndarray of int16 of shape (samples,). required Returns: Type Description np . ndarray Transcribed text from the audio. Source code in npc_engine\\services\\stt\\nemo_stt.py 124 125 126 127 128 129 130 131 132 133 134 def transcribe ( self , audio : List [ float ]) -> np . ndarray : \"\"\"Transcribe audio usign this pipeline. Args: audio: ndarray of int16 of shape (samples,). Returns: Transcribed text from the audio. \"\"\" logits = self . _predict ( np . asarray ( audio , dtype = np . float32 )) return logits stt_base Module that implements speech to text model API. SpeechToTextAPI Bases: BaseService Abstract base class for speech to text models. Source code in npc_engine\\services\\stt\\stt_base.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 class SpeechToTextAPI ( BaseService ): \"\"\"Abstract base class for speech to text models.\"\"\" API_METHODS : List [ str ] = [ \"listen\" , \"stt\" , \"get_devices\" , \"select_device\" , \"initialize_microphone_input\" , ] def __init__ ( self , min_speech_duration = 100 , max_silence_duration = 1000 , vad_mode = None , sample_rate = 16000 , vad_frame_ms = 10 , pad_size = 1000 , * args , ** kwargs , ): \"\"\"Initialize VAD part of the API.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True sd . default . samplerate = sample_rate self . max_silence_duration = max_silence_duration self . min_speech_duration = min_speech_duration self . vad = webrtcvad . Vad () if vad_mode is not None : self . vad . set_mode ( vad_mode ) self . sample_rate = sample_rate self . listen_queue = Queue ( 10 ) self . vad_frame_ms = vad_frame_ms self . vad_frame_size = int (( vad_frame_ms * sample_rate ) / 1000 ) self . running = False self . microphone_initialized = False self . silence_buffer = np . empty ([ 0 ]) self . pad_size = pad_size @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"SpeechToTextAPI\" def __del__ ( self ): \"\"\"Stop listening on destruction.\"\"\" if self . microphone_initialized : self . stream . stop () def initialize_microphone_input ( self ): \"\"\"Initialize microphone.\"\"\" if self . microphone_initialized : return self . running = False self . microphone_initialized = True def callback ( in_data , frame_count , time_info , status ): if self . running : try : self . listen_queue . put ( in_data . reshape ( - 1 ), block = False ) except Exception : return else : if not self . _vad_frame ( in_data . reshape ( - 1 ) ): # Register only silence for buffer self . silence_buffer = np . append ( self . silence_buffer , in_data . reshape ( - 1 ) ) self . silence_buffer = self . silence_buffer [ - self . pad_size :] self . stream = sd . InputStream ( samplerate = self . sample_rate , channels = 1 , blocksize = self . vad_frame_size , callback = callback , ) self . stream . start () while self . silence_buffer . shape [ 0 ] < self . pad_size : pass def listen ( self , context : str = None ) -> str : # pragma: no cover \"\"\"Listen for speech input and return text from speech when done. Listens for speech, if speech is active for longer than self.frame_size in milliseconds then starts transcribing it. On each voice activity detection (VAD) pause uses context to decide if transcribed text is a finished response to a context. If it is, applies preprocessing and returns the result. If transcribed text is not a response to a context but VAD pause persists through max_silence_duration then returns the results anyway. Requires a microphone input to be initialized. Args: context: A last line of the dialogue used to decide when to stop listening. It allows our STT system to not wait for a VAD timeout (max_silence_duration in ms). Returns: Recognized text from the audio. \"\"\" if not self . microphone_initialized : raise RuntimeError ( \"Microphone not initialized.\" ) self . listen_queue . queue . clear () context = re . sub ( r \"[^A-Za-z0-9 ]+\" , \"\" , context ) . lower () if context else None text = self . _transcribe_vad_pause ( context ) processed = self . postprocess ( text ) self . listen_queue . queue . clear () return processed def _transcribe_vad_pause ( self , context ) -> str : done = False total_pause_ms = 0 total_speech_ms = 0 speech_appeared = False tested_pause = False logits = None signal = self . silence_buffer self . running = True while not done : try : vad_frame = self . listen_queue . get ( block = False ) except Exception : continue is_speech = self . _vad_frame ( vad_frame ) total_speech_ms , total_pause_ms = self . _update_vad_stats ( is_speech , total_speech_ms , total_pause_ms ) if total_speech_ms > self . min_speech_duration : speech_appeared = True if total_pause_ms == 0 : tested_pause = False signal = np . append ( signal , vad_frame ) if not speech_appeared and total_speech_ms < self . min_speech_duration : # Keep only last minimum detectable speech duration + buffer signal = signal [ - self . _ms_to_samplenum ( self . min_speech_duration * 2 ) :] if signal . shape [ 0 ] >= self . _ms_to_samplenum ( self . min_speech_duration ): if speech_appeared and total_pause_ms > self . max_silence_duration : wrapped_signal = self . _wrap_signal ( signal ) logits = self . transcribe ( wrapped_signal ) text = self . decode ( logits ) done = True self . running = False return text elif ( speech_appeared and total_pause_ms > self . min_speech_duration and not tested_pause ): tested_pause = True logits = self . transcribe ( np . pad ( signal , ( 0 , 1000 ), \"wrap\" )) text = self . decode ( logits ) done = self . decide_finished ( context , text ) self . running = not done self . running = False return text def stt ( self , audio : List [ int ]) -> str : \"\"\"Transcribe speech. Args: audio: PMC data with bit depth 16. Returns: Recognized text from the audio. \"\"\" logits = self . transcribe ( audio ) text = self . decode ( logits ) text = self . postprocess ( text ) return text def get_devices ( self ) -> Dict [ int , str ]: # pragma: no cover \"\"\"Get available audio devices.\"\"\" return [ device [ \"name\" ] for device in sd . query_devices ()] def select_device ( self , device_id : int ): # pragma: no cover \"\"\"Get available audio devices.\"\"\" device_id = int ( device_id ) if device_id >= len ( sd . query_devices ()) or device_id < 0 : raise ValueError ( f \"Bad device id, valid device ids in range [0; { len ( sd . query_devices ()) } )\" ) sd . default . device = device_id def _wrap_signal ( self , signal : np . ndarray ) -> np . ndarray : \"\"\"Append silence buffer at both ends of the signal.\"\"\" signal_with_silence = np . append ( np . append ( self . silence_buffer , signal ), self . silence_buffer ) return signal_with_silence def _vad_frame ( self , frame ): # pragma: no cover \"\"\"Detect voice activity in a frame.\"\"\" is_speech = self . vad . is_speech ( np . frombuffer (( frame * 32767 ) . astype ( np . int16 ) . tobytes (), np . int8 ), sample_rate = 16000 , ) return is_speech def _ms_to_samplenum ( self , time ): return int ( time * self . sample_rate / 1000 ) def _samples_to_ms ( self , samples ): return samples * 1000 / self . sample_rate def _update_vad_stats ( self , is_speech , total_speech_ms , total_pause_ms ): \"\"\"Increase total_speech_ms if is_speech is true and appears for min_speech_duration frames consequently.\"\"\" if is_speech : total_speech_ms += self . vad_frame_ms total_pause_ms = 0 else : total_pause_ms += self . vad_frame_ms total_speech_ms = 0 return total_speech_ms , total_pause_ms @abstractmethod def transcribe ( self , audio : np . ndarray ) -> np . ndarray : \"\"\"Abstract method for audio transcription. Should be implemented by the specific model. Args: audio: ndarray of int16 of shape (samples,). Returns: Transcribed logits from the audio. \"\"\" return None @abstractmethod def decode ( self , logits : np . ndarray ) -> str : \"\"\"Decode logits into text. Args: logits: ndarray of float32 of shape (timesteps, vocab_size). Returns: Decoded string. \"\"\" return None @abstractmethod def decide_finished ( self , context : str , text : str ) -> bool : \"\"\"Abstract method for deciding if audio transcription should be finished. Should be implemented by the specific model. Args: context: Text context of the speech recognized (e.g. a question to which speech recognized is a reply to). text: Recognized speech so far. Returns: Decision to stop recognition and finalize results. \"\"\" return None @abstractmethod def postprocess ( self , text : str ) -> str : \"\"\"Abstract method for audio transcription postprocessing. Should be implemented by the specific model. Args: text: audio transcription. Returns: Postprocessed text transcribtion. \"\"\" return None __del__ () Stop listening on destruction. Source code in npc_engine\\services\\stt\\stt_base.py 62 63 64 65 def __del__ ( self ): \"\"\"Stop listening on destruction.\"\"\" if self . microphone_initialized : self . stream . stop () __init__ ( min_speech_duration = 100 , max_silence_duration = 1000 , vad_mode = None , sample_rate = 16000 , vad_frame_ms = 10 , pad_size = 1000 , * args , ** kwargs ) Initialize VAD part of the API. Source code in npc_engine\\services\\stt\\stt_base.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , min_speech_duration = 100 , max_silence_duration = 1000 , vad_mode = None , sample_rate = 16000 , vad_frame_ms = 10 , pad_size = 1000 , * args , ** kwargs , ): \"\"\"Initialize VAD part of the API.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True sd . default . samplerate = sample_rate self . max_silence_duration = max_silence_duration self . min_speech_duration = min_speech_duration self . vad = webrtcvad . Vad () if vad_mode is not None : self . vad . set_mode ( vad_mode ) self . sample_rate = sample_rate self . listen_queue = Queue ( 10 ) self . vad_frame_ms = vad_frame_ms self . vad_frame_size = int (( vad_frame_ms * sample_rate ) / 1000 ) self . running = False self . microphone_initialized = False self . silence_buffer = np . empty ([ 0 ]) self . pad_size = pad_size _update_vad_stats ( is_speech , total_speech_ms , total_pause_ms ) Increase total_speech_ms if is_speech is true and appears for min_speech_duration frames consequently. Source code in npc_engine\\services\\stt\\stt_base.py 231 232 233 234 235 236 237 238 239 def _update_vad_stats ( self , is_speech , total_speech_ms , total_pause_ms ): \"\"\"Increase total_speech_ms if is_speech is true and appears for min_speech_duration frames consequently.\"\"\" if is_speech : total_speech_ms += self . vad_frame_ms total_pause_ms = 0 else : total_pause_ms += self . vad_frame_ms total_speech_ms = 0 return total_speech_ms , total_pause_ms _vad_frame ( frame ) Detect voice activity in a frame. Source code in npc_engine\\services\\stt\\stt_base.py 217 218 219 220 221 222 223 def _vad_frame ( self , frame ): # pragma: no cover \"\"\"Detect voice activity in a frame.\"\"\" is_speech = self . vad . is_speech ( np . frombuffer (( frame * 32767 ) . astype ( np . int16 ) . tobytes (), np . int8 ), sample_rate = 16000 , ) return is_speech _wrap_signal ( signal ) Append silence buffer at both ends of the signal. Source code in npc_engine\\services\\stt\\stt_base.py 210 211 212 213 214 215 def _wrap_signal ( self , signal : np . ndarray ) -> np . ndarray : \"\"\"Append silence buffer at both ends of the signal.\"\"\" signal_with_silence = np . append ( np . append ( self . silence_buffer , signal ), self . silence_buffer ) return signal_with_silence decide_finished ( context , text ) abstractmethod Abstract method for deciding if audio transcription should be finished. Should be implemented by the specific model. Parameters: Name Type Description Default context str Text context of the speech recognized (e.g. a question to which speech recognized is a reply to). required text str Recognized speech so far. required Returns: Type Description bool Decision to stop recognition and finalize results. Source code in npc_engine\\services\\stt\\stt_base.py 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 @abstractmethod def decide_finished ( self , context : str , text : str ) -> bool : \"\"\"Abstract method for deciding if audio transcription should be finished. Should be implemented by the specific model. Args: context: Text context of the speech recognized (e.g. a question to which speech recognized is a reply to). text: Recognized speech so far. Returns: Decision to stop recognition and finalize results. \"\"\" return None decode ( logits ) abstractmethod Decode logits into text. Parameters: Name Type Description Default logits np . ndarray ndarray of float32 of shape (timesteps, vocab_size). required Returns: Type Description str Decoded string. Source code in npc_engine\\services\\stt\\stt_base.py 255 256 257 258 259 260 261 262 263 264 265 @abstractmethod def decode ( self , logits : np . ndarray ) -> str : \"\"\"Decode logits into text. Args: logits: ndarray of float32 of shape (timesteps, vocab_size). Returns: Decoded string. \"\"\" return None get_api_name () classmethod Get the API name. Source code in npc_engine\\services\\stt\\stt_base.py 57 58 59 60 @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"SpeechToTextAPI\" get_devices () Get available audio devices. Source code in npc_engine\\services\\stt\\stt_base.py 197 198 199 def get_devices ( self ) -> Dict [ int , str ]: # pragma: no cover \"\"\"Get available audio devices.\"\"\" return [ device [ \"name\" ] for device in sd . query_devices ()] initialize_microphone_input () Initialize microphone. Source code in npc_engine\\services\\stt\\stt_base.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def initialize_microphone_input ( self ): \"\"\"Initialize microphone.\"\"\" if self . microphone_initialized : return self . running = False self . microphone_initialized = True def callback ( in_data , frame_count , time_info , status ): if self . running : try : self . listen_queue . put ( in_data . reshape ( - 1 ), block = False ) except Exception : return else : if not self . _vad_frame ( in_data . reshape ( - 1 ) ): # Register only silence for buffer self . silence_buffer = np . append ( self . silence_buffer , in_data . reshape ( - 1 ) ) self . silence_buffer = self . silence_buffer [ - self . pad_size :] self . stream = sd . InputStream ( samplerate = self . sample_rate , channels = 1 , blocksize = self . vad_frame_size , callback = callback , ) self . stream . start () while self . silence_buffer . shape [ 0 ] < self . pad_size : pass listen ( context = None ) Listen for speech input and return text from speech when done. Listens for speech, if speech is active for longer than self.frame_size in milliseconds then starts transcribing it. On each voice activity detection (VAD) pause uses context to decide if transcribed text is a finished response to a context. If it is, applies preprocessing and returns the result. If transcribed text is not a response to a context but VAD pause persists through max_silence_duration then returns the results anyway. Requires a microphone input to be initialized. Parameters: Name Type Description Default context str A last line of the dialogue used to decide when to stop listening. It allows our STT system to not wait for a VAD timeout (max_silence_duration in ms). None Returns: Type Description str Recognized text from the audio. Source code in npc_engine\\services\\stt\\stt_base.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def listen ( self , context : str = None ) -> str : # pragma: no cover \"\"\"Listen for speech input and return text from speech when done. Listens for speech, if speech is active for longer than self.frame_size in milliseconds then starts transcribing it. On each voice activity detection (VAD) pause uses context to decide if transcribed text is a finished response to a context. If it is, applies preprocessing and returns the result. If transcribed text is not a response to a context but VAD pause persists through max_silence_duration then returns the results anyway. Requires a microphone input to be initialized. Args: context: A last line of the dialogue used to decide when to stop listening. It allows our STT system to not wait for a VAD timeout (max_silence_duration in ms). Returns: Recognized text from the audio. \"\"\" if not self . microphone_initialized : raise RuntimeError ( \"Microphone not initialized.\" ) self . listen_queue . queue . clear () context = re . sub ( r \"[^A-Za-z0-9 ]+\" , \"\" , context ) . lower () if context else None text = self . _transcribe_vad_pause ( context ) processed = self . postprocess ( text ) self . listen_queue . queue . clear () return processed postprocess ( text ) abstractmethod Abstract method for audio transcription postprocessing. Should be implemented by the specific model. Parameters: Name Type Description Default text str audio transcription. required Returns: Type Description str Postprocessed text transcribtion. Source code in npc_engine\\services\\stt\\stt_base.py 283 284 285 286 287 288 289 290 291 292 293 294 295 @abstractmethod def postprocess ( self , text : str ) -> str : \"\"\"Abstract method for audio transcription postprocessing. Should be implemented by the specific model. Args: text: audio transcription. Returns: Postprocessed text transcribtion. \"\"\" return None select_device ( device_id ) Get available audio devices. Source code in npc_engine\\services\\stt\\stt_base.py 201 202 203 204 205 206 207 208 def select_device ( self , device_id : int ): # pragma: no cover \"\"\"Get available audio devices.\"\"\" device_id = int ( device_id ) if device_id >= len ( sd . query_devices ()) or device_id < 0 : raise ValueError ( f \"Bad device id, valid device ids in range [0; { len ( sd . query_devices ()) } )\" ) sd . default . device = device_id stt ( audio ) Transcribe speech. Parameters: Name Type Description Default audio List [ int ] PMC data with bit depth 16. required Returns: Type Description str Recognized text from the audio. Source code in npc_engine\\services\\stt\\stt_base.py 183 184 185 186 187 188 189 190 191 192 193 194 195 def stt ( self , audio : List [ int ]) -> str : \"\"\"Transcribe speech. Args: audio: PMC data with bit depth 16. Returns: Recognized text from the audio. \"\"\" logits = self . transcribe ( audio ) text = self . decode ( logits ) text = self . postprocess ( text ) return text transcribe ( audio ) abstractmethod Abstract method for audio transcription. Should be implemented by the specific model. Parameters: Name Type Description Default audio np . ndarray ndarray of int16 of shape (samples,). required Returns: Type Description np . ndarray Transcribed logits from the audio. Source code in npc_engine\\services\\stt\\stt_base.py 241 242 243 244 245 246 247 248 249 250 251 252 253 @abstractmethod def transcribe ( self , audio : np . ndarray ) -> np . ndarray : \"\"\"Abstract method for audio transcription. Should be implemented by the specific model. Args: audio: ndarray of int16 of shape (samples,). Returns: Transcribed logits from the audio. \"\"\" return None tts Text to speech specific model implementations. This module implements specific models and wraps them under the common interface for loading and inference. Example from npc_engine.models.tts import TextToSpeechAPI model = TextToSpeechAPI.load(\"path/to/model_dir\") model.run(speaker_id=0, text=\"Hello, world!\") flowtron Flowtron (https://github.com/NVIDIA/flowtron) text to speech inference implementation. FlowtronTTS Bases: TextToSpeechAPI Implements Flowtron architecture inference. Paper: arXiv:2005.05957 Code: https://github.com/NVIDIA/flowtron Onnx export script can be found in this fork https://github.com/npc-engine/flowtron. This model class requires four ONNX models encoder.onnx , backward_flow.onnx , forward_flow.onnx and vocoder.onnx where first three are layers from Flowtron architecture ( flow corresponding to one direction pass of affine coupling layers) and vocoder.onnx is neural vocoder. For detailed specs refer to https://github.com/npc-engine/flowtron. Source code in npc_engine\\services\\tts\\flowtron.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 class FlowtronTTS ( TextToSpeechAPI ): \"\"\"Implements Flowtron architecture inference. Paper: [arXiv:2005.05957](https://arxiv.org/abs/2005.05957) Code: https://github.com/NVIDIA/flowtron Onnx export script can be found in this fork https://github.com/npc-engine/flowtron. This model class requires four ONNX models `encoder.onnx`, `backward_flow.onnx`, `forward_flow.onnx` and `vocoder.onnx` where first three are layers from Flowtron architecture (`flow` corresponding to one direction pass of affine coupling layers) and `vocoder.onnx` is neural vocoder. For detailed specs refer to https://github.com/npc-engine/flowtron. \"\"\" def __init__ ( self , model_path , max_frames = 400 , gate_threshold = 0.5 , sigma = 0.8 , smoothing_window = 3 , smoothing_weight = 0.5 , * args , ** kwargs ): \"\"\"Create and load Flowtron and vocoder models.\"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = onnxruntime . SessionOptions () sess_options . graph_optimization_level = ( onnxruntime . GraphOptimizationLevel . ORT_ENABLE_ALL ) provider = onnxruntime . get_available_providers ()[ 0 ] logging . info ( \"FlowtronTTS using provider {} \" . format ( provider )) self . max_frames = max_frames self . gate_threshold = gate_threshold self . sigma = sigma self . smoothing_window = smoothing_window self . smoothing_weight = smoothing_weight self . encoder = onnxruntime . InferenceSession ( path . join ( model_path , \"encoder.onnx\" ), providers = [ provider ], sess_options = sess_options , ) self . backward_flow = onnxruntime . InferenceSession ( path . join ( model_path , \"backward_flow.onnx\" ), providers = [ provider ], sess_options = sess_options , ) self . forward_flow = onnxruntime . InferenceSession ( path . join ( model_path , \"forward_flow.onnx\" ), providers = [ provider ], sess_options = sess_options , ) self . vocoder = onnxruntime . InferenceSession ( path . join ( model_path , \"vocoder.onnx\" ), providers = [ provider ], sess_options = sess_options , ) self . speaker_ids = [ str ( i ) for i in range ( 127 )] self . speaker_ids_map = { idx : i for i , idx in enumerate ( self . speaker_ids )} def get_speaker_ids ( self ) -> List [ str ]: \"\"\"Return available ids of different speakers.\"\"\" return self . speaker_ids def run ( self , speaker_id : str , text : str , n_chunks : int ) -> Iterator [ np . ndarray ]: \"\"\"Create a generator for iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. Returns: Generator that yields next chunk of speech in the form of f32 ndarray. \"\"\" text = self . _get_text ( text ) speaker_id = np . asarray ([[ self . speaker_ids_map [ speaker_id ]]], dtype = np . int64 ) enc_outps_ortvalue = onnxruntime . OrtValue . ortvalue_from_shape_and_type ( [ text . shape [ 1 ], 1 , 640 ], np . float32 , \"cpu\" , 0 ) io_binding = self . encoder . io_binding () io_binding . bind_ortvalue_output ( \"text_emb\" , enc_outps_ortvalue ) io_binding . bind_cpu_input ( \"speaker_vecs\" , speaker_id ) io_binding . bind_cpu_input ( \"text\" , text . reshape ([ 1 , - 1 ])) self . encoder . run_with_iobinding ( io_binding ) residual = np . random . normal ( 0 , self . sigma , size = [ self . max_frames , 1 , 80 ] ) . astype ( np . float32 ) residual = self . _run_backward_flow ( residual , enc_outps_ortvalue ) residual = self . _run_forward_flow ( residual , enc_outps_ortvalue , num_split = self . max_frames // n_chunks ) last_audio = None for residual in residual : residual = np . transpose ( residual , axes = ( 1 , 2 , 0 )) audio = self . vocoder . run ( None , { \"mels\" : residual })[ 0 ] # audio = np.where( # (audio > (audio.mean() - audio.std())) # | (audio < (audio.mean() + audio.std())), # audio, # audio.mean(), # ) tmp = audio if last_audio is None : audio = audio [:, 1000 :] if last_audio is not None : cumsum_vec = np . cumsum ( np . concatenate ([ last_audio , audio ], axis = 1 ), axis = 1 ) ma_vec = ( cumsum_vec [:, self . smoothing_window :] - cumsum_vec [:, : - self . smoothing_window ] ) / self . smoothing_window audio = ( 1 - self . smoothing_weight ) * audio [ :, self . smoothing_window : ] + self . smoothing_weight * ma_vec [:, last_audio . shape [ 1 ] :] last_audio = tmp audio = audio . reshape ( - 1 ) # audio = audio / np.abs(audio).max() yield audio def _get_text ( self , text : str ): text = _clean_text ( text , [ \"flowtron_cleaners\" ]) words = re . findall ( r \"\\S*\\{.*?\\}\\S*|\\S+\" , text ) text = \" \" . join ( words ) text_norm = np . asarray ( text_to_sequence ( text ), dtype = np . int64 ) . reshape ([ 1 , - 1 ]) return text_norm def _run_backward_flow ( self , residual , enc_outps_ortvalue ): residual_o , hidden_att , hidden_lstm = self . _init_states ( residual ) hidden_att_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 0 ], \"cpu\" , 0 ) hidden_att_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 1 ], \"cpu\" , 0 ) hidden_lstm_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 0 ], \"cpu\" , 0 ) hidden_lstm_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 1 ], \"cpu\" , 0 ) hidden_att_o_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 0 ], \"cpu\" , 0 ) hidden_att_o_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 1 ], \"cpu\" , 0 ) hidden_lstm_o_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 0 ], \"cpu\" , 0 ) hidden_lstm_o_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 1 ], \"cpu\" , 0 ) residual_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( residual_o , \"cpu\" , 0 ) residual_outp = [ residual_ortvalue ] for i in range ( residual . shape [ 0 ] - 1 , - 1 , - 1 ): io_binding = self . backward_flow . io_binding () io_binding . bind_cpu_input ( \"residual\" , residual [ i ]) io_binding . bind_ortvalue_input ( \"text\" , enc_outps_ortvalue ) io_binding . bind_ortvalue_input ( \"last_output\" , residual_outp [ 0 ]) io_binding . bind_ortvalue_input ( \"hidden_att\" , hidden_att_ortvalue ) io_binding . bind_ortvalue_input ( \"hidden_att_c\" , hidden_att_c_ortvalue ) io_binding . bind_ortvalue_input ( \"hidden_lstm\" , hidden_lstm_ortvalue ) io_binding . bind_ortvalue_input ( \"hidden_lstm_c\" , hidden_lstm_c_ortvalue ) io_binding . bind_output ( \"output\" , \"cpu\" ) io_binding . bind_output ( \"gate\" , \"cpu\" ) io_binding . bind_ortvalue_output ( \"hidden_att_o\" , hidden_att_o_ortvalue ) io_binding . bind_ortvalue_output ( \"hidden_att_o_c\" , hidden_att_o_c_ortvalue ) io_binding . bind_ortvalue_output ( \"hidden_lstm_o\" , hidden_lstm_o_ortvalue ) io_binding . bind_ortvalue_output ( \"hidden_lstm_o_c\" , hidden_lstm_o_c_ortvalue ) self . backward_flow . run_with_iobinding ( io_binding ) outp = io_binding . get_outputs () gates = outp [ 1 ] . numpy () residual_outp = [ outp [ 0 ]] + residual_outp if ( gates > self . gate_threshold ) . any (): break # Switch input and output to use latest output as input ( hidden_att_ortvalue , hidden_att_o_ortvalue ) = ( hidden_att_o_ortvalue , hidden_att_ortvalue , ) ( hidden_att_c_ortvalue , hidden_att_o_c_ortvalue ) = ( hidden_att_o_c_ortvalue , hidden_att_c_ortvalue , ) ( hidden_lstm_ortvalue , hidden_lstm_o_ortvalue ) = ( hidden_lstm_o_ortvalue , hidden_lstm_ortvalue , ) ( hidden_lstm_c_ortvalue , hidden_lstm_o_c_ortvalue ) = ( hidden_lstm_o_c_ortvalue , hidden_lstm_c_ortvalue , ) residual = np . concatenate ( [ residual_ort . numpy () for residual_ort in residual_outp ], axis = 0 ) return residual def _run_forward_flow ( self , residual , enc_outps_ortvalue , num_split ): residual_o , hidden_att , hidden_lstm = self . _init_states ( residual ) hidden_att_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 0 ], \"cpu\" , 0 ) hidden_att_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 1 ], \"cpu\" , 0 ) hidden_lstm_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 0 ], \"cpu\" , 0 ) hidden_lstm_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 1 ], \"cpu\" , 0 ) hidden_att_o_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 0 ], \"cpu\" , 0 ) hidden_att_o_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 1 ], \"cpu\" , 0 ) hidden_lstm_o_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 0 ], \"cpu\" , 0 ) hidden_lstm_o_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 1 ], \"cpu\" , 0 ) residual_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( residual_o , \"cpu\" , 0 ) residual_outp = [ residual_ortvalue ] last_output = residual_ortvalue for i in range ( residual . shape [ 0 ]): io_binding = self . forward_flow . io_binding () io_binding . bind_cpu_input ( \"residual\" , residual [ i ]) io_binding . bind_ortvalue_input ( \"text\" , enc_outps_ortvalue ) io_binding . bind_ortvalue_input ( \"last_output\" , last_output ) io_binding . bind_ortvalue_input ( \"hidden_att\" , hidden_att_ortvalue ) io_binding . bind_ortvalue_input ( \"hidden_att_c\" , hidden_att_c_ortvalue ) io_binding . bind_ortvalue_input ( \"hidden_lstm\" , hidden_lstm_ortvalue ) io_binding . bind_ortvalue_input ( \"hidden_lstm_c\" , hidden_lstm_c_ortvalue ) io_binding . bind_output ( \"output\" , \"cpu\" ) io_binding . bind_output ( \"gate\" , \"cpu\" ) io_binding . bind_ortvalue_output ( \"hidden_att_o\" , hidden_att_o_ortvalue ) io_binding . bind_ortvalue_output ( \"hidden_att_o_c\" , hidden_att_o_c_ortvalue ) io_binding . bind_ortvalue_output ( \"hidden_lstm_o\" , hidden_lstm_o_ortvalue ) io_binding . bind_ortvalue_output ( \"hidden_lstm_o_c\" , hidden_lstm_o_c_ortvalue ) self . forward_flow . run_with_iobinding ( io_binding ) outp = io_binding . get_outputs () gates = outp [ 1 ] . numpy () residual_outp . append ( outp [ 0 ]) last_output = outp [ 0 ] if ( gates > self . gate_threshold ) . any (): break # Switch input and output to use latest output as input ( hidden_att_ortvalue , hidden_att_o_ortvalue ) = ( hidden_att_o_ortvalue , hidden_att_ortvalue , ) ( hidden_att_c_ortvalue , hidden_att_o_c_ortvalue ) = ( hidden_att_o_c_ortvalue , hidden_att_c_ortvalue , ) ( hidden_lstm_ortvalue , hidden_lstm_o_ortvalue ) = ( hidden_lstm_o_ortvalue , hidden_lstm_ortvalue , ) ( hidden_lstm_c_ortvalue , hidden_lstm_o_c_ortvalue ) = ( hidden_lstm_o_c_ortvalue , hidden_lstm_c_ortvalue , ) if len ( residual_outp ) % num_split == 0 and i != 0 : residual_o = np . concatenate ( [ residual_ort . numpy () for residual_ort in residual_outp ], axis = 0 ) yield residual_o residual_outp = [] if len ( residual_outp ) > 0 : residual_o = np . concatenate ( [ residual_ort . numpy () for residual_ort in residual_outp ], axis = 0 ) yield residual_o def _init_states ( self , residual ): last_outputs = np . zeros ( [ 1 , residual . shape [ 1 ], residual . shape [ 2 ]], dtype = np . float32 ) hidden_att = [ np . zeros ([ 1 , 1 , 1024 ], dtype = np . float32 ), np . zeros ([ 1 , 1 , 1024 ], dtype = np . float32 ), ] hidden_lstm = [ np . zeros ([ 2 , 1 , 1024 ], dtype = np . float32 ), np . zeros ([ 2 , 1 , 1024 ], dtype = np . float32 ), ] return last_outputs , hidden_att , hidden_lstm __init__ ( model_path , max_frames = 400 , gate_threshold = 0.5 , sigma = 0.8 , smoothing_window = 3 , smoothing_weight = 0.5 , * args , ** kwargs ) Create and load Flowtron and vocoder models. Source code in npc_engine\\services\\tts\\flowtron.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def __init__ ( self , model_path , max_frames = 400 , gate_threshold = 0.5 , sigma = 0.8 , smoothing_window = 3 , smoothing_weight = 0.5 , * args , ** kwargs ): \"\"\"Create and load Flowtron and vocoder models.\"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = onnxruntime . SessionOptions () sess_options . graph_optimization_level = ( onnxruntime . GraphOptimizationLevel . ORT_ENABLE_ALL ) provider = onnxruntime . get_available_providers ()[ 0 ] logging . info ( \"FlowtronTTS using provider {} \" . format ( provider )) self . max_frames = max_frames self . gate_threshold = gate_threshold self . sigma = sigma self . smoothing_window = smoothing_window self . smoothing_weight = smoothing_weight self . encoder = onnxruntime . InferenceSession ( path . join ( model_path , \"encoder.onnx\" ), providers = [ provider ], sess_options = sess_options , ) self . backward_flow = onnxruntime . InferenceSession ( path . join ( model_path , \"backward_flow.onnx\" ), providers = [ provider ], sess_options = sess_options , ) self . forward_flow = onnxruntime . InferenceSession ( path . join ( model_path , \"forward_flow.onnx\" ), providers = [ provider ], sess_options = sess_options , ) self . vocoder = onnxruntime . InferenceSession ( path . join ( model_path , \"vocoder.onnx\" ), providers = [ provider ], sess_options = sess_options , ) self . speaker_ids = [ str ( i ) for i in range ( 127 )] self . speaker_ids_map = { idx : i for i , idx in enumerate ( self . speaker_ids )} get_speaker_ids () Return available ids of different speakers. Source code in npc_engine\\services\\tts\\flowtron.py 81 82 83 def get_speaker_ids ( self ) -> List [ str ]: \"\"\"Return available ids of different speakers.\"\"\" return self . speaker_ids run ( speaker_id , text , n_chunks ) Create a generator for iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Returns: Type Description Iterator [ np . ndarray ] Generator that yields next chunk of speech in the form of f32 ndarray. Source code in npc_engine\\services\\tts\\flowtron.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def run ( self , speaker_id : str , text : str , n_chunks : int ) -> Iterator [ np . ndarray ]: \"\"\"Create a generator for iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. Returns: Generator that yields next chunk of speech in the form of f32 ndarray. \"\"\" text = self . _get_text ( text ) speaker_id = np . asarray ([[ self . speaker_ids_map [ speaker_id ]]], dtype = np . int64 ) enc_outps_ortvalue = onnxruntime . OrtValue . ortvalue_from_shape_and_type ( [ text . shape [ 1 ], 1 , 640 ], np . float32 , \"cpu\" , 0 ) io_binding = self . encoder . io_binding () io_binding . bind_ortvalue_output ( \"text_emb\" , enc_outps_ortvalue ) io_binding . bind_cpu_input ( \"speaker_vecs\" , speaker_id ) io_binding . bind_cpu_input ( \"text\" , text . reshape ([ 1 , - 1 ])) self . encoder . run_with_iobinding ( io_binding ) residual = np . random . normal ( 0 , self . sigma , size = [ self . max_frames , 1 , 80 ] ) . astype ( np . float32 ) residual = self . _run_backward_flow ( residual , enc_outps_ortvalue ) residual = self . _run_forward_flow ( residual , enc_outps_ortvalue , num_split = self . max_frames // n_chunks ) last_audio = None for residual in residual : residual = np . transpose ( residual , axes = ( 1 , 2 , 0 )) audio = self . vocoder . run ( None , { \"mels\" : residual })[ 0 ] # audio = np.where( # (audio > (audio.mean() - audio.std())) # | (audio < (audio.mean() + audio.std())), # audio, # audio.mean(), # ) tmp = audio if last_audio is None : audio = audio [:, 1000 :] if last_audio is not None : cumsum_vec = np . cumsum ( np . concatenate ([ last_audio , audio ], axis = 1 ), axis = 1 ) ma_vec = ( cumsum_vec [:, self . smoothing_window :] - cumsum_vec [:, : - self . smoothing_window ] ) / self . smoothing_window audio = ( 1 - self . smoothing_weight ) * audio [ :, self . smoothing_window : ] + self . smoothing_weight * ma_vec [:, last_audio . shape [ 1 ] :] last_audio = tmp audio = audio . reshape ( - 1 ) # audio = audio / np.abs(audio).max() yield audio tts_base Module that implements text to speech model API. TextToSpeechAPI Bases: BaseService Abstract base class for text-to-speech models. Source code in npc_engine\\services\\tts\\tts_base.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 class TextToSpeechAPI ( BaseService ): \"\"\"Abstract base class for text-to-speech models.\"\"\" #: Methods that are going to be exposed as services. API_METHODS : List [ str ] = [ \"tts_start\" , \"tts_get_results\" , \"get_speaker_ids\" ] def __init__ ( self , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" self . generator = None super () . __init__ ( * args , ** kwargs ) self . initialized = True @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"TextToSpeechAPI\" def tts_start ( self , speaker_id : str , text : str , n_chunks : int ) -> None : \"\"\"Initiate iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. \"\"\" sentences = re . split ( r \"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\" , text ) self . generator = self . _chain_run ( speaker_id , sentences , n_chunks ) def _chain_run ( self , speaker_id , sentences , n_chunks ) -> Iterable [ np . ndarray ]: \"\"\"Chain the run method to be used in the generator.\"\"\" for sentence in sentences : sentence = sentence . strip () if sentence != \"\" : yield from self . run ( speaker_id , sentence , n_chunks ) def tts_get_results ( self ) -> Iterable [ np . ndarray ]: \"\"\"Retrieve the next chunk of generated speech. Returns: Next chunk of speech in the form of f32 ndarray. \"\"\" if self . generator is not None : return next ( self . generator ) . tolist () else : raise ValueError ( \"Speech generation was not started. Use tts_start to start it\" ) @abstractmethod def run ( self , speaker_id : str , text : str , n_chunks : int ) -> Iterable [ np . ndarray ]: \"\"\"Create a generator for iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. Returns: Generator that yields next chunk of speech in the form of f32 ndarray. \"\"\" return None @abstractmethod def get_speaker_ids ( self ) -> List [ str ]: \"\"\"Get list of available speaker ids. Returns: The return value. True for success, False otherwise. \"\"\" return None __init__ ( * args , ** kwargs ) Empty initialization method for API to be similar to other model base classes. Source code in npc_engine\\services\\tts\\tts_base.py 16 17 18 19 20 def __init__ ( self , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" self . generator = None super () . __init__ ( * args , ** kwargs ) self . initialized = True _chain_run ( speaker_id , sentences , n_chunks ) Chain the run method to be used in the generator. Source code in npc_engine\\services\\tts\\tts_base.py 39 40 41 42 43 44 def _chain_run ( self , speaker_id , sentences , n_chunks ) -> Iterable [ np . ndarray ]: \"\"\"Chain the run method to be used in the generator.\"\"\" for sentence in sentences : sentence = sentence . strip () if sentence != \"\" : yield from self . run ( speaker_id , sentence , n_chunks ) get_api_name () classmethod Get the API name. Source code in npc_engine\\services\\tts\\tts_base.py 22 23 24 25 @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"TextToSpeechAPI\" get_speaker_ids () abstractmethod Get list of available speaker ids. Returns: Type Description List [ str ] The return value. True for success, False otherwise. Source code in npc_engine\\services\\tts\\tts_base.py 73 74 75 76 77 78 79 80 @abstractmethod def get_speaker_ids ( self ) -> List [ str ]: \"\"\"Get list of available speaker ids. Returns: The return value. True for success, False otherwise. \"\"\" return None run ( speaker_id , text , n_chunks ) abstractmethod Create a generator for iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Returns: Type Description Iterable [ np . ndarray ] Generator that yields next chunk of speech in the form of f32 ndarray. Source code in npc_engine\\services\\tts\\tts_base.py 59 60 61 62 63 64 65 66 67 68 69 70 71 @abstractmethod def run ( self , speaker_id : str , text : str , n_chunks : int ) -> Iterable [ np . ndarray ]: \"\"\"Create a generator for iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. Returns: Generator that yields next chunk of speech in the form of f32 ndarray. \"\"\" return None tts_get_results () Retrieve the next chunk of generated speech. Returns: Type Description Iterable [ np . ndarray ] Next chunk of speech in the form of f32 ndarray. Source code in npc_engine\\services\\tts\\tts_base.py 46 47 48 49 50 51 52 53 54 55 56 57 def tts_get_results ( self ) -> Iterable [ np . ndarray ]: \"\"\"Retrieve the next chunk of generated speech. Returns: Next chunk of speech in the form of f32 ndarray. \"\"\" if self . generator is not None : return next ( self . generator ) . tolist () else : raise ValueError ( \"Speech generation was not started. Use tts_start to start it\" ) tts_start ( speaker_id , text , n_chunks ) Initiate iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Source code in npc_engine\\services\\tts\\tts_base.py 27 28 29 30 31 32 33 34 35 36 37 def tts_start ( self , speaker_id : str , text : str , n_chunks : int ) -> None : \"\"\"Initiate iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. \"\"\" sentences = re . split ( r \"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\" , text ) self . generator = self . _chain_run ( speaker_id , sentences , n_chunks ) utils Utility methods for model handling. config Config related functions. get_model_type_name ( models_path , model_id ) Get model type name. Parameters: Name Type Description Default models_path str Path to the models folder. required model_id str Model id (dirname). required Returns: Type Description str Model type name. Source code in npc_engine\\services\\utils\\config.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def get_model_type_name ( models_path : str , model_id : str ) -> str : \"\"\"Get model type name. Args: models_path: Path to the models folder. model_id: Model id (dirname). Returns: Model type name. \"\"\" model_path = os . path . join ( models_path , model_id ) config_path = os . path . join ( model_path , \"config.yml\" ) with open ( config_path , \"r\" ) as f : config = yaml . safe_load ( f ) return get_type_from_dict ( config ) get_type_from_dict ( config_dict ) Get model type from config dict. Parameters: Name Type Description Default config_dict dict Config dict. required Returns: Type Description str Model type. Source code in npc_engine\\services\\utils\\config.py 8 9 10 11 12 13 14 15 16 17 def get_type_from_dict ( config_dict : dict ) -> str : \"\"\"Get model type from config dict. Args: config_dict: Config dict. Returns: Model type. \"\"\" return config_dict . get ( \"model_type\" , config_dict . get ( \"type\" , \"\" )) validate_hub_model ( models_path , model_id ) Validate huggingface hub model by id. Parameters: Name Type Description Default models_path str Path to the models folder. required model_id str Huggingface hub model id. required Source code in npc_engine\\services\\utils\\config.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def validate_hub_model ( models_path : str , model_id : str ) -> bool : \"\"\"Validate huggingface hub model by id. Args: models_path: Path to the models folder. model_id: Huggingface hub model id. \"\"\" tmp_model_path = os . path . join ( models_path , model_id . replace ( \"/\" , \"_\" )) model_correct = True try : try : hf_hub_download ( repo_id = model_id , filename = \"config.yml\" , cache_dir = tmp_model_path , force_filename = \"config.yml\" , ) except HTTPError : return False config_path = os . path . join ( models_path , model_id . replace ( \"/\" , \"_\" ), \"config.yml\" ) with open ( config_path ) as f : config_dict = yaml . load ( f , Loader = yaml . Loader ) if \"model_type\" not in config_dict : model_correct = False except ValueError : model_correct = False if os . path . exists ( config_path ): os . remove ( config_path ) if os . path . exists ( tmp_model_path ): os . rmdir ( tmp_model_path ) return model_correct validate_local_model ( models_path , model_id ) Validate local model by id. Parameters: Name Type Description Default models_path str Path to the models folder. required model_id str Model id. required Source code in npc_engine\\services\\utils\\config.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def validate_local_model ( models_path : str , model_id : str ) -> bool : \"\"\"Validate local model by id. Args: models_path: Path to the models folder. model_id: Model id. \"\"\" model_correct = True model_path = os . path . join ( models_path , model_id ) if not os . path . exists ( model_path ): model_correct = False else : try : _ = get_model_type_name ( models_path , model_id ) except FileNotFoundError : model_correct = False return model_correct validate_model ( models_path , model_id ) Validate model by id. Parameters: Name Type Description Default models_path str Path to the models folder. required model_id str Model id. required Source code in npc_engine\\services\\utils\\config.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def validate_model ( models_path : str , model_id : str ) -> bool : \"\"\"Validate model by id. Args: models_path: Path to the models folder. model_id: Model id. \"\"\" model_correct = True model_path = os . path . join ( models_path , model_id ) if not os . path . exists ( model_path ): model_correct = validate_hub_model ( models_path , model_id ) else : model_correct = validate_local_model ( models_path , model_id ) return model_correct lru_cache LRU cache. NumpyLRUCache Dict based LRU cache for numpy arrays. Source code in npc_engine\\services\\utils\\lru_cache.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class NumpyLRUCache : \"\"\"Dict based LRU cache for numpy arrays.\"\"\" def __init__ ( self , size ): \"\"\"Crate cache.\"\"\" self . size = size self . lru_cache = collections . OrderedDict () self . common_dim = None def _get ( self , key : Any , default = None ) -> np . ndarray : try : value = self . lru_cache . pop ( key ) self . lru_cache [ key ] = value return value except KeyError : return default def _put ( self , key : Any , value : np . ndarray ): try : self . lru_cache . pop ( key ) except KeyError : if len ( self . lru_cache ) >= self . size : self . lru_cache . popitem ( last = False ) self . lru_cache [ key ] = value def cache_compute ( self , keys : List [ Any ], function : Callable ) -> Tuple [ np . ndarray , List [ Any ]]: \"\"\"Get batch from cache and compute missing. Args: keys: List of keys Returns: np.ndarray or None: Found entries concatenated over 0 axis. list(_) or None: Keys that were not found. \"\"\" if len ( self . lru_cache ) == 0 : result = function ( keys ) self . put_batch ( keys , result ) return result else : result = np . zeros (( len ( keys ), * self . common_dim )) items = [ self . _get ( key ) for key in keys ] not_found = [ key for item , key in zip ( items , keys ) if item is None ] if len ( not_found ) > 0 : computed = function ( not_found ) computed_idx = 0 for idx , item in enumerate ( items ): if item is None : result_slc = tuple ([ idx ] + [ slice ( None )] * len ( self . common_dim )) computed_slc = tuple ( [ computed_idx ] + [ slice ( None )] * len ( self . common_dim ) ) result [ result_slc ] = computed [ computed_slc ] computed_idx += 1 else : result_slc = tuple ([ idx ] + [ slice ( None )] * len ( self . common_dim )) result [ result_slc ] = item return result def put_batch ( self , keys : List [ Any ], values : np . ndarray ): \"\"\"Put batch to cache. Args: keys: List of keys values: Ndarray of shape (len(keys), *common_dim) \"\"\" self . _validate_shape ( values ) for key , item in zip ( keys , values ): self . _put ( key , item ) def _validate_shape ( self , value ): if self . common_dim is None : self . common_dim = value . shape [ 1 :] else : if self . common_dim != value . shape [ 1 :]: raise ValueError ( f \"\"\"Cached arrays must have the same shape. Shape expected: { self . common_dim } Shape found: { value . shape [ 1 :] } \"\"\" ) __init__ ( size ) Crate cache. Source code in npc_engine\\services\\utils\\lru_cache.py 10 11 12 13 14 def __init__ ( self , size ): \"\"\"Crate cache.\"\"\" self . size = size self . lru_cache = collections . OrderedDict () self . common_dim = None cache_compute ( keys , function ) Get batch from cache and compute missing. Parameters: Name Type Description Default keys List [ Any ] List of keys required Returns: Type Description np . ndarray np.ndarray or None: Found entries concatenated over 0 axis. List [ Any ] list(_) or None: Keys that were not found. Source code in npc_engine\\services\\utils\\lru_cache.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def cache_compute ( self , keys : List [ Any ], function : Callable ) -> Tuple [ np . ndarray , List [ Any ]]: \"\"\"Get batch from cache and compute missing. Args: keys: List of keys Returns: np.ndarray or None: Found entries concatenated over 0 axis. list(_) or None: Keys that were not found. \"\"\" if len ( self . lru_cache ) == 0 : result = function ( keys ) self . put_batch ( keys , result ) return result else : result = np . zeros (( len ( keys ), * self . common_dim )) items = [ self . _get ( key ) for key in keys ] not_found = [ key for item , key in zip ( items , keys ) if item is None ] if len ( not_found ) > 0 : computed = function ( not_found ) computed_idx = 0 for idx , item in enumerate ( items ): if item is None : result_slc = tuple ([ idx ] + [ slice ( None )] * len ( self . common_dim )) computed_slc = tuple ( [ computed_idx ] + [ slice ( None )] * len ( self . common_dim ) ) result [ result_slc ] = computed [ computed_slc ] computed_idx += 1 else : result_slc = tuple ([ idx ] + [ slice ( None )] * len ( self . common_dim )) result [ result_slc ] = item return result put_batch ( keys , values ) Put batch to cache. Parameters: Name Type Description Default keys List [ Any ] List of keys required values np . ndarray Ndarray of shape (len(keys), *common_dim) required Source code in npc_engine\\services\\utils\\lru_cache.py 68 69 70 71 72 73 74 75 76 77 def put_batch ( self , keys : List [ Any ], values : np . ndarray ): \"\"\"Put batch to cache. Args: keys: List of keys values: Ndarray of shape (len(keys), *common_dim) \"\"\" self . _validate_shape ( values ) for key , item in zip ( keys , values ): self . _put ( key , item ) mock Utility script to mock models for testing. build_output_shape_tensor_ ( graph , output , dynamic_shape_map , shape_name = 'dynamic_shape' , override = {}) Build output shape tensor for dynamic shape models. Parameters: Name Type Description Default graph Graph to add the output shape tensor to. required output Model output. required dynamic_shape_map Map of input names to their dynamic shape indices. required shape_name Name of the output shape tensor. 'dynamic_shape' Source code in npc_engine\\services\\utils\\mock.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def build_output_shape_tensor_ ( graph , output , dynamic_shape_map , shape_name = \"dynamic_shape\" , override = {} ): \"\"\"Build output shape tensor for dynamic shape models. Args: graph: Graph to add the output shape tensor to. output: Model output. dynamic_shape_map: Map of input names to their dynamic shape indices. shape_name: Name of the output shape tensor. \"\"\" dimensions_retrieved = [] for i , dim in enumerate ( output . type . tensor_type . shape . dim ): if \" + \" in dim . dim_param : dim1 , dim2 = dim . dim_param . split ( \" + \" ) create_dim_variable_ ( graph , shape_name , dim1 , override . get ( dim1 , dim . dim_value ), i , dynamic_shape_map , postfix = \"1\" , ) create_dim_variable_ ( graph , shape_name , dim2 , override . get ( dim2 , dim . dim_value ), i , dynamic_shape_map , postfix = \"2\" , ) so . add_node ( graph , so . node ( \"Add\" , inputs = [ f \" { shape_name } _ { i } _1\" , f \" { shape_name } _ { i } _2\" ], outputs = [ f \" { shape_name } _ { i } \" ], ), ) else : create_dim_variable_ ( graph , shape_name , dim . dim_param , override . get ( dim . dim_param , dim . dim_value ), i , dynamic_shape_map , ) dimensions_retrieved . append ( f \" { shape_name } _ { i } \" ) node = so . node ( \"Concat\" , inputs = dimensions_retrieved , outputs = [ f \" { shape_name } \" ], axis = 0 , ) so . add_node ( graph , node ) create_dim_variable_ ( graph , shape_name , dim_param , dim_value , dim_id , dynamic_shape_map , postfix = None ) Create a dimension variable for a dynamic shape model. Parameters: Name Type Description Default graph Graph to add the dimension variable to. required shape_name Name of the output shape tensor. required dim_param Dimension parameter name. required dim_value Dimension value. required dim_id Index of the dimension variable. required dynamic_shape_map Map of dynamic axes names to their inputs indices. required Source code in npc_engine\\services\\utils\\mock.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def create_dim_variable_ ( graph , shape_name , dim_param , dim_value , dim_id , dynamic_shape_map , postfix = None ): \"\"\"Create a dimension variable for a dynamic shape model. Args: graph: Graph to add the dimension variable to. shape_name: Name of the output shape tensor. dim_param: Dimension parameter name. dim_value: Dimension value. dim_id: Index of the dimension variable. dynamic_shape_map: Map of dynamic axes names to their inputs indices. \"\"\" if dim_param != \"\" and dim_param in dynamic_shape_map : node1 = so . node ( \"Shape\" , inputs = [ dynamic_shape_map [ dim_param ][ 0 ]], outputs = [ f \" { shape_name } _ { dim_id } \" if postfix is None else f \" { shape_name } _ { dim_id } _ { postfix } \" ], start = dynamic_shape_map [ dim_param ][ 1 ], end = dynamic_shape_map [ dim_param ][ 1 ] + 1 , ) so . add_node ( graph , node1 ) else : so . add_constant ( graph , f \" { shape_name } _ { dim_id } \" if postfix is None else f \" { shape_name } _ { dim_id } _ { postfix } \" , np . array ([ dim_value if dim_value != 0 else 1 ], dtype = np . int64 ,), data_type = \"INT64\" , ) create_stub_onnx_model ( onnx_model_path , output_path , dynamic_shape_values ) Create stub onnx model for tests with correct input and output shapes and names. Parameters: Name Type Description Default onnx_model_path str Path to the onnx model. required output_path str Path to the output mock model. required dynamic_shape_values dict Map to specify dynamic dimension values. required Source code in npc_engine\\services\\utils\\mock.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def create_stub_onnx_model ( onnx_model_path : str , output_path : str , dynamic_shape_values : dict ): \"\"\"Create stub onnx model for tests with correct input and output shapes and names. Args: onnx_model_path: Path to the onnx model. output_path: Path to the output mock model. dynamic_shape_values: Map to specify dynamic dimension values. \"\"\" onnx_model = so . graph_from_file ( onnx_model_path ) inputs = onnx_model . input outputs = onnx_model . output mock_graph = so . empty_graph () inverse_data_dict = { value : key for key , value in glob . DATA_TYPES . items ()} dynamic_shape_map = {} for input_ in inputs : so . add_input ( mock_graph , name = input_ . name , dimensions = [ dim . dim_param if dim . dim_param != \"\" else dim . dim_value for dim in input_ . type . tensor_type . shape . dim ], data_type = inverse_data_dict [ input_ . type . tensor_type . elem_type ], ) for i , dim in enumerate ( input_ . type . tensor_type . shape . dim ): if dim . dim_param != \"\" : dynamic_shape_map [ dim . dim_param ] = ( input_ . name , i ) for output in outputs : so . add_output ( mock_graph , name = output . name , dimensions = [ dim . dim_param if dim . dim_param != \"\" else dim . dim_value for dim in output . type . tensor_type . shape . dim ], data_type = inverse_data_dict [ output . type . tensor_type . elem_type ], ) input_names = [ inp . name for inp in inputs ] if output . name not in input_names : build_output_shape_tensor_ ( mock_graph , output , dynamic_shape_map , f \"dynamic_shape_ { output . name } \" , override = dynamic_shape_values , ) node = so . node ( \"ConstantOfShape\" , inputs = [ f \"dynamic_shape_ { output . name } \" ], outputs = [ output . name ], value = xhelp . make_tensor ( name = f \"dynamic_shape_ { output . name } _value\" , data_type = output . type . tensor_type . elem_type , dims = [ 1 ], vals = [ 0 ], ), name = f \"ConstantOfShape_ { output . name } \" , ) so . add_node ( mock_graph , node ) so . graph_to_file ( mock_graph , output_path , onnx_opset_version = 15 ) main ( onnx_model_path , output_path , dynamic_dim ) Create stub onnx model for tests with correct input and output shapes and names. Source code in npc_engine\\services\\utils\\mock.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 @click . command () @click . option ( \"-d\" , \"--dynamic-dim\" , type = str , multiple = True , help = \"Specify dynamic dimension in format `<dim name>:<dim value>`.\" , ) @click . option ( \"-m\" , \"--onnx-model-path\" , required = True , type = str ) @click . option ( \"-o\" , \"--output-path\" , required = True , type = str ) def main ( onnx_model_path : str , output_path : str , dynamic_dim : List [ str ]): \"\"\"Create stub onnx model for tests with correct input and output shapes and names.\"\"\" dynamic_dims_map = {} for dim in dynamic_dim : dim_name , dim_value = dim . split ( \":\" ) dynamic_dims_map [ dim_name ] = int ( dim_value ) create_stub_onnx_model ( onnx_model_path , output_path , dynamic_dims_map ) text from https://github.com/keithito/tacotron cleaners adapted from https://github.com/keithito/tacotron. Cleaners are transformations that run over the input text at both training and eval time. Cleaners can be selected by passing a comma-delimited list of cleaner names as the \"cleaners\" hyperparameter. Some cleaners are English-specific. You'll typically want to use: 1. \"english_cleaners\" for English text 2. \"transliteration_cleaners\" for non-English text that can be transliterated to ASCII using the Unidecode library (https://pypi.python.org/pypi/Unidecode) 3. \"basic_cleaners\" if you do not want to transliterate (in this case, you should also update the symbols in symbols.py to match your data). flowtron_cleaners ( text ) Clean text with a set of cleaners. Source code in npc_engine\\text\\cleaners.py 98 99 100 101 102 103 104 105 def flowtron_cleaners ( text ): \"\"\"Clean text with a set of cleaners.\"\"\" text = collapse_whitespace ( text ) text = remove_hyphens ( text ) text = expand_datestime ( text ) text = expand_numbers ( text ) text = expand_safe_abbreviations ( text ) return text numbers from https://github.com/keithito/tacotron symbols from https://github.com/keithito/tacotron version This module contains project version information.","title":"Reference"},{"location":"inference_engine/reference/#npc_engine.cli","text":"This is the entry point for the command-line interface that starts npc-engine server.","title":"cli"},{"location":"inference_engine/reference/#npc_engine.cli.cli","text":"NPC engine JSON RPC server CLI. Source code in npc_engine\\cli.py 26 27 28 29 30 31 32 33 34 35 36 37 38 @click . group () @click . option ( \"--verbose/--silent\" , \"-v\" , default = False , help = \"Enable verbose output.\" ) def cli ( verbose : bool ): \"\"\"NPC engine JSON RPC server CLI.\"\"\" # Use the verbosity count to determine the logging level... logger . remove () if verbose : logger . add ( sys . stdout , format = \" {time} {level} {message} \" , level = \"INFO\" , enqueue = True ) click . echo ( click . style ( \"Verbose logging is enabled. (LEVEL=INFO)\" , fg = \"yellow\" ,) )","title":"cli()"},{"location":"inference_engine/reference/#npc_engine.cli.download_default_models","text":"Download default models into the folder. Source code in npc_engine\\cli.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 @cli . command () @click . option ( \"--models-path\" , default = os . environ . get ( \"NPC_ENGINE_MODELS_PATH\" , \"./models\" ), ) def download_default_models ( models_path : str ): \"\"\"Download default models into the folder.\"\"\" model_names = [ \"npc-engine/exported-paraphrase-MiniLM-L6-v2\" , \"npc-engine/exported-bart-light-gail-chatbot\" , \"npc-engine/exported-nemo-quartznet-ctc-stt\" , \"npc-engine/exported-flowtron-waveglow-librispeech-tts\" , ] for model in model_names : logger . info ( \"Downloading model {} \" , model ) logger . info ( \"Downloading {} \" , model ) snapshot_download ( repo_id = model , revision = \"main\" , cache_dir = models_path )","title":"download_default_models()"},{"location":"inference_engine/reference/#npc_engine.cli.download_model","text":"Download the model. Source code in npc_engine\\cli.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 @cli . command () @click . option ( \"--models-path\" , default = os . environ . get ( \"NPC_ENGINE_MODELS_PATH\" , \"./models\" ), ) @click . argument ( \"model_id\" ) def download_model ( models_path : str , model_id : str ): \"\"\"Download the model.\"\"\" model_correct = validate_hub_model ( model_id ) if model_correct : logger . info ( \"Downloading model {} \" , model_id ) snapshot_download ( repo_id = model_id , revision = \"main\" , cache_dir = models_path ) else : if click . confirm ( click . style ( f \" { model_id } is not a valid npc-engine model.\" + \" \\n Do you want to export it?\" , fg = \"yellow\" , ) ): export_model ( models_path , model_id , True )","title":"download_model()"},{"location":"inference_engine/reference/#npc_engine.cli.export_model","text":"Export the model. Source code in npc_engine\\cli.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 @cli . command () @click . option ( \"--models-path\" , default = os . environ . get ( \"NPC_ENGINE_MODELS_PATH\" , \"./models\" ), ) @click . argument ( \"model_id\" ) def export_model ( models_path : str , model_id : str , remove_source : bool = False ): \"\"\"Export the model.\"\"\" from npc_engine.exporters.base_exporter import Exporter logger . info ( \"Downloading source model {} \" , model_id ) if os . path . exists ( model_id ): source_path = model_id else : source_path = snapshot_download ( repo_id = model_id , revision = \"main\" , cache_dir = models_path ) remove_source = True export_path = os . path . join ( models_path , \"exported-\" + model_id . replace ( \" \\\\ \" , \"/\" ) . split ( \"/\" )[ - 1 ] ) os . makedirs ( export_path , exist_ok = True ) logger . info ( \"Exporting model {} to {} \" , model_id , export_path ) exporters = Exporter . get_exporters () click . echo ( \"Available exporters:\" ) for i , exporter in enumerate ( exporters ): click . echo ( f \" { i + 1 } . { exporter . description () } \" ) exporter_id = click . prompt ( \"Please select an exporter\" , type = int ) exporter = exporters [ exporter_id - 1 ] exporter . export ( source_path , export_path ) exporter . create_config ( export_path ) if remove_source : shutil . rmtree ( source_path )","title":"export_model()"},{"location":"inference_engine/reference/#npc_engine.cli.list_models","text":"List the models in the folder. Source code in npc_engine\\cli.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 @cli . command () @click . option ( \"--models-path\" , default = os . environ . get ( \"NPC_ENGINE_MODELS_PATH\" , \"./models\" ), ) def list_models ( models_path : str ): \"\"\"List the models in the folder.\"\"\" from npc_engine.service_manager.service_manager import ServiceManager model_manager = ServiceManager ( models_path ) metadata_list = model_manager . get_services_metadata () for metadata in metadata_list : click . echo ( metadata [ \"id\" ]) click . echo ( metadata [ \"type\" ]) click . echo ( \"Service description:\" ) click . echo ( metadata [ \"service_short_description\" ]) click . echo ( \"Model description:\" ) click . echo ( metadata [ \"readme\" ]) click . echo ( \"--------------------\" )","title":"list_models()"},{"location":"inference_engine/reference/#npc_engine.cli.run","text":"Load the models and start JSONRPC server. Source code in npc_engine\\cli.py 41 42 43 44 45 46 47 48 49 50 51 52 53 @cli . command () @click . option ( \"--models-path\" , default = \"./models\" ) @click . option ( \"--port\" , default = \"5555\" ) @click . option ( \"--start-all/--dont-start\" , default = True ) def run ( models_path : str , port : str , start_all : bool ): \"\"\"Load the models and start JSONRPC server.\"\"\" from npc_engine.service_manager.service_manager import ServiceManager from npc_engine.service_manager.server import Server context = zmq . asyncio . Context ( io_threads = 5 ) model_manager = ServiceManager ( context , models_path ) server = Server ( context , model_manager , port , start_all ) server . run ()","title":"run()"},{"location":"inference_engine/reference/#npc_engine.cli.set_models_path","text":"Set the models path. Source code in npc_engine\\cli.py 74 75 76 77 78 @cli . command () @click . option ( \"--models-path\" , type = click . Path ( exists = True )) def set_models_path ( models_path : str ): \"\"\"Set the models path.\"\"\" os . environ [ \"NPC_ENGINE_MODELS_PATH\" ] = models_path","title":"set_models_path()"},{"location":"inference_engine/reference/#npc_engine.cli.test_model","text":"Send test request to the model and print reply. Source code in npc_engine\\cli.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 @cli . command () @click . option ( \"--models-path\" , default = os . environ . get ( \"NPC_ENGINE_MODELS_PATH\" , \"./models\" ), ) @click . argument ( \"model_id\" ) def test_model ( models_path : str , model_id : str ): \"\"\"Send test request to the model and print reply.\"\"\" from npc_engine.exporters.base_exporter import Exporter if not validate_local_model ( models_path , model_id ): click . echo ( click . style ( f \" { ( model_id ) } is not a valid npc-engine model.\" , fg = \"red\" ,) ) return 1 model_type = get_model_type_name ( models_path , model_id ) exporters = Exporter . get_exporters () for exporter in exporters : if exporter . get_model_name () == model_type : exporter . test_model ( models_path , model_id ) return 0","title":"test_model()"},{"location":"inference_engine/reference/#npc_engine.cli.version","text":"Get the npc engine version. Source code in npc_engine\\cli.py 180 181 182 183 @cli . command () def version (): \"\"\"Get the npc engine version.\"\"\" click . echo ( click . style ( f \" { __version__ } \" , bold = True ))","title":"version()"},{"location":"inference_engine/reference/#npc_engine.exporters","text":"","title":"exporters"},{"location":"inference_engine/reference/#npc_engine.exporters.base_exporter","text":"Module with Exporters base class.","title":"base_exporter"},{"location":"inference_engine/reference/#npc_engine.exporters.base_exporter.Exporter","text":"Bases: ABC Abstract base class for exporter. Exporters are classes that handle converting models to be used with npc-engine. Source code in npc_engine\\exporters\\base_exporter.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class Exporter ( ABC ): \"\"\"Abstract base class for exporter. Exporters are classes that handle converting models to be used with npc-engine. \"\"\" exporters = {} def __init_subclass__ ( cls , ** kwargs ): \"\"\"Init subclass where model classes get registered to be loadable.\"\"\" super () . __init_subclass__ ( ** kwargs ) cls . exporters [ cls . __name__ ] = cls @classmethod def get_exporters ( cls ) -> List [ Any ]: \"\"\"Create all exporters.\"\"\" return [ cls . exporters [ name ]() for name in cls . exporters if not inspect . isabstract ( cls . exporters [ name ]) ] @classmethod def description ( cls ) -> str : \"\"\"Print the exporter.\"\"\" return cls . __name__ + \" \\n\\t \" + cls . __doc__ . split ( \" \\n\\n \" )[ 0 ] @abstractmethod def export ( self , model_path : str , export_path : str ): \"\"\"Export the model to the export path.\"\"\" pass @abstractmethod def create_config ( self , export_path : str ): \"\"\"Create the config for the model.\"\"\" pass @classmethod @abstractmethod def get_api ( cls ) -> str : \"\"\"Get the api for the exporter.\"\"\" pass @classmethod @abstractmethod def get_model_name ( cls ) -> str : \"\"\"Get the model name.\"\"\" pass def test_model ( self , models_path : str , model_id : str ): \"\"\"Test the model.\"\"\" start_server = click . confirm ( \"Start testing server? (It should be already running otherwise)\" ) if start_server : start_test_server ( \"5555\" , models_path ) self . test_model_impl ( models_path , model_id ) @abstractmethod def test_model_impl ( self , models_path : str , model_id : str ): \"\"\"Test the model.\"\"\" pass","title":"Exporter"},{"location":"inference_engine/reference/#npc_engine.exporters.base_exporter.Exporter.__init_subclass__","text":"Init subclass where model classes get registered to be loadable. Source code in npc_engine\\exporters\\base_exporter.py 17 18 19 20 def __init_subclass__ ( cls , ** kwargs ): \"\"\"Init subclass where model classes get registered to be loadable.\"\"\" super () . __init_subclass__ ( ** kwargs ) cls . exporters [ cls . __name__ ] = cls","title":"__init_subclass__()"},{"location":"inference_engine/reference/#npc_engine.exporters.base_exporter.Exporter.create_config","text":"Create the config for the model. Source code in npc_engine\\exporters\\base_exporter.py 41 42 43 44 @abstractmethod def create_config ( self , export_path : str ): \"\"\"Create the config for the model.\"\"\" pass","title":"create_config()"},{"location":"inference_engine/reference/#npc_engine.exporters.base_exporter.Exporter.description","text":"Print the exporter. Source code in npc_engine\\exporters\\base_exporter.py 31 32 33 34 @classmethod def description ( cls ) -> str : \"\"\"Print the exporter.\"\"\" return cls . __name__ + \" \\n\\t \" + cls . __doc__ . split ( \" \\n\\n \" )[ 0 ]","title":"description()"},{"location":"inference_engine/reference/#npc_engine.exporters.base_exporter.Exporter.export","text":"Export the model to the export path. Source code in npc_engine\\exporters\\base_exporter.py 36 37 38 39 @abstractmethod def export ( self , model_path : str , export_path : str ): \"\"\"Export the model to the export path.\"\"\" pass","title":"export()"},{"location":"inference_engine/reference/#npc_engine.exporters.base_exporter.Exporter.get_api","text":"Get the api for the exporter. Source code in npc_engine\\exporters\\base_exporter.py 46 47 48 49 50 @classmethod @abstractmethod def get_api ( cls ) -> str : \"\"\"Get the api for the exporter.\"\"\" pass","title":"get_api()"},{"location":"inference_engine/reference/#npc_engine.exporters.base_exporter.Exporter.get_exporters","text":"Create all exporters. Source code in npc_engine\\exporters\\base_exporter.py 22 23 24 25 26 27 28 29 @classmethod def get_exporters ( cls ) -> List [ Any ]: \"\"\"Create all exporters.\"\"\" return [ cls . exporters [ name ]() for name in cls . exporters if not inspect . isabstract ( cls . exporters [ name ]) ]","title":"get_exporters()"},{"location":"inference_engine/reference/#npc_engine.exporters.base_exporter.Exporter.get_model_name","text":"Get the model name. Source code in npc_engine\\exporters\\base_exporter.py 52 53 54 55 56 @classmethod @abstractmethod def get_model_name ( cls ) -> str : \"\"\"Get the model name.\"\"\" pass","title":"get_model_name()"},{"location":"inference_engine/reference/#npc_engine.exporters.base_exporter.Exporter.test_model","text":"Test the model. Source code in npc_engine\\exporters\\base_exporter.py 58 59 60 61 62 63 64 65 def test_model ( self , models_path : str , model_id : str ): \"\"\"Test the model.\"\"\" start_server = click . confirm ( \"Start testing server? (It should be already running otherwise)\" ) if start_server : start_test_server ( \"5555\" , models_path ) self . test_model_impl ( models_path , model_id )","title":"test_model()"},{"location":"inference_engine/reference/#npc_engine.exporters.base_exporter.Exporter.test_model_impl","text":"Test the model. Source code in npc_engine\\exporters\\base_exporter.py 67 68 69 70 @abstractmethod def test_model_impl ( self , models_path : str , model_id : str ): \"\"\"Test the model.\"\"\" pass","title":"test_model_impl()"},{"location":"inference_engine/reference/#npc_engine.exporters.base_hf_exporter","text":"Abstract class for huggingface exporters.","title":"base_hf_exporter"},{"location":"inference_engine/reference/#npc_engine.exporters.base_hf_exporter.BaseHfExporter","text":"Bases: Exporter Exporter for the Huggingface transformer models. Source code in npc_engine\\exporters\\base_hf_exporter.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class BaseHfExporter ( Exporter ): \"\"\"Exporter for the Huggingface transformer models.\"\"\" def export ( self , model_path : str , export_path : str ): \"\"\"Export the model to the export path.\"\"\" click . echo ( \"Exporting model to onnx\" ) sys . argv = [ \"onnx_converter\" , \"--model\" , model_path , \"--atol\" , \"0.0001\" , \"--feature\" , self . get_export_feature (), export_path , ] from transformers.onnx import __main__ __main__ . logger = logger __main__ . main () tokenizer = AutoTokenizer . from_pretrained ( model_path ) tokenizer . save_pretrained ( export_path ) @abstractmethod def get_export_feature ( self ) -> str : \"\"\"Get the transformers.onnx feature argument that describes what interface model should have. Possible values are: 'causal-lm', 'causal-lm-with-past', 'default', 'default-with-past', 'masked-lm', 'question-answering', 'seq2seq-lm', 'seq2seq-lm-with-past', 'sequence-classification', 'token-classification' \"\"\" pass","title":"BaseHfExporter"},{"location":"inference_engine/reference/#npc_engine.exporters.base_hf_exporter.BaseHfExporter.export","text":"Export the model to the export path. Source code in npc_engine\\exporters\\base_hf_exporter.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def export ( self , model_path : str , export_path : str ): \"\"\"Export the model to the export path.\"\"\" click . echo ( \"Exporting model to onnx\" ) sys . argv = [ \"onnx_converter\" , \"--model\" , model_path , \"--atol\" , \"0.0001\" , \"--feature\" , self . get_export_feature (), export_path , ] from transformers.onnx import __main__ __main__ . logger = logger __main__ . main () tokenizer = AutoTokenizer . from_pretrained ( model_path ) tokenizer . save_pretrained ( export_path )","title":"export()"},{"location":"inference_engine/reference/#npc_engine.exporters.base_hf_exporter.BaseHfExporter.get_export_feature","text":"Get the transformers.onnx feature argument that describes what interface model should have. Possible values are 'causal-lm', 'causal-lm-with-past', 'default', 'default-with-past', 'masked-lm', 'question-answering', 'seq2seq-lm', 'seq2seq-lm-with-past', 'sequence-classification', 'token-classification' Source code in npc_engine\\exporters\\base_hf_exporter.py 34 35 36 37 38 39 40 41 42 43 @abstractmethod def get_export_feature ( self ) -> str : \"\"\"Get the transformers.onnx feature argument that describes what interface model should have. Possible values are: 'causal-lm', 'causal-lm-with-past', 'default', 'default-with-past', 'masked-lm', 'question-answering', 'seq2seq-lm', 'seq2seq-lm-with-past', 'sequence-classification', 'token-classification' \"\"\" pass","title":"get_export_feature()"},{"location":"inference_engine/reference/#npc_engine.exporters.hf_chatbot_exporter","text":"Exporter implementation for the Huggingface text generation models.","title":"hf_chatbot_exporter"},{"location":"inference_engine/reference/#npc_engine.exporters.hf_chatbot_exporter.HfChatbotExporter","text":"Bases: BaseHfExporter Base exporter for Huggingface transformer models to chatbot API. Source code in npc_engine\\exporters\\hf_chatbot_exporter.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 class HfChatbotExporter ( BaseHfExporter ): \"\"\"Base exporter for Huggingface transformer models to chatbot API.\"\"\" SUPPORTED_FEATURES = [ \"causal-lm\" , \"causal-lm-with-past\" , \"seq2seq-lm\" , \"seq2seq-lm-with-past\" , ] @classmethod def get_api ( cls ) -> str : \"\"\"Get the api for the exporter.\"\"\" return \"ChatbotAPI\" @classmethod def get_model_name ( cls ): \"\"\"Get the model name.\"\"\" return \"HfChatbot\" def create_config ( self , export_path : str ): \"\"\"Create the config for the model.\"\"\" config_dict = {} config_dict [ \"model_type\" ] = self . get_model_name () config_dict [ \"template_string\" ] = click . edit ( \"{# \\n \" + \"Please create a template that will map context fields to prompt \\n \" + \"Any context fields defined here must be then sent over the request via json as context arg \\n \" + \"See Jinja docs for template design https://jinja.palletsprojects.com/en/3.1.x/templates/# \\n \" + \"this example expects a list of strings named history \\n \" + \"#} \\n \" + \"{ % f or line in history %} \\n \" + \"{{ line }} \\n \" + \"{ % e ndfor -%}\" ) config_dict [ \"min_length\" ] = click . prompt ( \"Minimum generation length:\" , type = int , default = 3 ) config_dict [ \"max_length\" ] = click . prompt ( \"Maximum generation length (affects performance):\" , type = int , default = 128 ) config_dict [ \"repetition_penalty\" ] = click . prompt ( \"Repetition penalty (probability multiplier for repeated tokens):\" , type = float , default = 1.0 , ) yaml . dump ( config_dict , open ( os . path . join ( export_path , \"config.yml\" ), \"w\" )) def get_export_feature ( self ) -> str : \"\"\"Select and return hf feature for export.\"\"\" return click . prompt ( \"Please select one of the supported features (refer to huggingface docs for details):\" , type = click . Choice ( self . SUPPORTED_FEATURES ), show_default = False , ) def test_model_impl ( self , models_path : str , model_id : str ): \"\"\"Run test request. Args: models_path: path to models model_id: model id (directory name of the model) \"\"\" config_path = os . path . join ( models_path , model_id , \"config.yml\" ) with open ( config_path , \"r\" ) as f : config = yaml . load ( f , Loader = yaml . FullLoader ) use_default_text = click . confirm ( \"Use default text for variables?\" ) schema = to_json_schema ( infer ( config [ \"template_string\" ])) if use_default_text : def get_text ( _ ): return \"Hello world\" else : def get_text ( field_name ): click . prompt ( f \"Please enter { field_name } :\" ) context = schema_to_json ( schema , get_text ) print ( f \"Context: { context } \" ) zmq_context = zmq . Context () control_client = ControlClient ( zmq_context , \"5555\" ) chatbot_client = ChatbotClient ( zmq_context , \"5555\" , model_id ) control_client . start_service ( model_id ) time . sleep ( 1 ) response = None while response is None : try : response = chatbot_client . generate_reply ( context ) except RuntimeError as e : if \"is not running\" in str ( e ): print ( \"Model is not running, waiting for it to start..\" ) time . sleep ( 1 ) else : raise e click . echo ( click . style ( \"Request context:\" , fg = \"green\" )) click . echo ( json . dumps ( context , indent = 2 )) click . echo ( click . style ( \"Reply:\" , fg = \"green\" )) click . echo ( json . dumps ( response , indent = 2 ))","title":"HfChatbotExporter"},{"location":"inference_engine/reference/#npc_engine.exporters.hf_chatbot_exporter.HfChatbotExporter.create_config","text":"Create the config for the model. Source code in npc_engine\\exporters\\hf_chatbot_exporter.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def create_config ( self , export_path : str ): \"\"\"Create the config for the model.\"\"\" config_dict = {} config_dict [ \"model_type\" ] = self . get_model_name () config_dict [ \"template_string\" ] = click . edit ( \"{# \\n \" + \"Please create a template that will map context fields to prompt \\n \" + \"Any context fields defined here must be then sent over the request via json as context arg \\n \" + \"See Jinja docs for template design https://jinja.palletsprojects.com/en/3.1.x/templates/# \\n \" + \"this example expects a list of strings named history \\n \" + \"#} \\n \" + \"{ % f or line in history %} \\n \" + \"{{ line }} \\n \" + \"{ % e ndfor -%}\" ) config_dict [ \"min_length\" ] = click . prompt ( \"Minimum generation length:\" , type = int , default = 3 ) config_dict [ \"max_length\" ] = click . prompt ( \"Maximum generation length (affects performance):\" , type = int , default = 128 ) config_dict [ \"repetition_penalty\" ] = click . prompt ( \"Repetition penalty (probability multiplier for repeated tokens):\" , type = float , default = 1.0 , ) yaml . dump ( config_dict , open ( os . path . join ( export_path , \"config.yml\" ), \"w\" ))","title":"create_config()"},{"location":"inference_engine/reference/#npc_engine.exporters.hf_chatbot_exporter.HfChatbotExporter.get_api","text":"Get the api for the exporter. Source code in npc_engine\\exporters\\hf_chatbot_exporter.py 25 26 27 28 @classmethod def get_api ( cls ) -> str : \"\"\"Get the api for the exporter.\"\"\" return \"ChatbotAPI\"","title":"get_api()"},{"location":"inference_engine/reference/#npc_engine.exporters.hf_chatbot_exporter.HfChatbotExporter.get_export_feature","text":"Select and return hf feature for export. Source code in npc_engine\\exporters\\hf_chatbot_exporter.py 63 64 65 66 67 68 69 def get_export_feature ( self ) -> str : \"\"\"Select and return hf feature for export.\"\"\" return click . prompt ( \"Please select one of the supported features (refer to huggingface docs for details):\" , type = click . Choice ( self . SUPPORTED_FEATURES ), show_default = False , )","title":"get_export_feature()"},{"location":"inference_engine/reference/#npc_engine.exporters.hf_chatbot_exporter.HfChatbotExporter.get_model_name","text":"Get the model name. Source code in npc_engine\\exporters\\hf_chatbot_exporter.py 30 31 32 33 @classmethod def get_model_name ( cls ): \"\"\"Get the model name.\"\"\" return \"HfChatbot\"","title":"get_model_name()"},{"location":"inference_engine/reference/#npc_engine.exporters.hf_chatbot_exporter.HfChatbotExporter.test_model_impl","text":"Run test request. Parameters: Name Type Description Default models_path str path to models required model_id str model id (directory name of the model) required Source code in npc_engine\\exporters\\hf_chatbot_exporter.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def test_model_impl ( self , models_path : str , model_id : str ): \"\"\"Run test request. Args: models_path: path to models model_id: model id (directory name of the model) \"\"\" config_path = os . path . join ( models_path , model_id , \"config.yml\" ) with open ( config_path , \"r\" ) as f : config = yaml . load ( f , Loader = yaml . FullLoader ) use_default_text = click . confirm ( \"Use default text for variables?\" ) schema = to_json_schema ( infer ( config [ \"template_string\" ])) if use_default_text : def get_text ( _ ): return \"Hello world\" else : def get_text ( field_name ): click . prompt ( f \"Please enter { field_name } :\" ) context = schema_to_json ( schema , get_text ) print ( f \"Context: { context } \" ) zmq_context = zmq . Context () control_client = ControlClient ( zmq_context , \"5555\" ) chatbot_client = ChatbotClient ( zmq_context , \"5555\" , model_id ) control_client . start_service ( model_id ) time . sleep ( 1 ) response = None while response is None : try : response = chatbot_client . generate_reply ( context ) except RuntimeError as e : if \"is not running\" in str ( e ): print ( \"Model is not running, waiting for it to start..\" ) time . sleep ( 1 ) else : raise e click . echo ( click . style ( \"Request context:\" , fg = \"green\" )) click . echo ( json . dumps ( context , indent = 2 )) click . echo ( click . style ( \"Reply:\" , fg = \"green\" )) click . echo ( json . dumps ( response , indent = 2 ))","title":"test_model_impl()"},{"location":"inference_engine/reference/#npc_engine.exporters.hf_classifier_exporter","text":"Exporter for huggingface sequence classification models.","title":"hf_classifier_exporter"},{"location":"inference_engine/reference/#npc_engine.exporters.hf_classifier_exporter.HfClassifierExporter","text":"Bases: BaseHfExporter Exporter for the Huggingface classification models. Source code in npc_engine\\exporters\\hf_classifier_exporter.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 class HfClassifierExporter ( BaseHfExporter ): \"\"\"Exporter for the Huggingface classification models.\"\"\" def get_api ( self ) -> str : \"\"\"Get the api for the exporter.\"\"\" return \"SequenceClassifierAPI\" @classmethod def get_model_name ( cls ): \"\"\"Get the model name.\"\"\" return \"HfClassifier\" def create_config ( self , export_path : str ): \"\"\"Create the config for the model.\"\"\" config_dict = {} config_dict [ \"type\" ] = self . get_model_name () config_dict [ \"cache_size\" ] = click . prompt ( \"Cache size\" , type = int ) with open ( os . path . join ( export_path , \"config.yml\" ), \"w\" ) as f : yaml . dump ( config_dict , f ) def get_export_feature ( self ) -> str : \"\"\"Create the config for the model.\"\"\" return \"sequence-classification\" def test_model_impl ( self , models_path : str , model_id : str ): \"\"\"Run test request. Args: models_path: path to models model_id: model id (directory name of the model) \"\"\" use_default_text = click . confirm ( \"Use default text for variables?\" ) if use_default_text : texts = [[ \"Hello\" , \"world\" ], \"Hello world\" ] else : texts = [ click . prompt ( \"Text 1\" ), click . prompt ( \"Text 2\" )] zmq_context = zmq . Context () control_client = ControlClient ( zmq_context , \"5555\" ) service_client = SequenceClassifierClient ( zmq_context , \"5555\" , model_id ) control_client . start_service ( model_id ) ready = False while not ready : time . sleep ( 1 ) status = control_client . get_service_status ( model_id ) ready = status == \"running\" if status == \"error\" : raise Exception ( \"Error while loading model\" ) if not ready : print ( \"Waiting for service to start...\" ) scores = service_client . classify ( texts ) click . echo ( click . style ( \"Scores:\" , fg = \"green\" )) click . echo ( json . dumps ( scores , indent = 2 )) click . echo ( click . style ( \"Argmax classes:\" , fg = \"green\" )) click . echo ( json . dumps ([ classes . index ( max ( classes )) for classes in scores ], indent = 2 ) )","title":"HfClassifierExporter"},{"location":"inference_engine/reference/#npc_engine.exporters.hf_classifier_exporter.HfClassifierExporter.create_config","text":"Create the config for the model. Source code in npc_engine\\exporters\\hf_classifier_exporter.py 24 25 26 27 28 29 30 def create_config ( self , export_path : str ): \"\"\"Create the config for the model.\"\"\" config_dict = {} config_dict [ \"type\" ] = self . get_model_name () config_dict [ \"cache_size\" ] = click . prompt ( \"Cache size\" , type = int ) with open ( os . path . join ( export_path , \"config.yml\" ), \"w\" ) as f : yaml . dump ( config_dict , f )","title":"create_config()"},{"location":"inference_engine/reference/#npc_engine.exporters.hf_classifier_exporter.HfClassifierExporter.get_api","text":"Get the api for the exporter. Source code in npc_engine\\exporters\\hf_classifier_exporter.py 15 16 17 def get_api ( self ) -> str : \"\"\"Get the api for the exporter.\"\"\" return \"SequenceClassifierAPI\"","title":"get_api()"},{"location":"inference_engine/reference/#npc_engine.exporters.hf_classifier_exporter.HfClassifierExporter.get_export_feature","text":"Create the config for the model. Source code in npc_engine\\exporters\\hf_classifier_exporter.py 32 33 34 def get_export_feature ( self ) -> str : \"\"\"Create the config for the model.\"\"\" return \"sequence-classification\"","title":"get_export_feature()"},{"location":"inference_engine/reference/#npc_engine.exporters.hf_classifier_exporter.HfClassifierExporter.get_model_name","text":"Get the model name. Source code in npc_engine\\exporters\\hf_classifier_exporter.py 19 20 21 22 @classmethod def get_model_name ( cls ): \"\"\"Get the model name.\"\"\" return \"HfClassifier\"","title":"get_model_name()"},{"location":"inference_engine/reference/#npc_engine.exporters.hf_classifier_exporter.HfClassifierExporter.test_model_impl","text":"Run test request. Parameters: Name Type Description Default models_path str path to models required model_id str model id (directory name of the model) required Source code in npc_engine\\exporters\\hf_classifier_exporter.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def test_model_impl ( self , models_path : str , model_id : str ): \"\"\"Run test request. Args: models_path: path to models model_id: model id (directory name of the model) \"\"\" use_default_text = click . confirm ( \"Use default text for variables?\" ) if use_default_text : texts = [[ \"Hello\" , \"world\" ], \"Hello world\" ] else : texts = [ click . prompt ( \"Text 1\" ), click . prompt ( \"Text 2\" )] zmq_context = zmq . Context () control_client = ControlClient ( zmq_context , \"5555\" ) service_client = SequenceClassifierClient ( zmq_context , \"5555\" , model_id ) control_client . start_service ( model_id ) ready = False while not ready : time . sleep ( 1 ) status = control_client . get_service_status ( model_id ) ready = status == \"running\" if status == \"error\" : raise Exception ( \"Error while loading model\" ) if not ready : print ( \"Waiting for service to start...\" ) scores = service_client . classify ( texts ) click . echo ( click . style ( \"Scores:\" , fg = \"green\" )) click . echo ( json . dumps ( scores , indent = 2 )) click . echo ( click . style ( \"Argmax classes:\" , fg = \"green\" )) click . echo ( json . dumps ([ classes . index ( max ( classes )) for classes in scores ], indent = 2 ) )","title":"test_model_impl()"},{"location":"inference_engine/reference/#npc_engine.exporters.hf_similarity_exporter","text":"Exporter implementation for the Huggingface semantic similarity models.","title":"hf_similarity_exporter"},{"location":"inference_engine/reference/#npc_engine.exporters.hf_similarity_exporter.HfSimilarityExporter","text":"Bases: BaseHfExporter Exporter for the Huggingface transformer models. Source code in npc_engine\\exporters\\hf_similarity_exporter.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class HfSimilarityExporter ( BaseHfExporter ): \"\"\"Exporter for the Huggingface transformer models.\"\"\" def get_api ( self ) -> str : \"\"\"Get the api for the exporter.\"\"\" return \"SimilarityAPI\" @classmethod def get_model_name ( cls ): \"\"\"Get the model name.\"\"\" return \"TransformerSemanticSimilarity\" def create_config ( self , export_path : str ): \"\"\"Create the config for the model.\"\"\" config_dict = {} config_dict [ \"type\" ] = self . get_model_name ( export_path ) config_dict [ \"cache_size\" ] = click . prompt ( \"Cache size\" , type = int ) with open ( os . path . join ( export_path , \"config.yml\" ), \"w\" ) as f : yaml . dump ( config_dict , f ) def get_export_feature ( self ) -> str : \"\"\"Create the config for the model.\"\"\" return \"default\" def test_model_impl ( self , models_path : str , model_id : str ): \"\"\"Run test request. Args: models_path: path to models model_id: model id (directory name of the model) \"\"\" use_default_text = click . confirm ( \"Use default text for variables?\" ) if use_default_text : query = \"Hello world\" context = [ \"Whats up world\" ] else : query = click . prompt ( \"Query\" ) context = [ click . prompt ( \"Context\" )] zmq_context = zmq . Context () control_client = ControlClient ( zmq_context , \"5555\" ) service_client = SimilarityClient ( zmq_context , \"5555\" , model_id ) control_client . start_service ( model_id ) ready = False while not ready : time . sleep ( 1 ) status = control_client . get_service_status ( model_id ) ready = status == \"running\" if status == \"error\" : raise Exception ( \"Error while loading model\" ) if not ready : print ( \"Waiting for service to start...\" ) scores = service_client . compare ( query , context ) click . echo ( click . style ( \"Scores:\" , fg = \"green\" )) click . echo ( json . dumps ( scores , indent = 2 )) click . echo ( click . style ( \"Argmax classes:\" , fg = \"green\" )) click . echo ( json . dumps ([ classes . index ( max ( classes )) for classes in scores ], indent = 2 ) )","title":"HfSimilarityExporter"},{"location":"inference_engine/reference/#npc_engine.exporters.hf_similarity_exporter.HfSimilarityExporter.create_config","text":"Create the config for the model. Source code in npc_engine\\exporters\\hf_similarity_exporter.py 24 25 26 27 28 29 30 def create_config ( self , export_path : str ): \"\"\"Create the config for the model.\"\"\" config_dict = {} config_dict [ \"type\" ] = self . get_model_name ( export_path ) config_dict [ \"cache_size\" ] = click . prompt ( \"Cache size\" , type = int ) with open ( os . path . join ( export_path , \"config.yml\" ), \"w\" ) as f : yaml . dump ( config_dict , f )","title":"create_config()"},{"location":"inference_engine/reference/#npc_engine.exporters.hf_similarity_exporter.HfSimilarityExporter.get_api","text":"Get the api for the exporter. Source code in npc_engine\\exporters\\hf_similarity_exporter.py 15 16 17 def get_api ( self ) -> str : \"\"\"Get the api for the exporter.\"\"\" return \"SimilarityAPI\"","title":"get_api()"},{"location":"inference_engine/reference/#npc_engine.exporters.hf_similarity_exporter.HfSimilarityExporter.get_export_feature","text":"Create the config for the model. Source code in npc_engine\\exporters\\hf_similarity_exporter.py 32 33 34 def get_export_feature ( self ) -> str : \"\"\"Create the config for the model.\"\"\" return \"default\"","title":"get_export_feature()"},{"location":"inference_engine/reference/#npc_engine.exporters.hf_similarity_exporter.HfSimilarityExporter.get_model_name","text":"Get the model name. Source code in npc_engine\\exporters\\hf_similarity_exporter.py 19 20 21 22 @classmethod def get_model_name ( cls ): \"\"\"Get the model name.\"\"\" return \"TransformerSemanticSimilarity\"","title":"get_model_name()"},{"location":"inference_engine/reference/#npc_engine.exporters.hf_similarity_exporter.HfSimilarityExporter.test_model_impl","text":"Run test request. Parameters: Name Type Description Default models_path str path to models required model_id str model id (directory name of the model) required Source code in npc_engine\\exporters\\hf_similarity_exporter.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def test_model_impl ( self , models_path : str , model_id : str ): \"\"\"Run test request. Args: models_path: path to models model_id: model id (directory name of the model) \"\"\" use_default_text = click . confirm ( \"Use default text for variables?\" ) if use_default_text : query = \"Hello world\" context = [ \"Whats up world\" ] else : query = click . prompt ( \"Query\" ) context = [ click . prompt ( \"Context\" )] zmq_context = zmq . Context () control_client = ControlClient ( zmq_context , \"5555\" ) service_client = SimilarityClient ( zmq_context , \"5555\" , model_id ) control_client . start_service ( model_id ) ready = False while not ready : time . sleep ( 1 ) status = control_client . get_service_status ( model_id ) ready = status == \"running\" if status == \"error\" : raise Exception ( \"Error while loading model\" ) if not ready : print ( \"Waiting for service to start...\" ) scores = service_client . compare ( query , context ) click . echo ( click . style ( \"Scores:\" , fg = \"green\" )) click . echo ( json . dumps ( scores , indent = 2 )) click . echo ( click . style ( \"Argmax classes:\" , fg = \"green\" )) click . echo ( json . dumps ([ classes . index ( max ( classes )) for classes in scores ], indent = 2 ) )","title":"test_model_impl()"},{"location":"inference_engine/reference/#npc_engine.service_clients","text":"Module implementing the clients for services.","title":"service_clients"},{"location":"inference_engine/reference/#npc_engine.service_clients.chatbot_client","text":"Huggingface chatbot interface client implementation.","title":"chatbot_client"},{"location":"inference_engine/reference/#npc_engine.service_clients.chatbot_client.ChatbotClient","text":"Bases: ServiceClient Json rpc client for chatbot service. Source code in npc_engine\\service_clients\\chatbot_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class ChatbotClient ( ServiceClient ): \"\"\"Json rpc client for chatbot service.\"\"\" def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str = \"ChatbotAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , service_id ) def generate_reply ( self , context : Dict [ str , Any ]) -> str : \"\"\"Send a chatbot request to the server. Args: context: A dictionary containing the chatbot request. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"generate_reply\" , \"id\" : 0 , \"params\" : [ context ], } reply = self . send_request ( request ) return reply def get_prompt_template ( self ) -> str : \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_prompt_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) def get_context_template ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_context_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) def get_special_tokens ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_special_tokens\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request )","title":"ChatbotClient"},{"location":"inference_engine/reference/#npc_engine.service_clients.chatbot_client.ChatbotClient.__init__","text":"Connect to the server on the port. Source code in npc_engine\\service_clients\\chatbot_client.py 10 11 12 13 14 def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str = \"ChatbotAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , service_id )","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.service_clients.chatbot_client.ChatbotClient.generate_reply","text":"Send a chatbot request to the server. Parameters: Name Type Description Default context Dict [ str , Any ] A dictionary containing the chatbot request. required Source code in npc_engine\\service_clients\\chatbot_client.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def generate_reply ( self , context : Dict [ str , Any ]) -> str : \"\"\"Send a chatbot request to the server. Args: context: A dictionary containing the chatbot request. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"generate_reply\" , \"id\" : 0 , \"params\" : [ context ], } reply = self . send_request ( request ) return reply","title":"generate_reply()"},{"location":"inference_engine/reference/#npc_engine.service_clients.chatbot_client.ChatbotClient.get_context_template","text":"Send a chatbot request to the server. Source code in npc_engine\\service_clients\\chatbot_client.py 41 42 43 44 45 46 47 48 49 def get_context_template ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_context_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request )","title":"get_context_template()"},{"location":"inference_engine/reference/#npc_engine.service_clients.chatbot_client.ChatbotClient.get_prompt_template","text":"Send a chatbot request to the server. Source code in npc_engine\\service_clients\\chatbot_client.py 31 32 33 34 35 36 37 38 39 def get_prompt_template ( self ) -> str : \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_prompt_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request )","title":"get_prompt_template()"},{"location":"inference_engine/reference/#npc_engine.service_clients.chatbot_client.ChatbotClient.get_special_tokens","text":"Send a chatbot request to the server. Source code in npc_engine\\service_clients\\chatbot_client.py 51 52 53 54 55 56 57 58 59 def get_special_tokens ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_special_tokens\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request )","title":"get_special_tokens()"},{"location":"inference_engine/reference/#npc_engine.service_clients.control_client","text":"Control interface client implementation.","title":"control_client"},{"location":"inference_engine/reference/#npc_engine.service_clients.control_client.ControlClient","text":"Bases: ServiceClient Json rpc client for control requests. Source code in npc_engine\\service_clients\\control_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 class ControlClient ( ServiceClient ): \"\"\"Json rpc client for control requests.\"\"\" def __init__ ( self , zmq_context : zmq . Context , port : str ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , \"control\" ) def start_service ( self , service_id ): \"\"\"Send a start service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"start_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) def stop_service ( self , service_id ): \"\"\"Send a stop service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"stop_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) def get_service_status ( self , service_id ) -> str : \"\"\"Send a get service status request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_service_status\" , \"id\" : 0 , \"params\" : [ service_id ], } return self . send_request ( request ) def restart_service ( self , service_id ): \"\"\"Send a restart service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"restart_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) def get_services_metadata ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Send a get services metadata request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_services_metadata\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request )","title":"ControlClient"},{"location":"inference_engine/reference/#npc_engine.service_clients.control_client.ControlClient.__init__","text":"Connect to the server on the port. Source code in npc_engine\\service_clients\\control_client.py 10 11 12 def __init__ ( self , zmq_context : zmq . Context , port : str ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , \"control\" )","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.service_clients.control_client.ControlClient.get_service_status","text":"Send a get service status request to the server. Source code in npc_engine\\service_clients\\control_client.py 34 35 36 37 38 39 40 41 42 def get_service_status ( self , service_id ) -> str : \"\"\"Send a get service status request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_service_status\" , \"id\" : 0 , \"params\" : [ service_id ], } return self . send_request ( request )","title":"get_service_status()"},{"location":"inference_engine/reference/#npc_engine.service_clients.control_client.ControlClient.get_services_metadata","text":"Send a get services metadata request to the server. Source code in npc_engine\\service_clients\\control_client.py 54 55 56 57 58 59 60 61 62 def get_services_metadata ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Send a get services metadata request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_services_metadata\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request )","title":"get_services_metadata()"},{"location":"inference_engine/reference/#npc_engine.service_clients.control_client.ControlClient.restart_service","text":"Send a restart service request to the server. Source code in npc_engine\\service_clients\\control_client.py 44 45 46 47 48 49 50 51 52 def restart_service ( self , service_id ): \"\"\"Send a restart service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"restart_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request )","title":"restart_service()"},{"location":"inference_engine/reference/#npc_engine.service_clients.control_client.ControlClient.start_service","text":"Send a start service request to the server. Source code in npc_engine\\service_clients\\control_client.py 14 15 16 17 18 19 20 21 22 def start_service ( self , service_id ): \"\"\"Send a start service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"start_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request )","title":"start_service()"},{"location":"inference_engine/reference/#npc_engine.service_clients.control_client.ControlClient.stop_service","text":"Send a stop service request to the server. Source code in npc_engine\\service_clients\\control_client.py 24 25 26 27 28 29 30 31 32 def stop_service ( self , service_id ): \"\"\"Send a stop service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"stop_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request )","title":"stop_service()"},{"location":"inference_engine/reference/#npc_engine.service_clients.sequence_classifier_client","text":"Huggingface chatbot interface client implementation.","title":"sequence_classifier_client"},{"location":"inference_engine/reference/#npc_engine.service_clients.sequence_classifier_client.SequenceClassifierClient","text":"Bases: ServiceClient Json rpc client for chatbot service. Source code in npc_engine\\service_clients\\sequence_classifier_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class SequenceClassifierClient ( ServiceClient ): \"\"\"Json rpc client for chatbot service.\"\"\" def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str = \"SequenceClassifierAPI\" , ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , service_id ) def classify ( self , texts : List [ Union [ str , Tuple [ str , str ]]]) -> str : \"\"\"Send a classify request to the SequenceClassifierAPI. Args: texts: List of texts to classify. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"classify\" , \"id\" : 0 , \"params\" : [ texts ], } reply = self . send_request ( request ) return reply","title":"SequenceClassifierClient"},{"location":"inference_engine/reference/#npc_engine.service_clients.sequence_classifier_client.SequenceClassifierClient.__init__","text":"Connect to the server on the port. Source code in npc_engine\\service_clients\\sequence_classifier_client.py 10 11 12 13 14 15 16 17 def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str = \"SequenceClassifierAPI\" , ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , service_id )","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.service_clients.sequence_classifier_client.SequenceClassifierClient.classify","text":"Send a classify request to the SequenceClassifierAPI. Parameters: Name Type Description Default texts List [ Union [ str , Tuple [ str , str ]]] List of texts to classify. required Source code in npc_engine\\service_clients\\sequence_classifier_client.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def classify ( self , texts : List [ Union [ str , Tuple [ str , str ]]]) -> str : \"\"\"Send a classify request to the SequenceClassifierAPI. Args: texts: List of texts to classify. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"classify\" , \"id\" : 0 , \"params\" : [ texts ], } reply = self . send_request ( request ) return reply","title":"classify()"},{"location":"inference_engine/reference/#npc_engine.service_clients.service_client","text":"Module that implements ZMQ base client communication over JSON-RPC 2.0 (https://www.jsonrpc.org/specification).","title":"service_client"},{"location":"inference_engine/reference/#npc_engine.service_clients.service_client.ServiceClient","text":"Base json rpc client. Source code in npc_engine\\service_clients\\service_client.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class ServiceClient : \"\"\"Base json rpc client.\"\"\" def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str ): \"\"\"Connect to the server on the port.\"\"\" self . context = zmq_context self . socket = self . context . socket ( zmq . REQ ) self . socket . setsockopt ( zmq . LINGER , 0 ) self . socket . setsockopt ( zmq . IDENTITY , service_id . encode ( \"utf-8\" )) self . socket . connect ( f \"tcp://localhost: { port } \" ) logger . info ( \"Connected to server\" ) def send_request ( self , request : Dict [ str , Any ]) -> Any : \"\"\"Send request to the server and return the response. Args: request: The request to send to the server. Returns: The result from the server. \"\"\" logger . trace ( f \"Sending request: { request } \" ) self . socket . send_json ( request ) response = self . socket . recv_json () logger . trace ( f \"Received response: { response } \" ) if \"result\" in response : return response [ \"result\" ] elif \"code\" in response : raise RuntimeError ( f \"code: { response [ 'code' ] } . { response [ 'message' ] } \" )","title":"ServiceClient"},{"location":"inference_engine/reference/#npc_engine.service_clients.service_client.ServiceClient.__init__","text":"Connect to the server on the port. Source code in npc_engine\\service_clients\\service_client.py 12 13 14 15 16 17 18 19 def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str ): \"\"\"Connect to the server on the port.\"\"\" self . context = zmq_context self . socket = self . context . socket ( zmq . REQ ) self . socket . setsockopt ( zmq . LINGER , 0 ) self . socket . setsockopt ( zmq . IDENTITY , service_id . encode ( \"utf-8\" )) self . socket . connect ( f \"tcp://localhost: { port } \" ) logger . info ( \"Connected to server\" )","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.service_clients.service_client.ServiceClient.send_request","text":"Send request to the server and return the response. Parameters: Name Type Description Default request Dict [ str , Any ] The request to send to the server. required Returns: Type Description Any The result from the server. Source code in npc_engine\\service_clients\\service_client.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def send_request ( self , request : Dict [ str , Any ]) -> Any : \"\"\"Send request to the server and return the response. Args: request: The request to send to the server. Returns: The result from the server. \"\"\" logger . trace ( f \"Sending request: { request } \" ) self . socket . send_json ( request ) response = self . socket . recv_json () logger . trace ( f \"Received response: { response } \" ) if \"result\" in response : return response [ \"result\" ] elif \"code\" in response : raise RuntimeError ( f \"code: { response [ 'code' ] } . { response [ 'message' ] } \" )","title":"send_request()"},{"location":"inference_engine/reference/#npc_engine.service_clients.similarity_client","text":"Huggingface chatbot interface client implementation.","title":"similarity_client"},{"location":"inference_engine/reference/#npc_engine.service_clients.similarity_client.SimilarityClient","text":"Bases: ServiceClient Json rpc client for chatbot service. Source code in npc_engine\\service_clients\\similarity_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class SimilarityClient ( ServiceClient ): \"\"\"Json rpc client for chatbot service.\"\"\" def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str = \"SimilarityAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , service_id ) def compare ( self , query : str , context : List [ str ]) -> str : \"\"\"Send a chatbot request to the server. Args: context: A dictionary containing the chatbot request. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"compare\" , \"id\" : 0 , \"params\" : [ query , context ], } reply = self . send_request ( request ) return reply","title":"SimilarityClient"},{"location":"inference_engine/reference/#npc_engine.service_clients.similarity_client.SimilarityClient.__init__","text":"Connect to the server on the port. Source code in npc_engine\\service_clients\\similarity_client.py 10 11 12 13 14 def __init__ ( self , zmq_context : zmq . Context , port : str , service_id : str = \"SimilarityAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , port , service_id )","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.service_clients.similarity_client.SimilarityClient.compare","text":"Send a chatbot request to the server. Parameters: Name Type Description Default context List [ str ] A dictionary containing the chatbot request. required Source code in npc_engine\\service_clients\\similarity_client.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def compare ( self , query : str , context : List [ str ]) -> str : \"\"\"Send a chatbot request to the server. Args: context: A dictionary containing the chatbot request. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"compare\" , \"id\" : 0 , \"params\" : [ query , context ], } reply = self . send_request ( request ) return reply","title":"compare()"},{"location":"inference_engine/reference/#npc_engine.service_manager","text":"Module with RPC related functionality implementation.","title":"service_manager"},{"location":"inference_engine/reference/#npc_engine.service_manager.server","text":"Module that implements ZMQ server communication over JSON-RPC 2.0 (https://www.jsonrpc.org/specification).","title":"server"},{"location":"inference_engine/reference/#npc_engine.service_manager.server.Server","text":"Json rpc server over zmq. Source code in npc_engine\\service_manager\\server.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 class Server : \"\"\"Json rpc server over zmq.\"\"\" def __init__ ( self , zmq_context : zmq . asyncio . Context , service_manager : ServiceManager , port : str , start_services : bool = True , ): \"\"\"Create a server on the port.\"\"\" self . context = zmq_context self . socket = self . context . socket ( zmq . ROUTER ) self . socket . setsockopt ( zmq . LINGER , 0 ) self . socket . bind ( f \"tcp://*: { port } \" ) self . service_manager = service_manager self . start_services = start_services def run ( self ): \"\"\"Run an npc-engine json rpc server and start listening.\"\"\" logger . info ( \"Starting server\" ) if sys . platform == \"win32\" : asyncio . set_event_loop_policy ( asyncio . WindowsSelectorEventLoopPolicy ()) asyncio . get_event_loop () . run_until_complete ( self . loop ()) async def loop ( self ): \"\"\"Run the server loop.\"\"\" await asyncio . gather ( self . msg_loop (), self . interrupt_loop ()) async def interrupt_loop ( self ): \"\"\"Handle interrupts loop.\"\"\" while True : await asyncio . sleep ( 1 ) async def msg_loop ( self ): \"\"\"Asynchoriniously handle a request and reply.\"\"\" try : logger . info ( \"Starting services\" ) if self . start_services : for service in self . service_manager . services : self . service_manager . start_service ( service ) logger . info ( \"Starting message loop\" ) while True : address = await self . socket . recv () _ = await self . socket . recv () message = await self . socket . recv_string () logger . info ( f \"Received request to { address } : { message } \" ) asyncio . create_task ( self . handle_reply ( address , message )) except Exception as e : logger . error ( f \"Error in message loop: { e } \" ) logger . error ( tb . format_exc ()) raise e finally : logger . info ( \"Closing message loop\" ) self . socket . close () self . context . destroy () async def handle_reply ( self , address : str , message : str ): \"\"\"Handle message and reply.\"\"\" logging . info ( \"Handling reply\" ) start = time . time () try : address_str = address . decode ( \"utf-8\" ) except UnicodeDecodeError : address_str = address . hex () try : response = await self . service_manager . handle_request ( address_str , message ) except Exception as e : response = { \"code\" : - 32000 , \"message\" : f \"Internal error: { type ( e ) } { e } \" , \"data\" : tb . extract_tb ( e . __traceback__ ) . format () if hasattr ( e , \"__traceback__\" ) else None , } response = json . dumps ( response ) end = time . time () logger . info ( \"Handle message time: %d \" % ( end - start )) logger . info ( \"Message reply: %s \" % ( response )) # Send reply back to client await self . socket . send ( address , zmq . SNDMORE ) await self . socket . send_string ( \"\" , zmq . SNDMORE ) await self . socket . send_string ( response )","title":"Server"},{"location":"inference_engine/reference/#npc_engine.service_manager.server.Server.__init__","text":"Create a server on the port. Source code in npc_engine\\service_manager\\server.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def __init__ ( self , zmq_context : zmq . asyncio . Context , service_manager : ServiceManager , port : str , start_services : bool = True , ): \"\"\"Create a server on the port.\"\"\" self . context = zmq_context self . socket = self . context . socket ( zmq . ROUTER ) self . socket . setsockopt ( zmq . LINGER , 0 ) self . socket . bind ( f \"tcp://*: { port } \" ) self . service_manager = service_manager self . start_services = start_services","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.service_manager.server.Server.handle_reply","text":"Handle message and reply. Source code in npc_engine\\service_manager\\server.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 async def handle_reply ( self , address : str , message : str ): \"\"\"Handle message and reply.\"\"\" logging . info ( \"Handling reply\" ) start = time . time () try : address_str = address . decode ( \"utf-8\" ) except UnicodeDecodeError : address_str = address . hex () try : response = await self . service_manager . handle_request ( address_str , message ) except Exception as e : response = { \"code\" : - 32000 , \"message\" : f \"Internal error: { type ( e ) } { e } \" , \"data\" : tb . extract_tb ( e . __traceback__ ) . format () if hasattr ( e , \"__traceback__\" ) else None , } response = json . dumps ( response ) end = time . time () logger . info ( \"Handle message time: %d \" % ( end - start )) logger . info ( \"Message reply: %s \" % ( response )) # Send reply back to client await self . socket . send ( address , zmq . SNDMORE ) await self . socket . send_string ( \"\" , zmq . SNDMORE ) await self . socket . send_string ( response )","title":"handle_reply()"},{"location":"inference_engine/reference/#npc_engine.service_manager.server.Server.interrupt_loop","text":"Handle interrupts loop. Source code in npc_engine\\service_manager\\server.py 44 45 46 47 async def interrupt_loop ( self ): \"\"\"Handle interrupts loop.\"\"\" while True : await asyncio . sleep ( 1 )","title":"interrupt_loop()"},{"location":"inference_engine/reference/#npc_engine.service_manager.server.Server.loop","text":"Run the server loop. Source code in npc_engine\\service_manager\\server.py 40 41 42 async def loop ( self ): \"\"\"Run the server loop.\"\"\" await asyncio . gather ( self . msg_loop (), self . interrupt_loop ())","title":"loop()"},{"location":"inference_engine/reference/#npc_engine.service_manager.server.Server.msg_loop","text":"Asynchoriniously handle a request and reply. Source code in npc_engine\\service_manager\\server.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 async def msg_loop ( self ): \"\"\"Asynchoriniously handle a request and reply.\"\"\" try : logger . info ( \"Starting services\" ) if self . start_services : for service in self . service_manager . services : self . service_manager . start_service ( service ) logger . info ( \"Starting message loop\" ) while True : address = await self . socket . recv () _ = await self . socket . recv () message = await self . socket . recv_string () logger . info ( f \"Received request to { address } : { message } \" ) asyncio . create_task ( self . handle_reply ( address , message )) except Exception as e : logger . error ( f \"Error in message loop: { e } \" ) logger . error ( tb . format_exc ()) raise e finally : logger . info ( \"Closing message loop\" ) self . socket . close () self . context . destroy ()","title":"msg_loop()"},{"location":"inference_engine/reference/#npc_engine.service_manager.server.Server.run","text":"Run an npc-engine json rpc server and start listening. Source code in npc_engine\\service_manager\\server.py 33 34 35 36 37 38 def run ( self ): \"\"\"Run an npc-engine json rpc server and start listening.\"\"\" logger . info ( \"Starting server\" ) if sys . platform == \"win32\" : asyncio . set_event_loop_policy ( asyncio . WindowsSelectorEventLoopPolicy ()) asyncio . get_event_loop () . run_until_complete ( self . loop ())","title":"run()"},{"location":"inference_engine/reference/#npc_engine.service_manager.service_manager","text":"Module that implements lifetime and discoverability of the services.","title":"service_manager"},{"location":"inference_engine/reference/#npc_engine.service_manager.service_manager.ServiceManager","text":"Object for managing lifetime and discoverability of the services. Source code in npc_engine\\service_manager\\service_manager.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 class ServiceManager : \"\"\"Object for managing lifetime and discoverability of the services.\"\"\" def __init__ ( self , zmq_context : zmq . asyncio . Context , path ): \"\"\"Create model manager and load models from the given path.\"\"\" self . services = self . _scan_path ( path ) self . control_dispatcher = Dispatcher () self . control_dispatcher . update ( { \"get_services_metadata\" : self . get_services_metadata , \"get_service_status\" : self . get_service_status , \"start_service\" : self . start_service , \"stop_service\" : self . stop_service , \"restart_service\" : self . restart_service , } ) self . zmq_context = zmq_context os . makedirs ( \".npc_engine_tmp\" , exist_ok = True ) def __del__ ( self ): \"\"\"Stop all services.\"\"\" if hasattr ( self , \"services\" ): for service_id , service in self . services . items (): if service . process_data [ \"state\" ] == ServiceState . RUNNING : self . stop_service ( service_id ) async def handle_request ( self , address : str , request : str ) -> str : \"\"\"Parse request string and route request to correct service. Args: address (str): address of the service (either model name or class name) request (str): jsonRPC string Returns: str: jsonRPC response \"\"\" request_dict = json . loads ( request ) service_id = self . resolve_and_check_service ( address , request_dict [ \"method\" ]) logger . info ( f \"Request from { address } \\n Request: { request } \" ) if service_id == \"control\" : return JSONRPCResponseManager . handle ( request , self . control_dispatcher ) . json else : if self . services [ service_id ] . process_data [ \"state\" ] != ServiceState . RUNNING : raise ValueError ( f \"Service { service_id } is not running\" ) else : socket = self . services [ service_id ] . process_data [ \"socket\" ] await socket . send_string ( request ) response = await socket . recv_string () return response def resolve_and_check_service ( self , id_or_type , method = None ): \"\"\"Resolve service id or type to service id.\"\"\" service_id = None if id_or_type == \"control\" : service_id = \"control\" else : if id_or_type in self . services : service_id = id_or_type if service_id is None : for service_key , service in self . services . items (): if service . type == id_or_type or service . api_name == id_or_type : service_id = service_key break if service_id is None and method is not None : service_id = self . resolve_by_method ( method ) elif service_id is None : raise ValueError ( f \"Service { id_or_type } not found\" ) if ( self . services [ service_id ] . process_data [ \"state\" ] == ServiceState . RUNNING or self . services [ service_id ] . process_data [ \"state\" ] == ServiceState . STARTING or self . services [ service_id ] . process_data [ \"state\" ] == ServiceState . AWAITING ) and not self . services [ service_id ] . process_data [ \"process\" ] . is_alive (): self . services [ service_id ] . process_data [ \"state\" ] = ServiceState . ERROR raise ValueError ( f \"Error in service { service_id } . Process is not alive.\" ) return service_id def resolve_by_method ( self , method_name ): \"\"\"Resolve service id by method name.\"\"\" for service_id , service in self . services . items (): if service . process_data [ \"state\" ] == ServiceState . RUNNING : if method_name in service . api_methods : return service_id raise ValueError ( f \"Runnning service with method { method_name } not found\" ) def get_services_metadata ( self ): \"\"\"List the models in the folder.\"\"\" return [ services . BaseService . get_metadata ( descriptor . path ) for descriptor in self . services . values () ] def get_service_status ( self , service_id ): \"\"\"Get the status of the service.\"\"\" service_id = self . resolve_and_check_service ( service_id , None ) return self . services [ service_id ] . process_data [ \"state\" ] def start_service ( self , service_id ): \"\"\"Start the service.\"\"\" service_id = self . resolve_and_check_service ( service_id , None ) if service_id not in self . services : raise ValueError ( f \"Service { service_id } not found\" ) if self . services [ service_id ] . process_data [ \"state\" ] == ServiceState . RUNNING : raise ValueError ( f \"Service { service_id } is already running\" ) process = Process ( target = service_process , args = ( self . services [ service_id ] . path , self . services [ service_id ] . uri ), daemon = True , ) process . start () self . services [ service_id ] . process_data [ \"process\" ] = process self . services [ service_id ] . process_data [ \"state\" ] = ServiceState . STARTING self . services [ service_id ] . process_data [ \"socket\" ] = self . zmq_context . socket ( zmq . REQ ) self . services [ service_id ] . process_data [ \"socket\" ] . setsockopt ( zmq . LINGER , 0 ) self . services [ service_id ] . process_data [ \"socket\" ] . setsockopt ( zmq . RCVTIMEO , 10000 ) self . services [ service_id ] . process_data [ \"socket\" ] . connect ( self . services [ service_id ] . uri ) try : asyncio . create_task ( self . confirm_state_coroutine ( service_id )) except RuntimeError : logger . warning ( \"Create task to confirm service state failed.\" + \" Probably asyncio loop is not running.\" + \" Trying to execute it via asyncio.run()\" ) asyncio . run ( self . confirm_state_coroutine ( service_id )) async def confirm_state_coroutine ( self , service_id ): \"\"\"Confirm the state of the service.\"\"\" request = json . dumps ({ \"jsonrpc\" : \"2.0\" , \"method\" : \"status\" , \"id\" : 1 }) socket = self . services [ service_id ] . process_data [ \"socket\" ] await socket . send_string ( request ) response = await socket . recv_string () resp_dict = json . loads ( response ) if resp_dict [ \"result\" ] == ServiceState . RUNNING : self . services [ service_id ] . process_data [ \"state\" ] = ServiceState . RUNNING elif resp_dict [ \"result\" ] == ServiceState . STARTING : logger . info ( f \"Service { service_id } responds but still starting\" ) await asyncio . sleep ( 1 ) await self . confirm_state_coroutine ( service_id ) else : logger . warning ( f \"Service { service_id } failed to start and returned incorrect state.\" ) self . services [ service_id ] . process_data [ \"state\" ] = ServiceState . ERROR def stop_service ( self , service_id ): \"\"\"Stop the service.\"\"\" service_id = self . resolve_and_check_service ( service_id , None ) if service_id not in self . services : raise ValueError ( f \"Service { service_id } not found\" ) if self . services [ service_id ] . process_data [ \"state\" ] != ServiceState . RUNNING : raise ValueError ( f \"Service { service_id } is not running\" ) self . services [ service_id ] . process_data [ \"socket\" ] . close () self . services [ service_id ] . process_data [ \"socket\" ] = None self . services [ service_id ] . process_data [ \"process\" ] . terminate () self . services [ service_id ] . process_data [ \"process\" ] = None self . services [ service_id ] . process_data [ \"state\" ] = ServiceState . STOPPED def restart_service ( self , service_id ): \"\"\"Restart the service.\"\"\" self . stop_service ( service_id ) self . start_service ( service_id ) def _scan_path ( self , path : str ) -> Dict [ str , ServiceDescriptor ]: \"\"\"Scan services defined in the given path.\"\"\" norm_path = ntpath . normpath ( path ) . replace ( \" \\\\ \" , os . path . sep ) paths = [ f . path for f in os . scandir ( norm_path ) if f . is_dir () and os . path . exists ( os . path . join ( f , \"config.yml\" )) ] svcs = {} for path in paths : with open ( os . path . join ( path , \"config.yml\" )) as f : config_dict = yaml . safe_load ( f ) uri = f \"ipc:// { os . path . join ( user_cache_dir ( 'npc-engine' , 'NpcEngine' ), os . path . basename ( path )) } \" cls = getattr ( services , config_dict . get ( \"model_type\" , config_dict . get ( \"type\" , None )), ) svcs [ os . path . basename ( path )] = ServiceDescriptor ( os . path . basename ( path ), config_dict . get ( \"model_type\" , config_dict . get ( \"type\" , None )), path , uri , cls . get_api_name (), cls . API_METHODS , { \"process\" : None , \"socket\" : None , \"state\" : ServiceState . STOPPED }, ) return svcs","title":"ServiceManager"},{"location":"inference_engine/reference/#npc_engine.service_manager.service_manager.ServiceManager.__del__","text":"Stop all services. Source code in npc_engine\\service_manager\\service_manager.py 71 72 73 74 75 76 def __del__ ( self ): \"\"\"Stop all services.\"\"\" if hasattr ( self , \"services\" ): for service_id , service in self . services . items (): if service . process_data [ \"state\" ] == ServiceState . RUNNING : self . stop_service ( service_id )","title":"__del__()"},{"location":"inference_engine/reference/#npc_engine.service_manager.service_manager.ServiceManager.__init__","text":"Create model manager and load models from the given path. Source code in npc_engine\\service_manager\\service_manager.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def __init__ ( self , zmq_context : zmq . asyncio . Context , path ): \"\"\"Create model manager and load models from the given path.\"\"\" self . services = self . _scan_path ( path ) self . control_dispatcher = Dispatcher () self . control_dispatcher . update ( { \"get_services_metadata\" : self . get_services_metadata , \"get_service_status\" : self . get_service_status , \"start_service\" : self . start_service , \"stop_service\" : self . stop_service , \"restart_service\" : self . restart_service , } ) self . zmq_context = zmq_context os . makedirs ( \".npc_engine_tmp\" , exist_ok = True )","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.service_manager.service_manager.ServiceManager._scan_path","text":"Scan services defined in the given path. Source code in npc_engine\\service_manager\\service_manager.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 def _scan_path ( self , path : str ) -> Dict [ str , ServiceDescriptor ]: \"\"\"Scan services defined in the given path.\"\"\" norm_path = ntpath . normpath ( path ) . replace ( \" \\\\ \" , os . path . sep ) paths = [ f . path for f in os . scandir ( norm_path ) if f . is_dir () and os . path . exists ( os . path . join ( f , \"config.yml\" )) ] svcs = {} for path in paths : with open ( os . path . join ( path , \"config.yml\" )) as f : config_dict = yaml . safe_load ( f ) uri = f \"ipc:// { os . path . join ( user_cache_dir ( 'npc-engine' , 'NpcEngine' ), os . path . basename ( path )) } \" cls = getattr ( services , config_dict . get ( \"model_type\" , config_dict . get ( \"type\" , None )), ) svcs [ os . path . basename ( path )] = ServiceDescriptor ( os . path . basename ( path ), config_dict . get ( \"model_type\" , config_dict . get ( \"type\" , None )), path , uri , cls . get_api_name (), cls . API_METHODS , { \"process\" : None , \"socket\" : None , \"state\" : ServiceState . STOPPED }, ) return svcs","title":"_scan_path()"},{"location":"inference_engine/reference/#npc_engine.service_manager.service_manager.ServiceManager.confirm_state_coroutine","text":"Confirm the state of the service. Source code in npc_engine\\service_manager\\service_manager.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 async def confirm_state_coroutine ( self , service_id ): \"\"\"Confirm the state of the service.\"\"\" request = json . dumps ({ \"jsonrpc\" : \"2.0\" , \"method\" : \"status\" , \"id\" : 1 }) socket = self . services [ service_id ] . process_data [ \"socket\" ] await socket . send_string ( request ) response = await socket . recv_string () resp_dict = json . loads ( response ) if resp_dict [ \"result\" ] == ServiceState . RUNNING : self . services [ service_id ] . process_data [ \"state\" ] = ServiceState . RUNNING elif resp_dict [ \"result\" ] == ServiceState . STARTING : logger . info ( f \"Service { service_id } responds but still starting\" ) await asyncio . sleep ( 1 ) await self . confirm_state_coroutine ( service_id ) else : logger . warning ( f \"Service { service_id } failed to start and returned incorrect state.\" ) self . services [ service_id ] . process_data [ \"state\" ] = ServiceState . ERROR","title":"confirm_state_coroutine()"},{"location":"inference_engine/reference/#npc_engine.service_manager.service_manager.ServiceManager.get_service_status","text":"Get the status of the service. Source code in npc_engine\\service_manager\\service_manager.py 148 149 150 151 def get_service_status ( self , service_id ): \"\"\"Get the status of the service.\"\"\" service_id = self . resolve_and_check_service ( service_id , None ) return self . services [ service_id ] . process_data [ \"state\" ]","title":"get_service_status()"},{"location":"inference_engine/reference/#npc_engine.service_manager.service_manager.ServiceManager.get_services_metadata","text":"List the models in the folder. Source code in npc_engine\\service_manager\\service_manager.py 141 142 143 144 145 146 def get_services_metadata ( self ): \"\"\"List the models in the folder.\"\"\" return [ services . BaseService . get_metadata ( descriptor . path ) for descriptor in self . services . values () ]","title":"get_services_metadata()"},{"location":"inference_engine/reference/#npc_engine.service_manager.service_manager.ServiceManager.handle_request","text":"Parse request string and route request to correct service. Parameters: Name Type Description Default address str address of the service (either model name or class name) required request str jsonRPC string required Returns: Name Type Description str str jsonRPC response Source code in npc_engine\\service_manager\\service_manager.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 async def handle_request ( self , address : str , request : str ) -> str : \"\"\"Parse request string and route request to correct service. Args: address (str): address of the service (either model name or class name) request (str): jsonRPC string Returns: str: jsonRPC response \"\"\" request_dict = json . loads ( request ) service_id = self . resolve_and_check_service ( address , request_dict [ \"method\" ]) logger . info ( f \"Request from { address } \\n Request: { request } \" ) if service_id == \"control\" : return JSONRPCResponseManager . handle ( request , self . control_dispatcher ) . json else : if self . services [ service_id ] . process_data [ \"state\" ] != ServiceState . RUNNING : raise ValueError ( f \"Service { service_id } is not running\" ) else : socket = self . services [ service_id ] . process_data [ \"socket\" ] await socket . send_string ( request ) response = await socket . recv_string () return response","title":"handle_request()"},{"location":"inference_engine/reference/#npc_engine.service_manager.service_manager.ServiceManager.resolve_and_check_service","text":"Resolve service id or type to service id. Source code in npc_engine\\service_manager\\service_manager.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def resolve_and_check_service ( self , id_or_type , method = None ): \"\"\"Resolve service id or type to service id.\"\"\" service_id = None if id_or_type == \"control\" : service_id = \"control\" else : if id_or_type in self . services : service_id = id_or_type if service_id is None : for service_key , service in self . services . items (): if service . type == id_or_type or service . api_name == id_or_type : service_id = service_key break if service_id is None and method is not None : service_id = self . resolve_by_method ( method ) elif service_id is None : raise ValueError ( f \"Service { id_or_type } not found\" ) if ( self . services [ service_id ] . process_data [ \"state\" ] == ServiceState . RUNNING or self . services [ service_id ] . process_data [ \"state\" ] == ServiceState . STARTING or self . services [ service_id ] . process_data [ \"state\" ] == ServiceState . AWAITING ) and not self . services [ service_id ] . process_data [ \"process\" ] . is_alive (): self . services [ service_id ] . process_data [ \"state\" ] = ServiceState . ERROR raise ValueError ( f \"Error in service { service_id } . Process is not alive.\" ) return service_id","title":"resolve_and_check_service()"},{"location":"inference_engine/reference/#npc_engine.service_manager.service_manager.ServiceManager.resolve_by_method","text":"Resolve service id by method name. Source code in npc_engine\\service_manager\\service_manager.py 133 134 135 136 137 138 139 def resolve_by_method ( self , method_name ): \"\"\"Resolve service id by method name.\"\"\" for service_id , service in self . services . items (): if service . process_data [ \"state\" ] == ServiceState . RUNNING : if method_name in service . api_methods : return service_id raise ValueError ( f \"Runnning service with method { method_name } not found\" )","title":"resolve_by_method()"},{"location":"inference_engine/reference/#npc_engine.service_manager.service_manager.ServiceManager.restart_service","text":"Restart the service. Source code in npc_engine\\service_manager\\service_manager.py 219 220 221 222 def restart_service ( self , service_id ): \"\"\"Restart the service.\"\"\" self . stop_service ( service_id ) self . start_service ( service_id )","title":"restart_service()"},{"location":"inference_engine/reference/#npc_engine.service_manager.service_manager.ServiceManager.start_service","text":"Start the service. Source code in npc_engine\\service_manager\\service_manager.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def start_service ( self , service_id ): \"\"\"Start the service.\"\"\" service_id = self . resolve_and_check_service ( service_id , None ) if service_id not in self . services : raise ValueError ( f \"Service { service_id } not found\" ) if self . services [ service_id ] . process_data [ \"state\" ] == ServiceState . RUNNING : raise ValueError ( f \"Service { service_id } is already running\" ) process = Process ( target = service_process , args = ( self . services [ service_id ] . path , self . services [ service_id ] . uri ), daemon = True , ) process . start () self . services [ service_id ] . process_data [ \"process\" ] = process self . services [ service_id ] . process_data [ \"state\" ] = ServiceState . STARTING self . services [ service_id ] . process_data [ \"socket\" ] = self . zmq_context . socket ( zmq . REQ ) self . services [ service_id ] . process_data [ \"socket\" ] . setsockopt ( zmq . LINGER , 0 ) self . services [ service_id ] . process_data [ \"socket\" ] . setsockopt ( zmq . RCVTIMEO , 10000 ) self . services [ service_id ] . process_data [ \"socket\" ] . connect ( self . services [ service_id ] . uri ) try : asyncio . create_task ( self . confirm_state_coroutine ( service_id )) except RuntimeError : logger . warning ( \"Create task to confirm service state failed.\" + \" Probably asyncio loop is not running.\" + \" Trying to execute it via asyncio.run()\" ) asyncio . run ( self . confirm_state_coroutine ( service_id ))","title":"start_service()"},{"location":"inference_engine/reference/#npc_engine.service_manager.service_manager.ServiceManager.stop_service","text":"Stop the service. Source code in npc_engine\\service_manager\\service_manager.py 206 207 208 209 210 211 212 213 214 215 216 217 def stop_service ( self , service_id ): \"\"\"Stop the service.\"\"\" service_id = self . resolve_and_check_service ( service_id , None ) if service_id not in self . services : raise ValueError ( f \"Service { service_id } not found\" ) if self . services [ service_id ] . process_data [ \"state\" ] != ServiceState . RUNNING : raise ValueError ( f \"Service { service_id } is not running\" ) self . services [ service_id ] . process_data [ \"socket\" ] . close () self . services [ service_id ] . process_data [ \"socket\" ] = None self . services [ service_id ] . process_data [ \"process\" ] . terminate () self . services [ service_id ] . process_data [ \"process\" ] = None self . services [ service_id ] . process_data [ \"state\" ] = ServiceState . STOPPED","title":"stop_service()"},{"location":"inference_engine/reference/#npc_engine.service_manager.service_manager.ServiceState","text":"Enum for the state of the service. Source code in npc_engine\\service_manager\\service_manager.py 19 20 21 22 23 24 25 26 27 class ServiceState : \"\"\"Enum for the state of the service.\"\"\" STARTING = \"starting\" RUNNING = \"running\" STOPPED = \"stopped\" AWAITING = \"awaiting\" TIMEOUT = \"timeout\" ERROR = \"error\"","title":"ServiceState"},{"location":"inference_engine/reference/#npc_engine.service_manager.service_manager.service_process","text":"Service subprocess function. Starts the service and runs it's loop. Source code in npc_engine\\service_manager\\service_manager.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def service_process ( service_path : str , uri : str ) -> None : \"\"\"Service subprocess function. Starts the service and runs it's loop. \"\"\" logger . remove () logger . add ( os . path . join ( \"logs\" , f \" { service_path . split ( os . path . sep )[ - 1 ] } .log\" ), rotation = \"10 MB\" , enqueue = True , ) context = zmq . Context () service = services . BaseService . create ( context , service_path , uri ) service . start ()","title":"service_process()"},{"location":"inference_engine/reference/#npc_engine.service_manager.utils","text":"Utility functions for RPC communication.","title":"utils"},{"location":"inference_engine/reference/#npc_engine.service_manager.utils.schema_to_json","text":"Iterate the schema and return simplified dictionary. Source code in npc_engine\\service_manager\\utils.py 6 7 8 9 10 11 12 13 14 15 16 17 def schema_to_json ( s : Dict [ str , Any ], fill_value : Callable [[ str ], Any ] = lambda _ : \"\" ) -> Dict [ str , Any ]: \"\"\"Iterate the schema and return simplified dictionary.\"\"\" if \"type\" not in s and \"anyOf\" in s : return fill_value ( s [ \"title\" ]) elif \"type\" in s and s [ \"type\" ] == \"object\" : return { k : schema_to_json ( v ) for k , v in s [ \"properties\" ] . items ()} elif \"type\" in s and s [ \"type\" ] == \"array\" : return [ schema_to_json ( s [ \"items\" ])] else : raise ValueError ( f \"Unknown schema type: { s } \" )","title":"schema_to_json()"},{"location":"inference_engine/reference/#npc_engine.service_manager.utils.start_test_server","text":"Start the test server. Parameters: Name Type Description Default port str The port to start the server on. required models_path str The path to the models. required Source code in npc_engine\\service_manager\\utils.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def start_test_server ( port : str , models_path : str ): # pragma: no cover \"\"\"Start the test server. Args: port: The port to start the server on. models_path: The path to the models. \"\"\" subprocess . Popen ( [ \"npc-engine\" , \"--verbose\" , \"run\" , \"--port\" , port , \"--models-path\" , models_path , ], creationflags = subprocess . CREATE_NEW_CONSOLE , )","title":"start_test_server()"},{"location":"inference_engine/reference/#npc_engine.services","text":"Module that contains everything related to deep learning models. For your model API to be discovered it must be imported here","title":"services"},{"location":"inference_engine/reference/#npc_engine.services.base_service","text":"Module with Model base class.","title":"base_service"},{"location":"inference_engine/reference/#npc_engine.services.base_service.BaseService","text":"Bases: ABC Abstract base class for managed services. Source code in npc_engine\\services\\base_service.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 class BaseService ( ABC ): \"\"\"Abstract base class for managed services.\"\"\" models = {} def __init_subclass__ ( cls , ** kwargs ): \"\"\"Init subclass where service classes get registered to be discovered.\"\"\" super () . __init_subclass__ ( ** kwargs ) cls . models [ cls . __name__ ] = cls def __init__ ( self , context : zmq . Context , uri : str , * args , ** kwargs ): \"\"\"Initialize the service.\"\"\" self . zmq_context = context self . socket = context . socket ( zmq . REP ) self . socket . setsockopt ( zmq . LINGER , 0 ) print ( uri ) if uri . startswith ( \"ipc://\" ): os . makedirs ( Path ( uri . replace ( \"ipc://\" , \"\" )) . parent , exist_ok = True ) os . chmod ( Path ( uri . replace ( \"ipc://\" , \"\" )) . parent , 777 ) self . socket . bind ( uri ) @classmethod @abstractmethod def get_api_name ( cls ): \"\"\"Return the name of the API.\"\"\" pass @classmethod def create ( cls , context : zmq . Context , path : str , uri : str ): \"\"\"Create a service from the path.\"\"\" config_path = os . path . join ( path , \"config.yml\" ) with open ( config_path ) as f : config_dict = yaml . load ( f , Loader = yaml . Loader ) config_dict [ \"model_path\" ] = path config_dict [ \"uri\" ] = uri model_cls = cls . models [ get_type_from_dict ( config_dict )] return model_cls ( ** config_dict , context = context ) def start ( self ): \"\"\"Run service main loop that accepts json rpc.\"\"\" try : dispatcher = Dispatcher () dispatcher . update ( self . build_api_dict ()) dispatcher . update ({ \"status\" : self . status }) while True : request = self . socket . recv_string () response = JSONRPCResponseManager . handle ( request , dispatcher ) self . socket . send_string ( response . json ) except Exception as e : logger . exception ( e ) raise e finally : self . socket . close () self . zmq_context . destroy () def status ( self ): \"\"\"Return status of the service.\"\"\" from npc_engine.service_manager.service_manager import ServiceState return ServiceState . RUNNING def build_api_dict ( self ): \"\"\"Build api dict. Returns: dict(str,str): Mapping \"method_name\" -> callable that will be exposed to API \"\"\" api_dict = {} for method in type ( self ) . API_METHODS : logger . info ( f \"Registering method { method } for model { type ( self ) . __name__ } \" ) # TODO api_dict [ method ] = getattr ( self , method ) return api_dict @classmethod def get_metadata ( cls , path : str ) -> Dict [ str , str ]: \"\"\"Print the model from the path.\"\"\" path = path . replace ( \" \\\\ \" , os . path . sep ) model_id = path . split ( os . path . sep )[ - 1 ] config_path = os . path . join ( path , \"config.yml\" ) readme_path = os . path . join ( path , \"README.md\" ) with open ( config_path ) as f : config_dict = yaml . load ( f , Loader = yaml . Loader ) try : with open ( readme_path ) as f : readme = f . read () . split ( \"---\" )[ - 1 ] except FileNotFoundError : readme = \"\" return { \"id\" : model_id , \"service\" : get_type_from_dict ( config_dict ), \"path\" : path , \"service_short_description\" : cls . models [ get_type_from_dict ( config_dict ) ] . __doc__ . split ( \" \\n\\n \" )[ 0 ], \"service_description\" : cls . models [ get_type_from_dict ( config_dict )] . __doc__ , \"readme\" : readme , }","title":"BaseService"},{"location":"inference_engine/reference/#npc_engine.services.base_service.BaseService.__init__","text":"Initialize the service. Source code in npc_engine\\services\\base_service.py 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , context : zmq . Context , uri : str , * args , ** kwargs ): \"\"\"Initialize the service.\"\"\" self . zmq_context = context self . socket = context . socket ( zmq . REP ) self . socket . setsockopt ( zmq . LINGER , 0 ) print ( uri ) if uri . startswith ( \"ipc://\" ): os . makedirs ( Path ( uri . replace ( \"ipc://\" , \"\" )) . parent , exist_ok = True ) os . chmod ( Path ( uri . replace ( \"ipc://\" , \"\" )) . parent , 777 ) self . socket . bind ( uri )","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.services.base_service.BaseService.__init_subclass__","text":"Init subclass where service classes get registered to be discovered. Source code in npc_engine\\services\\base_service.py 19 20 21 22 def __init_subclass__ ( cls , ** kwargs ): \"\"\"Init subclass where service classes get registered to be discovered.\"\"\" super () . __init_subclass__ ( ** kwargs ) cls . models [ cls . __name__ ] = cls","title":"__init_subclass__()"},{"location":"inference_engine/reference/#npc_engine.services.base_service.BaseService.build_api_dict","text":"Build api dict. Returns: Name Type Description dict str , str Mapping \"method_name\" -> callable that will be exposed to API Source code in npc_engine\\services\\base_service.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def build_api_dict ( self ): \"\"\"Build api dict. Returns: dict(str,str): Mapping \"method_name\" -> callable that will be exposed to API \"\"\" api_dict = {} for method in type ( self ) . API_METHODS : logger . info ( f \"Registering method { method } for model { type ( self ) . __name__ } \" ) # TODO api_dict [ method ] = getattr ( self , method ) return api_dict","title":"build_api_dict()"},{"location":"inference_engine/reference/#npc_engine.services.base_service.BaseService.create","text":"Create a service from the path. Source code in npc_engine\\services\\base_service.py 41 42 43 44 45 46 47 48 49 50 @classmethod def create ( cls , context : zmq . Context , path : str , uri : str ): \"\"\"Create a service from the path.\"\"\" config_path = os . path . join ( path , \"config.yml\" ) with open ( config_path ) as f : config_dict = yaml . load ( f , Loader = yaml . Loader ) config_dict [ \"model_path\" ] = path config_dict [ \"uri\" ] = uri model_cls = cls . models [ get_type_from_dict ( config_dict )] return model_cls ( ** config_dict , context = context )","title":"create()"},{"location":"inference_engine/reference/#npc_engine.services.base_service.BaseService.get_api_name","text":"Return the name of the API. Source code in npc_engine\\services\\base_service.py 35 36 37 38 39 @classmethod @abstractmethod def get_api_name ( cls ): \"\"\"Return the name of the API.\"\"\" pass","title":"get_api_name()"},{"location":"inference_engine/reference/#npc_engine.services.base_service.BaseService.get_metadata","text":"Print the model from the path. Source code in npc_engine\\services\\base_service.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 @classmethod def get_metadata ( cls , path : str ) -> Dict [ str , str ]: \"\"\"Print the model from the path.\"\"\" path = path . replace ( \" \\\\ \" , os . path . sep ) model_id = path . split ( os . path . sep )[ - 1 ] config_path = os . path . join ( path , \"config.yml\" ) readme_path = os . path . join ( path , \"README.md\" ) with open ( config_path ) as f : config_dict = yaml . load ( f , Loader = yaml . Loader ) try : with open ( readme_path ) as f : readme = f . read () . split ( \"---\" )[ - 1 ] except FileNotFoundError : readme = \"\" return { \"id\" : model_id , \"service\" : get_type_from_dict ( config_dict ), \"path\" : path , \"service_short_description\" : cls . models [ get_type_from_dict ( config_dict ) ] . __doc__ . split ( \" \\n\\n \" )[ 0 ], \"service_description\" : cls . models [ get_type_from_dict ( config_dict )] . __doc__ , \"readme\" : readme , }","title":"get_metadata()"},{"location":"inference_engine/reference/#npc_engine.services.base_service.BaseService.start","text":"Run service main loop that accepts json rpc. Source code in npc_engine\\services\\base_service.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def start ( self ): \"\"\"Run service main loop that accepts json rpc.\"\"\" try : dispatcher = Dispatcher () dispatcher . update ( self . build_api_dict ()) dispatcher . update ({ \"status\" : self . status }) while True : request = self . socket . recv_string () response = JSONRPCResponseManager . handle ( request , dispatcher ) self . socket . send_string ( response . json ) except Exception as e : logger . exception ( e ) raise e finally : self . socket . close () self . zmq_context . destroy ()","title":"start()"},{"location":"inference_engine/reference/#npc_engine.services.base_service.BaseService.status","text":"Return status of the service. Source code in npc_engine\\services\\base_service.py 69 70 71 72 73 def status ( self ): \"\"\"Return status of the service.\"\"\" from npc_engine.service_manager.service_manager import ServiceState return ServiceState . RUNNING","title":"status()"},{"location":"inference_engine/reference/#npc_engine.services.chatbot","text":"Chatbot model implementations. This module implements specific models and wraps them under the common interface for loading and inference. Example from npc_engine.models.chatbot import ChatbotAPI model = ChatbotAPI.load(\"path/to/model_dir\") model.generate_reply(context, temperature=0.8, topk=None,)","title":"chatbot"},{"location":"inference_engine/reference/#npc_engine.services.chatbot.bart","text":"BART based chatbot implementation.","title":"bart"},{"location":"inference_engine/reference/#npc_engine.services.chatbot.bart.BartChatbot","text":"Bases: ChatbotAPI BART based chatbot implementation class. This model class requires two ONNX models encoder_bart.onnx and decoder_bart.onnx that correspond to encoder and decoder from transformers EncoderDecoderModel and a tokenizer.json with huggingface tokenizers definition. encoder_bart.onnx spec: - inputs: `input_ids` - outputs: `encoder_hidden_state` decoder_bart.onnx spec: - inputs: `encoder_hidden_state` `decoder_input_ids` - outputs: `logits` Source code in npc_engine\\services\\chatbot\\bart.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 class BartChatbot ( ChatbotAPI ): \"\"\"BART based chatbot implementation class. This model class requires two ONNX models `encoder_bart.onnx` and `decoder_bart.onnx` that correspond to encoder and decoder from transformers [EncoderDecoderModel](https://huggingface.co/transformers/model_doc/encoderdecoder.html) and a tokenizer.json with huggingface tokenizers definition. encoder_bart.onnx spec: - inputs: `input_ids` - outputs: `encoder_hidden_state` decoder_bart.onnx spec: - inputs: `encoder_hidden_state` `decoder_input_ids` - outputs: `logits` \"\"\" def __init__ ( self , model_path , max_steps = 100 , min_length = 2 , repetition_penalty = 1 , bos_token_id = 0 , eos_token_id = 2 , pad_token_id = 1 , sep_token_id = None , * args , ** kwargs , ): \"\"\"Create the chatbot from config args and kwargs. Args: model_path: path to scan for model files (weights and configs) max_steps: stop generation at this number of tokens min_length: model can't stop generating text before it's atleast this long in tokens repetition_penalty: probability coef for same tokens to appear multiple times bos_token_id: beginning of sequence token id eos_token_id: end of sequence token id pad_token_id: padding token id sep_token_id: token id for separating sequence into multiple parts \"\"\" super () . __init__ ( * args , ** kwargs ) self . bos_token_id = bos_token_id self . eos_token_id = eos_token_id self . sep_token_id = eos_token_id if sep_token_id is None else sep_token_id self . pad_token_id = pad_token_id sess_options = rt . SessionOptions () sess_options . graph_optimization_level = ( rt . GraphOptimizationLevel . ORT_DISABLE_ALL ) self . encoder_model = rt . InferenceSession ( os . path . join ( model_path , \"encoder_bart.onnx\" ), providers = [ rt . get_available_providers ()[ 0 ]], sess_options = sess_options , ) self . decoder_model = rt . InferenceSession ( os . path . join ( model_path , \"decoder_bart.onnx\" ), providers = [ rt . get_available_providers ()[ 0 ]], sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) added_tokens_path = os . path . join ( model_path , \"added_tokens.txt\" ) if os . path . exists ( added_tokens_path ): with open ( added_tokens_path ) as f : added_tokens = json . load ( f ) added_tokens = [ key for key , _ in sorted ( list ( added_tokens . items ()), key = lambda x : x [ 1 ]) ] self . tokenizer . add_tokens ( added_tokens ) self . special_tokens = { \"bos_token\" : self . tokenizer . decode ( [ bos_token_id ], skip_special_tokens = False ), \"eos_token\" : self . tokenizer . decode ( [ eos_token_id ], skip_special_tokens = False ), \"sep_token\" : self . tokenizer . decode ( [ self . sep_token_id ], skip_special_tokens = False ), \"pad_token\" : self . tokenizer . decode ( [ pad_token_id ], skip_special_tokens = False ), ** { f \"added_token { self . tokenizer . token_to_id ( token ) } \" : token for token in added_tokens }, } self . max_steps = max_steps self . min_length = min_length self . repetition_penalty = repetition_penalty def run ( self , prompt : str , temperature : float = 1.0 , topk : int = None ) -> str : \"\"\"Run text generation from given prompt and parameters. Args: prompt: Fromatted prompt. temperature: Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness topk: If not none selects top n of predictions to sample from during generation. Returns: Generated text \"\"\" tokens = self . tokenizer . encode ( prompt ) total = np . asarray ( tokens . ids , dtype = np . int64 ) . reshape ([ 1 , - 1 ]) total_enc = self . encoder_model . run ( None , { \"input_ids\" : total })[ 0 ] utterance = np . asarray ([ self . eos_token_id ], dtype = np . int64 ) . reshape ([ 1 , 1 ]) for i in range ( self . max_steps ): o = self . decoder_model . run ( None , { \"encoder_hidden_state\" : total_enc , \"decoder_input_ids\" : utterance }, ) logits = o [ 0 ][ 0 , - 1 , :] if i < self . min_length : logits [ self . eos_token_id ] = float ( \"-inf\" ) if topk is not None : ind = np . argpartition ( logits , - topk )[ - topk :] new_logits = np . zeros ( logits . shape ) new_logits [ ind ] = logits [ ind ] logits = new_logits probs = scp . softmax ( logits / temperature , axis = 0 ) token = np . random . choice ( np . arange ( probs . shape [ 0 ]), p = probs ) token = token . reshape ([ 1 , 1 ]) utterance = np . concatenate ([ utterance , token ], axis = 1 ) if token [ 0 , 0 ] == self . eos_token_id : break return self . tokenizer . decode ( utterance [ 0 , :] . tolist (), skip_special_tokens = True ) def get_special_tokens ( self ) -> Dict [ str , str ]: \"\"\"Retrun dict of special tokens to be renderable from template.\"\"\" return self . special_tokens","title":"BartChatbot"},{"location":"inference_engine/reference/#npc_engine.services.chatbot.bart.BartChatbot.__init__","text":"Create the chatbot from config args and kwargs. Parameters: Name Type Description Default model_path path to scan for model files (weights and configs) required max_steps stop generation at this number of tokens 100 min_length model can't stop generating text before it's atleast this long in tokens 2 repetition_penalty probability coef for same tokens to appear multiple times 1 bos_token_id beginning of sequence token id 0 eos_token_id end of sequence token id 2 pad_token_id padding token id 1 sep_token_id token id for separating sequence into multiple parts None Source code in npc_engine\\services\\chatbot\\bart.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def __init__ ( self , model_path , max_steps = 100 , min_length = 2 , repetition_penalty = 1 , bos_token_id = 0 , eos_token_id = 2 , pad_token_id = 1 , sep_token_id = None , * args , ** kwargs , ): \"\"\"Create the chatbot from config args and kwargs. Args: model_path: path to scan for model files (weights and configs) max_steps: stop generation at this number of tokens min_length: model can't stop generating text before it's atleast this long in tokens repetition_penalty: probability coef for same tokens to appear multiple times bos_token_id: beginning of sequence token id eos_token_id: end of sequence token id pad_token_id: padding token id sep_token_id: token id for separating sequence into multiple parts \"\"\" super () . __init__ ( * args , ** kwargs ) self . bos_token_id = bos_token_id self . eos_token_id = eos_token_id self . sep_token_id = eos_token_id if sep_token_id is None else sep_token_id self . pad_token_id = pad_token_id sess_options = rt . SessionOptions () sess_options . graph_optimization_level = ( rt . GraphOptimizationLevel . ORT_DISABLE_ALL ) self . encoder_model = rt . InferenceSession ( os . path . join ( model_path , \"encoder_bart.onnx\" ), providers = [ rt . get_available_providers ()[ 0 ]], sess_options = sess_options , ) self . decoder_model = rt . InferenceSession ( os . path . join ( model_path , \"decoder_bart.onnx\" ), providers = [ rt . get_available_providers ()[ 0 ]], sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) added_tokens_path = os . path . join ( model_path , \"added_tokens.txt\" ) if os . path . exists ( added_tokens_path ): with open ( added_tokens_path ) as f : added_tokens = json . load ( f ) added_tokens = [ key for key , _ in sorted ( list ( added_tokens . items ()), key = lambda x : x [ 1 ]) ] self . tokenizer . add_tokens ( added_tokens ) self . special_tokens = { \"bos_token\" : self . tokenizer . decode ( [ bos_token_id ], skip_special_tokens = False ), \"eos_token\" : self . tokenizer . decode ( [ eos_token_id ], skip_special_tokens = False ), \"sep_token\" : self . tokenizer . decode ( [ self . sep_token_id ], skip_special_tokens = False ), \"pad_token\" : self . tokenizer . decode ( [ pad_token_id ], skip_special_tokens = False ), ** { f \"added_token { self . tokenizer . token_to_id ( token ) } \" : token for token in added_tokens }, } self . max_steps = max_steps self . min_length = min_length self . repetition_penalty = repetition_penalty","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.services.chatbot.bart.BartChatbot.get_special_tokens","text":"Retrun dict of special tokens to be renderable from template. Source code in npc_engine\\services\\chatbot\\bart.py 157 158 159 def get_special_tokens ( self ) -> Dict [ str , str ]: \"\"\"Retrun dict of special tokens to be renderable from template.\"\"\" return self . special_tokens","title":"get_special_tokens()"},{"location":"inference_engine/reference/#npc_engine.services.chatbot.bart.BartChatbot.run","text":"Run text generation from given prompt and parameters. Parameters: Name Type Description Default prompt str Fromatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1.0 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text Source code in npc_engine\\services\\chatbot\\bart.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def run ( self , prompt : str , temperature : float = 1.0 , topk : int = None ) -> str : \"\"\"Run text generation from given prompt and parameters. Args: prompt: Fromatted prompt. temperature: Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness topk: If not none selects top n of predictions to sample from during generation. Returns: Generated text \"\"\" tokens = self . tokenizer . encode ( prompt ) total = np . asarray ( tokens . ids , dtype = np . int64 ) . reshape ([ 1 , - 1 ]) total_enc = self . encoder_model . run ( None , { \"input_ids\" : total })[ 0 ] utterance = np . asarray ([ self . eos_token_id ], dtype = np . int64 ) . reshape ([ 1 , 1 ]) for i in range ( self . max_steps ): o = self . decoder_model . run ( None , { \"encoder_hidden_state\" : total_enc , \"decoder_input_ids\" : utterance }, ) logits = o [ 0 ][ 0 , - 1 , :] if i < self . min_length : logits [ self . eos_token_id ] = float ( \"-inf\" ) if topk is not None : ind = np . argpartition ( logits , - topk )[ - topk :] new_logits = np . zeros ( logits . shape ) new_logits [ ind ] = logits [ ind ] logits = new_logits probs = scp . softmax ( logits / temperature , axis = 0 ) token = np . random . choice ( np . arange ( probs . shape [ 0 ]), p = probs ) token = token . reshape ([ 1 , 1 ]) utterance = np . concatenate ([ utterance , token ], axis = 1 ) if token [ 0 , 0 ] == self . eos_token_id : break return self . tokenizer . decode ( utterance [ 0 , :] . tolist (), skip_special_tokens = True )","title":"run()"},{"location":"inference_engine/reference/#npc_engine.services.chatbot.chatbot_base","text":"Module that implements chatbot model API.","title":"chatbot_base"},{"location":"inference_engine/reference/#npc_engine.services.chatbot.chatbot_base.ChatbotAPI","text":"Bases: BaseService Abstract base class for Chatbot models. Source code in npc_engine\\services\\chatbot\\chatbot_base.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 class ChatbotAPI ( BaseService ): \"\"\"Abstract base class for Chatbot models.\"\"\" API_METHODS : List [ str ] = [ \"generate_reply\" , \"get_prompt_template\" , \"get_special_tokens\" , \"get_context_template\" , ] def __init__ ( self , template_string : str , * args , ** kwargs ): \"\"\"Initialize prompt formatting variables. Args: template_string: Template string to be rendered as prompt. \"\"\" super () . __init__ ( * args , ** kwargs ) self . template_string = template_string self . template = Template ( template_string ) self . initialized = True @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"ChatbotAPI\" def generate_reply ( self , context : Dict [ str , Any ], * args , ** kwargs ) -> str : \"\"\"Format the model prompt and generate response. Args: context: Prompt context. *args **kwargs Returns: Text response to a prompt. \"\"\" if not self . initialized : raise AssertionError ( \"Can not generate replies before base Chatbot class was initialized\" ) prompt = self . template . render ( ** context , ** self . get_special_tokens ()) return self . run ( prompt , * args , ** kwargs ) @abstractmethod def run ( self , prompt : str , temperature : float = 1 , topk : int = None ) -> str : \"\"\"Abstract method for concrete implementation of generation. Args: prompt: Fromatted prompt. temperature: Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness topk: If not none selects top n of predictions to sample from during generation. Returns: Generated text \"\"\" return None @abstractmethod def get_special_tokens ( self ) -> Dict [ str , str ]: \"\"\"Return dictionary mapping for special tokens. To be implemented by child class. Can then be used in template string as fields Returns: Dictionary of special tokens \"\"\" return None def get_prompt_template ( self ) -> str : \"\"\"Return prompt template string used to render model prompt. Returns: A template string. \"\"\" return self . template_string def get_context_template ( self ) -> Dict [ str , Any ]: \"\"\"Return context template. Returns: Example context \"\"\" return schema_to_json ( to_json_schema ( infer ( self . template_string )))","title":"ChatbotAPI"},{"location":"inference_engine/reference/#npc_engine.services.chatbot.chatbot_base.ChatbotAPI.__init__","text":"Initialize prompt formatting variables. Parameters: Name Type Description Default template_string str Template string to be rendered as prompt. required Source code in npc_engine\\services\\chatbot\\chatbot_base.py 22 23 24 25 26 27 28 29 30 31 def __init__ ( self , template_string : str , * args , ** kwargs ): \"\"\"Initialize prompt formatting variables. Args: template_string: Template string to be rendered as prompt. \"\"\" super () . __init__ ( * args , ** kwargs ) self . template_string = template_string self . template = Template ( template_string ) self . initialized = True","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.services.chatbot.chatbot_base.ChatbotAPI.generate_reply","text":"Format the model prompt and generate response. Parameters: Name Type Description Default context Dict [ str , Any ] Prompt context. required Returns: Type Description str Text response to a prompt. Source code in npc_engine\\services\\chatbot\\chatbot_base.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def generate_reply ( self , context : Dict [ str , Any ], * args , ** kwargs ) -> str : \"\"\"Format the model prompt and generate response. Args: context: Prompt context. *args **kwargs Returns: Text response to a prompt. \"\"\" if not self . initialized : raise AssertionError ( \"Can not generate replies before base Chatbot class was initialized\" ) prompt = self . template . render ( ** context , ** self . get_special_tokens ()) return self . run ( prompt , * args , ** kwargs )","title":"generate_reply()"},{"location":"inference_engine/reference/#npc_engine.services.chatbot.chatbot_base.ChatbotAPI.get_api_name","text":"Get the API name. Source code in npc_engine\\services\\chatbot\\chatbot_base.py 33 34 35 36 @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"ChatbotAPI\"","title":"get_api_name()"},{"location":"inference_engine/reference/#npc_engine.services.chatbot.chatbot_base.ChatbotAPI.get_context_template","text":"Return context template. Returns: Type Description Dict [ str , Any ] Example context Source code in npc_engine\\services\\chatbot\\chatbot_base.py 90 91 92 93 94 95 96 def get_context_template ( self ) -> Dict [ str , Any ]: \"\"\"Return context template. Returns: Example context \"\"\" return schema_to_json ( to_json_schema ( infer ( self . template_string )))","title":"get_context_template()"},{"location":"inference_engine/reference/#npc_engine.services.chatbot.chatbot_base.ChatbotAPI.get_prompt_template","text":"Return prompt template string used to render model prompt. Returns: Type Description str A template string. Source code in npc_engine\\services\\chatbot\\chatbot_base.py 82 83 84 85 86 87 88 def get_prompt_template ( self ) -> str : \"\"\"Return prompt template string used to render model prompt. Returns: A template string. \"\"\" return self . template_string","title":"get_prompt_template()"},{"location":"inference_engine/reference/#npc_engine.services.chatbot.chatbot_base.ChatbotAPI.get_special_tokens","text":"Return dictionary mapping for special tokens. To be implemented by child class. Can then be used in template string as fields Returns: Type Description Dict [ str , str ] Dictionary of special tokens Source code in npc_engine\\services\\chatbot\\chatbot_base.py 71 72 73 74 75 76 77 78 79 80 @abstractmethod def get_special_tokens ( self ) -> Dict [ str , str ]: \"\"\"Return dictionary mapping for special tokens. To be implemented by child class. Can then be used in template string as fields Returns: Dictionary of special tokens \"\"\" return None","title":"get_special_tokens()"},{"location":"inference_engine/reference/#npc_engine.services.chatbot.chatbot_base.ChatbotAPI.run","text":"Abstract method for concrete implementation of generation. Parameters: Name Type Description Default prompt str Fromatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text Source code in npc_engine\\services\\chatbot\\chatbot_base.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 @abstractmethod def run ( self , prompt : str , temperature : float = 1 , topk : int = None ) -> str : \"\"\"Abstract method for concrete implementation of generation. Args: prompt: Fromatted prompt. temperature: Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness topk: If not none selects top n of predictions to sample from during generation. Returns: Generated text \"\"\" return None","title":"run()"},{"location":"inference_engine/reference/#npc_engine.services.chatbot.hf_chatbot","text":"BART based chatbot implementation.","title":"hf_chatbot"},{"location":"inference_engine/reference/#npc_engine.services.chatbot.hf_chatbot.HfChatbot","text":"Bases: ChatbotAPI Chatbot that uses Huggingface transformer architectures. ONNX export of Huggingface transformer is required (see https://huggingface.co/docs/transformers/serialization). Features seq2seq-lm, causal-lm, seq2seq-lm-with-past, causal-lm-with-past are supported Source code in npc_engine\\services\\chatbot\\hf_chatbot.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 class HfChatbot ( ChatbotAPI ): \"\"\"Chatbot that uses Huggingface transformer architectures. ONNX export of Huggingface transformer is required (see https://huggingface.co/docs/transformers/serialization). Features seq2seq-lm, causal-lm, seq2seq-lm-with-past, causal-lm-with-past are supported \"\"\" def __init__ ( self , model_path , max_length = 100 , min_length = 2 , repetition_penalty = 1 , * args , ** kwargs , ): \"\"\"Create the chatbot from config args and kwargs. Args: model_path: path to scan for model files (weights and configs) max_length: stop generation at this number of tokens min_length: model can't stop generating text before it's atleast this long in tokens repetition_penalty: probability coef for same tokens to appear multiple times \"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = rt . GraphOptimizationLevel . ORT_ENABLE_ALL self . model = rt . InferenceSession ( os . path . join ( model_path , \"model.onnx\" ), providers = [ rt . get_available_providers ()[ 0 ]], sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) self . max_steps = max_length self . min_length = min_length self . repetition_penalty = repetition_penalty special_tokens_map_path = os . path . join ( model_path , \"special_tokens_map.json\" ) with open ( special_tokens_map_path , \"r\" ) as f : self . special_tokens = json . load ( f ) self . eos_token_id = self . tokenizer . encode ( self . special_tokens [ \"eos_token\" ]) . ids [ 0 ] self . model_inputs = self . model . get_inputs () self . is_encdec = ( len ([ i . name for i in self . model_inputs if \"decoder\" in i . name ]) > 0 ) self . with_past = ( len ([ i . name for i in self . model_inputs if \"past_key_values\" in i . name ]) > 0 ) self . shape_dict = { \"batch\" : 1 , \"past_encoder_sequence\" : 0 , \"past_decoder_sequence\" : 0 , \"past_sequence + sequence\" : 0 , } self . dtypes = { i . name : DTYPE_MAP [ i . type ] for i in self . model_inputs } def run ( self , prompt : str , temperature : float = 1.0 , topk : int = None ) -> str : \"\"\"Run text generation from given prompt and parameters. Args: prompt: Fromatted prompt. temperature: Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness topk: If not none selects top n of predictions to sample from during generation. Returns: Generated text \"\"\" inputs = self . create_starter_inputs ( prompt ) utterance = [] for i in range ( self . max_steps ): o = self . model . run ( None , inputs ,) logit = o [ 0 ][ 0 , - 1 , :] if i < self . min_length : logit [ self . eos_token_id ] = float ( \"-inf\" ) token = self . decode_logit ( logit , temperature , topk ) utterance . append ( token ) result_dict = { outp . name : o [ i ] for i , outp in enumerate ( self . model . get_outputs ()) } inputs = self . update_inputs_with_results ( inputs , result_dict , token ) if token == self . eos_token_id : break return self . tokenizer . decode ( utterance , skip_special_tokens = True ) def create_starter_inputs ( self , prompt : str = \"\" ) -> Dict [ str , Any ]: \"\"\"Create starter inputs for the model. Args: prompt: Prompt to start generation from. Returns: Dict of inputs to the model \"\"\" tokens = self . tokenizer . encode ( prompt ) . ids inputs = {} if self . is_encdec : prompt_start = tokens [ - 1 :] inputs [ \"input_ids\" ] = np . asarray ( tokens [: - 1 ], dtype = self . dtypes [ \"input_ids\" ] ) . reshape ([ 1 , - 1 ]) inputs [ \"decoder_input_ids\" ] = np . asarray ( prompt_start , dtype = self . dtypes [ \"decoder_input_ids\" ] ) . reshape ([ 1 , - 1 ]) inputs [ \"attention_mask\" ] = np . ones ( [ 1 , 6 ], dtype = self . dtypes [ \"attention_mask\" ] ) inputs [ \"decoder_attention_mask\" ] = np . ones ( [ 1 , 3 ], dtype = self . dtypes [ \"decoder_attention_mask\" ] ) else : inputs [ \"input_ids\" ] = np . asarray ( tokens , dtype = self . dtypes [ \"input_ids\" ] ) . reshape ([ 1 , - 1 ]) inputs [ \"attention_mask\" ] = np . ones_like ( inputs [ \"input_ids\" ], dtype = self . dtypes [ \"attention_mask\" ] ) if self . with_past : for i in self . model_inputs : if \"past_key_values\" in i . name : shape_tuple = [ self . shape_dict . get ( dim , dim ) for dim in i . shape ] inputs [ i . name ] = np . empty ( shape_tuple , dtype = self . dtypes [ i . name ]) return inputs def decode_logit ( self , logit : np . ndarray , temperature : float , topk : int ) -> int : \"\"\"Decode logit to token. Args: logit: Logit to decode of shape (vocab_size,) Returns: Decoded token of shape \"\"\" if topk is not None : ind = np . argpartition ( logit , - topk )[ - topk :] new_logits = np . zeros ( logit . shape ) new_logits [ ind ] = logit [ ind ] logit = new_logits probs = scp . softmax ( logit / temperature , axis = 0 ) token = np . random . choice ( np . arange ( probs . shape [ 0 ]), p = probs ) token = token . ravel ()[ 0 ] return token def update_inputs_with_results ( self , inputs : Dict [ str , np . ndarray ], results : Dict [ str , np . ndarray ], decoded_token : int , ) -> Dict [ str , np . ndarray ]: \"\"\"Update inputs with results from model. Args: inputs: Inputs to the model results: Results from the model Returns: Updated inputs Finished generation \"\"\" ids_name = \"decoder_input_ids\" if self . is_encdec else \"input_ids\" att_mask_name = \"decoder_attention_mask\" if self . is_encdec else \"attention_mask\" if self . with_past : inputs [ ids_name ] = np . asarray ( [ decoded_token ], dtype = self . dtypes [ ids_name ] ) . reshape ([ 1 , - 1 ]) inputs [ att_mask_name ] = np . ones ( [ 1 , inputs [ att_mask_name ] . shape [ - 1 ] + 1 ], dtype = self . dtypes [ att_mask_name ], ) for inp in self . model_inputs : if \"past_key_values\" in inp . name : inputs [ inp . name ] = results [ inp . name . replace ( \"past_key_values\" , \"present\" ) ] if self . is_encdec : inputs . pop ( \"input_ids\" , None ) else : decoder_input_ids = inputs [ ids_name ] decoder_attention_mask = inputs [ att_mask_name ] decoder_input_ids = np . concatenate ( [ decoder_input_ids , np . asarray ([[ decoded_token ]], dtype = np . int32 )], axis = 1 , ) decoder_attention_mask = np . ones_like ( decoder_input_ids , dtype = self . dtypes [ att_mask_name ] ) inputs [ ids_name ] = decoder_input_ids inputs [ att_mask_name ] = decoder_attention_mask return inputs def get_special_tokens ( self ) -> Dict [ str , str ]: \"\"\"Retrun dict of special tokens to be renderable from template.\"\"\" return self . special_tokens","title":"HfChatbot"},{"location":"inference_engine/reference/#npc_engine.services.chatbot.hf_chatbot.HfChatbot.__init__","text":"Create the chatbot from config args and kwargs. Parameters: Name Type Description Default model_path path to scan for model files (weights and configs) required max_length stop generation at this number of tokens 100 min_length model can't stop generating text before it's atleast this long in tokens 2 repetition_penalty probability coef for same tokens to appear multiple times 1 Source code in npc_engine\\services\\chatbot\\hf_chatbot.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def __init__ ( self , model_path , max_length = 100 , min_length = 2 , repetition_penalty = 1 , * args , ** kwargs , ): \"\"\"Create the chatbot from config args and kwargs. Args: model_path: path to scan for model files (weights and configs) max_length: stop generation at this number of tokens min_length: model can't stop generating text before it's atleast this long in tokens repetition_penalty: probability coef for same tokens to appear multiple times \"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = rt . GraphOptimizationLevel . ORT_ENABLE_ALL self . model = rt . InferenceSession ( os . path . join ( model_path , \"model.onnx\" ), providers = [ rt . get_available_providers ()[ 0 ]], sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) self . max_steps = max_length self . min_length = min_length self . repetition_penalty = repetition_penalty special_tokens_map_path = os . path . join ( model_path , \"special_tokens_map.json\" ) with open ( special_tokens_map_path , \"r\" ) as f : self . special_tokens = json . load ( f ) self . eos_token_id = self . tokenizer . encode ( self . special_tokens [ \"eos_token\" ]) . ids [ 0 ] self . model_inputs = self . model . get_inputs () self . is_encdec = ( len ([ i . name for i in self . model_inputs if \"decoder\" in i . name ]) > 0 ) self . with_past = ( len ([ i . name for i in self . model_inputs if \"past_key_values\" in i . name ]) > 0 ) self . shape_dict = { \"batch\" : 1 , \"past_encoder_sequence\" : 0 , \"past_decoder_sequence\" : 0 , \"past_sequence + sequence\" : 0 , } self . dtypes = { i . name : DTYPE_MAP [ i . type ] for i in self . model_inputs }","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.services.chatbot.hf_chatbot.HfChatbot.create_starter_inputs","text":"Create starter inputs for the model. Parameters: Name Type Description Default prompt str Prompt to start generation from. '' Returns: Type Description Dict [ str , Any ] Dict of inputs to the model Source code in npc_engine\\services\\chatbot\\hf_chatbot.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def create_starter_inputs ( self , prompt : str = \"\" ) -> Dict [ str , Any ]: \"\"\"Create starter inputs for the model. Args: prompt: Prompt to start generation from. Returns: Dict of inputs to the model \"\"\" tokens = self . tokenizer . encode ( prompt ) . ids inputs = {} if self . is_encdec : prompt_start = tokens [ - 1 :] inputs [ \"input_ids\" ] = np . asarray ( tokens [: - 1 ], dtype = self . dtypes [ \"input_ids\" ] ) . reshape ([ 1 , - 1 ]) inputs [ \"decoder_input_ids\" ] = np . asarray ( prompt_start , dtype = self . dtypes [ \"decoder_input_ids\" ] ) . reshape ([ 1 , - 1 ]) inputs [ \"attention_mask\" ] = np . ones ( [ 1 , 6 ], dtype = self . dtypes [ \"attention_mask\" ] ) inputs [ \"decoder_attention_mask\" ] = np . ones ( [ 1 , 3 ], dtype = self . dtypes [ \"decoder_attention_mask\" ] ) else : inputs [ \"input_ids\" ] = np . asarray ( tokens , dtype = self . dtypes [ \"input_ids\" ] ) . reshape ([ 1 , - 1 ]) inputs [ \"attention_mask\" ] = np . ones_like ( inputs [ \"input_ids\" ], dtype = self . dtypes [ \"attention_mask\" ] ) if self . with_past : for i in self . model_inputs : if \"past_key_values\" in i . name : shape_tuple = [ self . shape_dict . get ( dim , dim ) for dim in i . shape ] inputs [ i . name ] = np . empty ( shape_tuple , dtype = self . dtypes [ i . name ]) return inputs","title":"create_starter_inputs()"},{"location":"inference_engine/reference/#npc_engine.services.chatbot.hf_chatbot.HfChatbot.decode_logit","text":"Decode logit to token. Parameters: Name Type Description Default logit np . ndarray Logit to decode of shape (vocab_size,) required Returns: Type Description int Decoded token of shape Source code in npc_engine\\services\\chatbot\\hf_chatbot.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def decode_logit ( self , logit : np . ndarray , temperature : float , topk : int ) -> int : \"\"\"Decode logit to token. Args: logit: Logit to decode of shape (vocab_size,) Returns: Decoded token of shape \"\"\" if topk is not None : ind = np . argpartition ( logit , - topk )[ - topk :] new_logits = np . zeros ( logit . shape ) new_logits [ ind ] = logit [ ind ] logit = new_logits probs = scp . softmax ( logit / temperature , axis = 0 ) token = np . random . choice ( np . arange ( probs . shape [ 0 ]), p = probs ) token = token . ravel ()[ 0 ] return token","title":"decode_logit()"},{"location":"inference_engine/reference/#npc_engine.services.chatbot.hf_chatbot.HfChatbot.get_special_tokens","text":"Retrun dict of special tokens to be renderable from template. Source code in npc_engine\\services\\chatbot\\hf_chatbot.py 213 214 215 def get_special_tokens ( self ) -> Dict [ str , str ]: \"\"\"Retrun dict of special tokens to be renderable from template.\"\"\" return self . special_tokens","title":"get_special_tokens()"},{"location":"inference_engine/reference/#npc_engine.services.chatbot.hf_chatbot.HfChatbot.run","text":"Run text generation from given prompt and parameters. Parameters: Name Type Description Default prompt str Fromatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1.0 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text Source code in npc_engine\\services\\chatbot\\hf_chatbot.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def run ( self , prompt : str , temperature : float = 1.0 , topk : int = None ) -> str : \"\"\"Run text generation from given prompt and parameters. Args: prompt: Fromatted prompt. temperature: Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness topk: If not none selects top n of predictions to sample from during generation. Returns: Generated text \"\"\" inputs = self . create_starter_inputs ( prompt ) utterance = [] for i in range ( self . max_steps ): o = self . model . run ( None , inputs ,) logit = o [ 0 ][ 0 , - 1 , :] if i < self . min_length : logit [ self . eos_token_id ] = float ( \"-inf\" ) token = self . decode_logit ( logit , temperature , topk ) utterance . append ( token ) result_dict = { outp . name : o [ i ] for i , outp in enumerate ( self . model . get_outputs ()) } inputs = self . update_inputs_with_results ( inputs , result_dict , token ) if token == self . eos_token_id : break return self . tokenizer . decode ( utterance , skip_special_tokens = True )","title":"run()"},{"location":"inference_engine/reference/#npc_engine.services.chatbot.hf_chatbot.HfChatbot.update_inputs_with_results","text":"Update inputs with results from model. Parameters: Name Type Description Default inputs Dict [ str , np . ndarray ] Inputs to the model required results Dict [ str , np . ndarray ] Results from the model required Returns: Type Description Dict [ str , np . ndarray ] Updated inputs Dict [ str , np . ndarray ] Finished generation Source code in npc_engine\\services\\chatbot\\hf_chatbot.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def update_inputs_with_results ( self , inputs : Dict [ str , np . ndarray ], results : Dict [ str , np . ndarray ], decoded_token : int , ) -> Dict [ str , np . ndarray ]: \"\"\"Update inputs with results from model. Args: inputs: Inputs to the model results: Results from the model Returns: Updated inputs Finished generation \"\"\" ids_name = \"decoder_input_ids\" if self . is_encdec else \"input_ids\" att_mask_name = \"decoder_attention_mask\" if self . is_encdec else \"attention_mask\" if self . with_past : inputs [ ids_name ] = np . asarray ( [ decoded_token ], dtype = self . dtypes [ ids_name ] ) . reshape ([ 1 , - 1 ]) inputs [ att_mask_name ] = np . ones ( [ 1 , inputs [ att_mask_name ] . shape [ - 1 ] + 1 ], dtype = self . dtypes [ att_mask_name ], ) for inp in self . model_inputs : if \"past_key_values\" in inp . name : inputs [ inp . name ] = results [ inp . name . replace ( \"past_key_values\" , \"present\" ) ] if self . is_encdec : inputs . pop ( \"input_ids\" , None ) else : decoder_input_ids = inputs [ ids_name ] decoder_attention_mask = inputs [ att_mask_name ] decoder_input_ids = np . concatenate ( [ decoder_input_ids , np . asarray ([[ decoded_token ]], dtype = np . int32 )], axis = 1 , ) decoder_attention_mask = np . ones_like ( decoder_input_ids , dtype = self . dtypes [ att_mask_name ] ) inputs [ ids_name ] = decoder_input_ids inputs [ att_mask_name ] = decoder_attention_mask return inputs","title":"update_inputs_with_results()"},{"location":"inference_engine/reference/#npc_engine.services.sequence_classifier","text":"Sequence classification API module.","title":"sequence_classifier"},{"location":"inference_engine/reference/#npc_engine.services.sequence_classifier.hf_classifier","text":"Module that implements Huggingface transformers classification.","title":"hf_classifier"},{"location":"inference_engine/reference/#npc_engine.services.sequence_classifier.hf_classifier.HfClassifier","text":"Bases: SequenceClassifierAPI Huggingface transformers sequence classification. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with sequence-classification feature. Also requires saved tokenizer with huggingface tokenizers. Source code in npc_engine\\services\\sequence_classifier\\hf_classifier.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class HfClassifier ( SequenceClassifierAPI ): \"\"\"Huggingface transformers sequence classification. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with sequence-classification feature. Also requires saved tokenizer with huggingface tokenizers. \"\"\" def __init__ ( self , model_path : str , * args , ** kwargs ): \"\"\"Create and load biencoder model for semantic similarity. Args: model_path: A path where model config and weights are. \"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_ALL self . model = rt . InferenceSession ( os . path . join ( model_path , \"model.onnx\" ), providers = [ rt . get_available_providers ()[ 0 ]], sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) input_names = [ inp . name for inp in self . model . get_inputs ()] self . token_type_support = \"token_type_ids\" in input_names special_tokens_map_path = os . path . join ( model_path , \"special_tokens_map.json\" ) with open ( special_tokens_map_path , \"r\" ) as f : self . special_tokens = json . load ( f ) self . pad_token_id = self . tokenizer . encode ( self . special_tokens [ \"pad_token\" ]) . ids [ 0 ] self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) self . tokenizer . enable_padding ( direction = \"right\" , pad_id = self . pad_token_id , pad_type_id = 0 , pad_token = self . tokenizer . decode ( [ self . pad_token_id ], skip_special_tokens = False ), length = None , pad_to_multiple_of = None , ) self . tests = {} def compute_scores_batch ( self , texts : List [ Union [ str , Tuple [ str , str ]]] ) -> np . ndarray : \"\"\"Compute scores. Args: texts: Sentence to embed Returns: scores: Scores for each text \"\"\" tokenized = self . tokenizer . encode_batch ( texts ) ids = np . stack ([ np . asarray ( encoding . ids ) for encoding in tokenized ]) . astype ( np . int64 ) attention_mask = np . stack ( [ np . asarray ( encoding . attention_mask ) for encoding in tokenized ] ) . astype ( np . int64 ) if not self . token_type_support : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask } else : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : np . stack ( [ np . asarray ( encoding . type_ids ) for encoding in tokenized ] ) . astype ( np . int64 ), } outp = self . model . run ( None , input_dict ) return outp [ 0 ]","title":"HfClassifier"},{"location":"inference_engine/reference/#npc_engine.services.sequence_classifier.hf_classifier.HfClassifier.__init__","text":"Create and load biencoder model for semantic similarity. Parameters: Name Type Description Default model_path str A path where model config and weights are. required Source code in npc_engine\\services\\sequence_classifier\\hf_classifier.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , model_path : str , * args , ** kwargs ): \"\"\"Create and load biencoder model for semantic similarity. Args: model_path: A path where model config and weights are. \"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_ALL self . model = rt . InferenceSession ( os . path . join ( model_path , \"model.onnx\" ), providers = [ rt . get_available_providers ()[ 0 ]], sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) input_names = [ inp . name for inp in self . model . get_inputs ()] self . token_type_support = \"token_type_ids\" in input_names special_tokens_map_path = os . path . join ( model_path , \"special_tokens_map.json\" ) with open ( special_tokens_map_path , \"r\" ) as f : self . special_tokens = json . load ( f ) self . pad_token_id = self . tokenizer . encode ( self . special_tokens [ \"pad_token\" ]) . ids [ 0 ] self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) self . tokenizer . enable_padding ( direction = \"right\" , pad_id = self . pad_token_id , pad_type_id = 0 , pad_token = self . tokenizer . decode ( [ self . pad_token_id ], skip_special_tokens = False ), length = None , pad_to_multiple_of = None , ) self . tests = {}","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.services.sequence_classifier.hf_classifier.HfClassifier.compute_scores_batch","text":"Compute scores. Parameters: Name Type Description Default texts List [ Union [ str , Tuple [ str , str ]]] Sentence to embed required Returns: Name Type Description scores np . ndarray Scores for each text Source code in npc_engine\\services\\sequence_classifier\\hf_classifier.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def compute_scores_batch ( self , texts : List [ Union [ str , Tuple [ str , str ]]] ) -> np . ndarray : \"\"\"Compute scores. Args: texts: Sentence to embed Returns: scores: Scores for each text \"\"\" tokenized = self . tokenizer . encode_batch ( texts ) ids = np . stack ([ np . asarray ( encoding . ids ) for encoding in tokenized ]) . astype ( np . int64 ) attention_mask = np . stack ( [ np . asarray ( encoding . attention_mask ) for encoding in tokenized ] ) . astype ( np . int64 ) if not self . token_type_support : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask } else : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : np . stack ( [ np . asarray ( encoding . type_ids ) for encoding in tokenized ] ) . astype ( np . int64 ), } outp = self . model . run ( None , input_dict ) return outp [ 0 ]","title":"compute_scores_batch()"},{"location":"inference_engine/reference/#npc_engine.services.sequence_classifier.sequence_classifier_base","text":"Module that implements sequence classification API.","title":"sequence_classifier_base"},{"location":"inference_engine/reference/#npc_engine.services.sequence_classifier.sequence_classifier_base.SequenceClassifierAPI","text":"Bases: BaseService Abstract base class for text classification models. Source code in npc_engine\\services\\sequence_classifier\\sequence_classifier_base.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class SequenceClassifierAPI ( BaseService ): \"\"\"Abstract base class for text classification models.\"\"\" API_METHODS : List [ str ] = [ \"classify\" ] def __init__ ( self , cache_size = 0 , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True self . cache = NumpyLRUCache ( cache_size ) @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"SequenceClassifierAPI\" def classify ( self , texts : List [ str ]) -> List [ List [ float ]]: \"\"\"Classify a list of texts. Args: texts: A list of texts to classify. Returns: List of scores for each text. \"\"\" texts = [ row if isinstance ( row , str ) else tuple ( row ) for row in texts ] scores = self . cache . cache_compute ( texts , lambda values : self . compute_scores_batch ( values ) ) return scores . tolist () @abstractmethod def compute_scores_batch ( self , texts : List [ str ]) -> np . ndarray : \"\"\"Compute scores for a list of texts. Args: texts: A list of texts to compute scores for. Returns: List of scores for each text. \"\"\" pass","title":"SequenceClassifierAPI"},{"location":"inference_engine/reference/#npc_engine.services.sequence_classifier.sequence_classifier_base.SequenceClassifierAPI.__init__","text":"Empty initialization method for API to be similar to other model base classes. Source code in npc_engine\\services\\sequence_classifier\\sequence_classifier_base.py 15 16 17 18 19 def __init__ ( self , cache_size = 0 , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True self . cache = NumpyLRUCache ( cache_size )","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.services.sequence_classifier.sequence_classifier_base.SequenceClassifierAPI.classify","text":"Classify a list of texts. Parameters: Name Type Description Default texts List [ str ] A list of texts to classify. required Returns: Type Description List [ List [ float ]] List of scores for each text. Source code in npc_engine\\services\\sequence_classifier\\sequence_classifier_base.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def classify ( self , texts : List [ str ]) -> List [ List [ float ]]: \"\"\"Classify a list of texts. Args: texts: A list of texts to classify. Returns: List of scores for each text. \"\"\" texts = [ row if isinstance ( row , str ) else tuple ( row ) for row in texts ] scores = self . cache . cache_compute ( texts , lambda values : self . compute_scores_batch ( values ) ) return scores . tolist ()","title":"classify()"},{"location":"inference_engine/reference/#npc_engine.services.sequence_classifier.sequence_classifier_base.SequenceClassifierAPI.compute_scores_batch","text":"Compute scores for a list of texts. Parameters: Name Type Description Default texts List [ str ] A list of texts to compute scores for. required Returns: Type Description np . ndarray List of scores for each text. Source code in npc_engine\\services\\sequence_classifier\\sequence_classifier_base.py 41 42 43 44 45 46 47 48 49 50 51 @abstractmethod def compute_scores_batch ( self , texts : List [ str ]) -> np . ndarray : \"\"\"Compute scores for a list of texts. Args: texts: A list of texts to compute scores for. Returns: List of scores for each text. \"\"\" pass","title":"compute_scores_batch()"},{"location":"inference_engine/reference/#npc_engine.services.sequence_classifier.sequence_classifier_base.SequenceClassifierAPI.get_api_name","text":"Get the API name. Source code in npc_engine\\services\\sequence_classifier\\sequence_classifier_base.py 21 22 23 24 @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"SequenceClassifierAPI\"","title":"get_api_name()"},{"location":"inference_engine/reference/#npc_engine.services.similarity","text":"Similarity model implementations. This module implements specific models and wraps them under the common interface for loading and inference. Example from npc_engine.models.similarity import SimilarityAPI model = SimilarityAPI.load(\"path/to/model_dir\") model.compare(\"hello\", [\"Hello, world!\"])","title":"similarity"},{"location":"inference_engine/reference/#npc_engine.services.similarity.similarity_base","text":"Module that implements semantic similarity model API.","title":"similarity_base"},{"location":"inference_engine/reference/#npc_engine.services.similarity.similarity_base.SimilarityAPI","text":"Bases: BaseService Abstract base class for text similarity models. Source code in npc_engine\\services\\similarity\\similarity_base.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class SimilarityAPI ( BaseService ): \"\"\"Abstract base class for text similarity models.\"\"\" API_METHODS : List [ str ] = [ \"compare\" , \"cache\" ] def __init__ ( self , cache_size = 0 , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True self . lru_cache = NumpyLRUCache ( cache_size ) @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"SimilarityAPI\" def compare ( self , query : str , context : List [ str ]) -> List [ float ]: \"\"\"Compare a query to the context. Args: query: A sentence to compare. context: A list of sentences to compare to. This will be cached if caching is enabled Returns: List of similarities \"\"\" embedding_a = self . compute_embedding ( query ) embedding_b = self . lru_cache . cache_compute ( context , lambda values : self . compute_embedding_batch ( values ) ) similarities = self . metric ( embedding_a , embedding_b ) return similarities . tolist () def cache ( self , context : List [ str ]): \"\"\"Cache embeddings of given sequences. Args: context: A list of sentences to cache. \"\"\" self . lru_cache . cache_compute ( context , lambda values : self . compute_embedding_batch ( values ) ) @abstractmethod def compute_embedding_batch ( self , lines : List [ str ]) -> np . ndarray : \"\"\"Compute line embeddings in batch. Args: lines: List of sentences to embed Returns: Embedding batch of shape (batch_size, embedding_size) \"\"\" return None @abstractmethod def compute_embedding ( self , line : str ) -> np . ndarray : \"\"\"Compute sentence embedding. Args: line: Sentence to embed Returns: Embedding of shape (1, embedding_size) \"\"\" return None @abstractmethod def metric ( self , embedding_a : np . ndarray , embedding_b : np . ndarray ) -> np . ndarray : \"\"\"Compute distance between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Args: embedding_a: Embedding of shape (1 or batch_size, embedding_size) embedding_b: Embedding of shape (1 or batch_size, embedding_size) Returns: Vector of distances (batch_size or 1,) \"\"\" return None","title":"SimilarityAPI"},{"location":"inference_engine/reference/#npc_engine.services.similarity.similarity_base.SimilarityAPI.__init__","text":"Empty initialization method for API to be similar to other model base classes. Source code in npc_engine\\services\\similarity\\similarity_base.py 15 16 17 18 19 def __init__ ( self , cache_size = 0 , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True self . lru_cache = NumpyLRUCache ( cache_size )","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.services.similarity.similarity_base.SimilarityAPI.cache","text":"Cache embeddings of given sequences. Parameters: Name Type Description Default context List [ str ] A list of sentences to cache. required Source code in npc_engine\\services\\similarity\\similarity_base.py 43 44 45 46 47 48 49 50 51 def cache ( self , context : List [ str ]): \"\"\"Cache embeddings of given sequences. Args: context: A list of sentences to cache. \"\"\" self . lru_cache . cache_compute ( context , lambda values : self . compute_embedding_batch ( values ) )","title":"cache()"},{"location":"inference_engine/reference/#npc_engine.services.similarity.similarity_base.SimilarityAPI.compare","text":"Compare a query to the context. Parameters: Name Type Description Default query str A sentence to compare. required context List [ str ] A list of sentences to compare to. This will be cached if caching is enabled required Returns: Type Description List [ float ] List of similarities Source code in npc_engine\\services\\similarity\\similarity_base.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def compare ( self , query : str , context : List [ str ]) -> List [ float ]: \"\"\"Compare a query to the context. Args: query: A sentence to compare. context: A list of sentences to compare to. This will be cached if caching is enabled Returns: List of similarities \"\"\" embedding_a = self . compute_embedding ( query ) embedding_b = self . lru_cache . cache_compute ( context , lambda values : self . compute_embedding_batch ( values ) ) similarities = self . metric ( embedding_a , embedding_b ) return similarities . tolist ()","title":"compare()"},{"location":"inference_engine/reference/#npc_engine.services.similarity.similarity_base.SimilarityAPI.compute_embedding","text":"Compute sentence embedding. Parameters: Name Type Description Default line str Sentence to embed required Returns: Type Description np . ndarray Embedding of shape (1, embedding_size) Source code in npc_engine\\services\\similarity\\similarity_base.py 65 66 67 68 69 70 71 72 73 74 75 @abstractmethod def compute_embedding ( self , line : str ) -> np . ndarray : \"\"\"Compute sentence embedding. Args: line: Sentence to embed Returns: Embedding of shape (1, embedding_size) \"\"\" return None","title":"compute_embedding()"},{"location":"inference_engine/reference/#npc_engine.services.similarity.similarity_base.SimilarityAPI.compute_embedding_batch","text":"Compute line embeddings in batch. Parameters: Name Type Description Default lines List [ str ] List of sentences to embed required Returns: Type Description np . ndarray Embedding batch of shape (batch_size, embedding_size) Source code in npc_engine\\services\\similarity\\similarity_base.py 53 54 55 56 57 58 59 60 61 62 63 @abstractmethod def compute_embedding_batch ( self , lines : List [ str ]) -> np . ndarray : \"\"\"Compute line embeddings in batch. Args: lines: List of sentences to embed Returns: Embedding batch of shape (batch_size, embedding_size) \"\"\" return None","title":"compute_embedding_batch()"},{"location":"inference_engine/reference/#npc_engine.services.similarity.similarity_base.SimilarityAPI.get_api_name","text":"Get the API name. Source code in npc_engine\\services\\similarity\\similarity_base.py 21 22 23 24 @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"SimilarityAPI\"","title":"get_api_name()"},{"location":"inference_engine/reference/#npc_engine.services.similarity.similarity_base.SimilarityAPI.metric","text":"Compute distance between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Parameters: Name Type Description Default embedding_a np . ndarray Embedding of shape (1 or batch_size, embedding_size) required embedding_b np . ndarray Embedding of shape (1 or batch_size, embedding_size) required Returns: Type Description np . ndarray Vector of distances (batch_size or 1,) Source code in npc_engine\\services\\similarity\\similarity_base.py 77 78 79 80 81 82 83 84 85 86 87 88 89 @abstractmethod def metric ( self , embedding_a : np . ndarray , embedding_b : np . ndarray ) -> np . ndarray : \"\"\"Compute distance between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Args: embedding_a: Embedding of shape (1 or batch_size, embedding_size) embedding_b: Embedding of shape (1 or batch_size, embedding_size) Returns: Vector of distances (batch_size or 1,) \"\"\" return None","title":"metric()"},{"location":"inference_engine/reference/#npc_engine.services.similarity.similarity_transformers","text":"Module that implements Huggingface transformers semantic similarity.","title":"similarity_transformers"},{"location":"inference_engine/reference/#npc_engine.services.similarity.similarity_transformers.TransformerSemanticSimilarity","text":"Bases: SimilarityAPI Huggingface transformers semantic similarity. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with biencoder architecture. Also requires a tokenizer.json with huggingface tokenizers definition. model.onnx spec: - inputs: `input_ids` of shape `(batch_size, sequence)` `attention_mask` of shape `(batch_size, sequence)` (Optional) `input_type_ids` of shape `(batch_size, sequence)` - outputs: `token_embeddings` of shape `(batch_size, sequence, hidden_size)` Source code in npc_engine\\services\\similarity\\similarity_transformers.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 class TransformerSemanticSimilarity ( SimilarityAPI ): \"\"\"Huggingface transformers semantic similarity. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with biencoder architecture. Also requires a tokenizer.json with huggingface tokenizers definition. model.onnx spec: - inputs: `input_ids` of shape `(batch_size, sequence)` `attention_mask` of shape `(batch_size, sequence)` (Optional) `input_type_ids` of shape `(batch_size, sequence)` - outputs: `token_embeddings` of shape `(batch_size, sequence, hidden_size)` \"\"\" def __init__ ( self , model_path : str , metric : str = \"dot\" , * args , ** kwargs ): \"\"\"Create and load biencoder model for semantic similarity. Args: model_path: A path where model config and weights are metric: distance to compute semantic similarity \"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_ALL self . model = rt . InferenceSession ( os . path . join ( model_path , \"model.onnx\" ), providers = [ rt . get_available_providers ()[ 0 ]], sess_options = sess_options , ) input_names = [ inp . name for inp in self . model . get_inputs ()] self . token_type_support = \"token_type_ids\" in input_names special_tokens_map_path = os . path . join ( model_path , \"special_tokens_map.json\" ) with open ( special_tokens_map_path , \"r\" ) as f : self . special_tokens = json . load ( f ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) self . pad_token_id = self . tokenizer . encode ( self . special_tokens [ \"pad_token\" ]) . ids [ 0 ] self . tokenizer . enable_padding ( direction = \"right\" , pad_id = self . pad_token_id , pad_type_id = 0 , pad_token = self . tokenizer . decode ( [ self . pad_token_id ], skip_special_tokens = False ), length = None , pad_to_multiple_of = None , ) self . tests = {} self . metric_type = metric def compute_embedding ( self , line : str ) -> np . ndarray : \"\"\"Compute sentence embedding. Args: line: Sentence to embed Returns: Embedding of shape (1, embedding_size) \"\"\" ids = ( np . asarray ( self . tokenizer . encode ( line ) . ids ) . reshape ([ 1 , - 1 ]) . astype ( np . int64 ) ) attention_mask = np . ones_like ( ids ) if not self . token_type_support : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask } else : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : np . zeros_like ( ids ), } outp = self . model . run ( None , input_dict ) return self . _mean_pooling ( outp , attention_mask ) def compute_embedding_batch ( self , lines : List [ str ]) -> np . ndarray : \"\"\"Compute line embeddings in batch. Args: lines: List of sentences to embed Returns: Embedding batch of shape (batch_size, embedding_size) \"\"\" tokenized = self . tokenizer . encode_batch ( lines ) ids = np . stack ([ np . asarray ( encoding . ids ) for encoding in tokenized ]) . astype ( np . int64 ) attention_mask = np . stack ( [ np . asarray ( encoding . attention_mask ) for encoding in tokenized ] ) . astype ( np . int64 ) if not self . token_type_support : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask } else : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : np . stack ( [ np . asarray ( encoding . type_ids ) for encoding in tokenized ] ) . astype ( np . int64 ), } outp = self . model . run ( None , input_dict ) return self . _mean_pooling ( outp , attention_mask ) def _mean_pooling ( self , model_output , attention_mask ): token_embeddings = model_output [ 0 ] attention_mask = np . expand_dims ( attention_mask , - 1 ) sum_embeddings = np . sum ( token_embeddings * attention_mask , 1 ) sum_mask = np . clip ( attention_mask . sum ( 1 ), a_min = 1e-9 , a_max = None ) return sum_embeddings / sum_mask def metric ( self , embedding_a : np . ndarray , embedding_b : np . ndarray ) -> np . ndarray : \"\"\"Similarity between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Args: embedding_a: Embedding of shape (1 or batch_size, embedding_size) embedding_b: Embedding of shape (1 or batch_size, embedding_size) Returns: Vector of distances (batch_size or 1,) \"\"\" if self . metric_type == \"dot\" : return - np . dot ( embedding_a , embedding_b . T ) . squeeze ( 0 ) elif self . metric_type == \"cosine\" : return 1 - cdist ( embedding_a , embedding_b , metric = \"cosine\" ) . squeeze ( 0 )","title":"TransformerSemanticSimilarity"},{"location":"inference_engine/reference/#npc_engine.services.similarity.similarity_transformers.TransformerSemanticSimilarity.__init__","text":"Create and load biencoder model for semantic similarity. Parameters: Name Type Description Default model_path str A path where model config and weights are required metric str distance to compute semantic similarity 'dot' Source code in npc_engine\\services\\similarity\\similarity_transformers.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , model_path : str , metric : str = \"dot\" , * args , ** kwargs ): \"\"\"Create and load biencoder model for semantic similarity. Args: model_path: A path where model config and weights are metric: distance to compute semantic similarity \"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_ALL self . model = rt . InferenceSession ( os . path . join ( model_path , \"model.onnx\" ), providers = [ rt . get_available_providers ()[ 0 ]], sess_options = sess_options , ) input_names = [ inp . name for inp in self . model . get_inputs ()] self . token_type_support = \"token_type_ids\" in input_names special_tokens_map_path = os . path . join ( model_path , \"special_tokens_map.json\" ) with open ( special_tokens_map_path , \"r\" ) as f : self . special_tokens = json . load ( f ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) self . pad_token_id = self . tokenizer . encode ( self . special_tokens [ \"pad_token\" ]) . ids [ 0 ] self . tokenizer . enable_padding ( direction = \"right\" , pad_id = self . pad_token_id , pad_type_id = 0 , pad_token = self . tokenizer . decode ( [ self . pad_token_id ], skip_special_tokens = False ), length = None , pad_to_multiple_of = None , ) self . tests = {} self . metric_type = metric","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.services.similarity.similarity_transformers.TransformerSemanticSimilarity.compute_embedding","text":"Compute sentence embedding. Parameters: Name Type Description Default line str Sentence to embed required Returns: Type Description np . ndarray Embedding of shape (1, embedding_size) Source code in npc_engine\\services\\similarity\\similarity_transformers.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def compute_embedding ( self , line : str ) -> np . ndarray : \"\"\"Compute sentence embedding. Args: line: Sentence to embed Returns: Embedding of shape (1, embedding_size) \"\"\" ids = ( np . asarray ( self . tokenizer . encode ( line ) . ids ) . reshape ([ 1 , - 1 ]) . astype ( np . int64 ) ) attention_mask = np . ones_like ( ids ) if not self . token_type_support : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask } else : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : np . zeros_like ( ids ), } outp = self . model . run ( None , input_dict ) return self . _mean_pooling ( outp , attention_mask )","title":"compute_embedding()"},{"location":"inference_engine/reference/#npc_engine.services.similarity.similarity_transformers.TransformerSemanticSimilarity.compute_embedding_batch","text":"Compute line embeddings in batch. Parameters: Name Type Description Default lines List [ str ] List of sentences to embed required Returns: Type Description np . ndarray Embedding batch of shape (batch_size, embedding_size) Source code in npc_engine\\services\\similarity\\similarity_transformers.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def compute_embedding_batch ( self , lines : List [ str ]) -> np . ndarray : \"\"\"Compute line embeddings in batch. Args: lines: List of sentences to embed Returns: Embedding batch of shape (batch_size, embedding_size) \"\"\" tokenized = self . tokenizer . encode_batch ( lines ) ids = np . stack ([ np . asarray ( encoding . ids ) for encoding in tokenized ]) . astype ( np . int64 ) attention_mask = np . stack ( [ np . asarray ( encoding . attention_mask ) for encoding in tokenized ] ) . astype ( np . int64 ) if not self . token_type_support : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask } else : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : np . stack ( [ np . asarray ( encoding . type_ids ) for encoding in tokenized ] ) . astype ( np . int64 ), } outp = self . model . run ( None , input_dict ) return self . _mean_pooling ( outp , attention_mask )","title":"compute_embedding_batch()"},{"location":"inference_engine/reference/#npc_engine.services.similarity.similarity_transformers.TransformerSemanticSimilarity.metric","text":"Similarity between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Parameters: Name Type Description Default embedding_a np . ndarray Embedding of shape (1 or batch_size, embedding_size) required embedding_b np . ndarray Embedding of shape (1 or batch_size, embedding_size) required Returns: Type Description np . ndarray Vector of distances (batch_size or 1,) Source code in npc_engine\\services\\similarity\\similarity_transformers.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def metric ( self , embedding_a : np . ndarray , embedding_b : np . ndarray ) -> np . ndarray : \"\"\"Similarity between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Args: embedding_a: Embedding of shape (1 or batch_size, embedding_size) embedding_b: Embedding of shape (1 or batch_size, embedding_size) Returns: Vector of distances (batch_size or 1,) \"\"\" if self . metric_type == \"dot\" : return - np . dot ( embedding_a , embedding_b . T ) . squeeze ( 0 ) elif self . metric_type == \"cosine\" : return 1 - cdist ( embedding_a , embedding_b , metric = \"cosine\" ) . squeeze ( 0 )","title":"metric()"},{"location":"inference_engine/reference/#npc_engine.services.stt","text":"Speech to text API. This module implements specific models and wraps them under the common interface for loading and inference. Example from npc_engine.models.stt import SpeechToTextAPI model = SpeechToTextAPI.load(\"path/to/model_dir\") text = model.listen() # Say something","title":"stt"},{"location":"inference_engine/reference/#npc_engine.services.stt.nemo_stt","text":"Module that implements Huggingface transformers semantic similarity.","title":"nemo_stt"},{"location":"inference_engine/reference/#npc_engine.services.stt.nemo_stt.NemoSTT","text":"Bases: SpeechToTextAPI Text to speech pipeline based on Nemo toolkit. Uses ONNX export of EncDecCTCModel from Nemo toolkit. Punctuation distillbert model from Nemo toolkit. (requires tokenizer.json as well) Huggingface transformers model for predicting that sentence is finished (Cropped sentence -> 0 label, finished sentence -> 1 label). OpenSLR Librispeech 3-gram model converted to lowercase https://www.openslr.org/11/ References https://github.com/NVIDIA/NeMo https://catalog.ngc.nvidia.com/orgs/nvidia/models/quartznet15x5 https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/punctuation_and_capitalization.html ctc.onnx spec: - inputs: `audio_signal` mel spectogram of shape `(batch_size, 64, mel_sequence)` - outputs: `tokens` of shape `(batch_size, token_sequence, logits)` punctuation.onnx spec: - inputs: `input_ids` mel spectogram of shape `(batch_size, sequence)` `attention_mask` mel spectogram of shape `(batch_size, sequence)` - outputs: `punctuation` of shape `(batch_size, sequence, 4)` `capitalization` of shape `(batch_size, sequence, 2)` Source code in npc_engine\\services\\stt\\nemo_stt.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 class NemoSTT ( SpeechToTextAPI ): \"\"\"Text to speech pipeline based on Nemo toolkit. Uses: - ONNX export of EncDecCTCModel from Nemo toolkit. - Punctuation distillbert model from Nemo toolkit. (requires tokenizer.json as well) - Huggingface transformers model for predicting that sentence is finished (Cropped sentence -> 0 label, finished sentence -> 1 label). - OpenSLR Librispeech 3-gram model converted to lowercase https://www.openslr.org/11/ References: https://github.com/NVIDIA/NeMo https://catalog.ngc.nvidia.com/orgs/nvidia/models/quartznet15x5 https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/punctuation_and_capitalization.html ctc.onnx spec: - inputs: `audio_signal` mel spectogram of shape `(batch_size, 64, mel_sequence)` - outputs: `tokens` of shape `(batch_size, token_sequence, logits)` punctuation.onnx spec: - inputs: `input_ids` mel spectogram of shape `(batch_size, sequence)` `attention_mask` mel spectogram of shape `(batch_size, sequence)` - outputs: `punctuation` of shape `(batch_size, sequence, 4)` `capitalization` of shape `(batch_size, sequence, 2)` \"\"\" def __init__ ( self , model_path : str , frame_size : int = 1000 , sample_rate : int = 16000 , predict_punctuation : bool = False , * args , ** kwargs , ): \"\"\"Create and load biencoder model for semantic similarity. Args: model_path: A path where model config and weights are stored. frame_size: Size of the audio frame in milliseconds. sample_rate: Sample rate of the audio. predict_punctuation: Whether to predict punctuation and capitalization. \"\"\" super () . __init__ ( sample_rate = sample_rate , model_path = model_path , frame_size = frame_size , * args , ** kwargs , ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_BASIC provider = rt . get_available_providers ()[ 0 ] logger . info ( f \"STT uses { provider } provider\" ) self . stft_filterbanks = librosa . filters . mel ( 16000 , 512 , n_mels = 64 , fmin = 0 , fmax = 8000 ) self . stft_window = librosa . filters . get_window ( \"hann\" , 320 , fftbins = False ) self . mel_mean , self . mel_std = self . _fixed_normalization () self . asr_model = rt . InferenceSession ( path . join ( model_path , \"ctc.onnx\" ), providers = [ provider ], sess_options = sess_options , ) self . predict_punctuation = predict_punctuation if self . predict_punctuation : self . punctuation = rt . InferenceSession ( path . join ( model_path , \"punctuation.onnx\" ), providers = [ provider ], sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( path . join ( model_path , \"tokenizer.json\" ) ) self . asr_vocab = \" abcdefghijklmnopqrstuvwxyz'\" self . asr_vocab = list ( self . asr_vocab ) self . asr_vocab . append ( \"\" ) self . punct_labels = \"O,.?\" self . capit_labels = \"OU\" self . decoder = build_ctcdecoder ( self . asr_vocab ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_ALL self . sentence_model = rt . InferenceSession ( path . join ( model_path , \"sentence_prediction.onnx\" ), providers = [ rt . get_available_providers ()[ 0 ]], sess_options = sess_options , ) self . sentence_tokenizer = Tokenizer . from_file ( path . join ( model_path , \"sentence_tokenizer.json\" ) ) logger . info ( f \"Sentence classifier uses { rt . get_available_providers ()[ 0 ] } provider\" ) def transcribe ( self , audio : List [ float ]) -> np . ndarray : \"\"\"Transcribe audio usign this pipeline. Args: audio: ndarray of int16 of shape (samples,). Returns: Transcribed text from the audio. \"\"\" logits = self . _predict ( np . asarray ( audio , dtype = np . float32 )) return logits def decode ( self , logits : np . ndarray ) -> str : \"\"\"Decode logits into text. Args: logits: ndarray of float32 of shape (timesteps, vocab_size). Returns: Decoded string. \"\"\" return self . decoder . decode ( logits ) def decide_finished ( self , context : str , text : str ) -> bool : \"\"\"Decide if audio transcription should be finished. Args: context: Text context of the speech recognized (e.g. a question to which speech recognized is a reply to). text: Recognized speech so far Returns: Decision to stop recognition and finalize results. \"\"\" decision = self . _decide_sentence_finished ( context , text ) done = bool ( decision ) return done def postprocess ( self , text : str ) -> str : \"\"\"Add punctuation and capitalization. Args: text: audio transcription. Returns: Postprocessed text transcribtion. \"\"\" if self . predict_punctuation : enc = self . tokenizer . encode ( text ) punct , capit = self . punctuation . run ( None , { \"input_ids\" : np . asarray ( enc . ids , np . int64 ) . reshape ([ 1 , - 1 ]), \"attention_mask\" : np . asarray ( enc . attention_mask , np . int64 ) . reshape ( [ 1 , - 1 ] ), }, ) punct = punct . argmax ( 2 ) capit = capit . argmax ( 2 ) punctuated_capitalized = self . _apply_punct_capit_predictions ( text , punct [ 0 ][ 1 : - 1 ] . tolist (), capit [ 0 ][ 1 : - 1 ] . tolist () ) return punctuated_capitalized else : return text def _decide_sentence_finished ( self , context , text ): tokenized = self . sentence_tokenizer . encode ( context , text ) ids = np . asarray ( tokenized . ids ) . reshape ([ 1 , - 1 ]) . astype ( np . int64 ) type_ids = np . asarray ( tokenized . type_ids ) . reshape ([ 1 , - 1 ]) . astype ( np . int64 ) attention_mask = np . ones_like ( ids ) input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : type_ids , } logits = self . sentence_model . run ( None , input_dict )[ 0 ] return logits . argmax ( - 1 )[ 0 ] def _predict ( self , audio : np . ndarray ) -> np . ndarray : signal = audio . reshape ([ 1 , - 1 ]) audio_signal = self . _preprocess_signal ( signal ) . astype ( np . float32 ) return self . asr_model . run ( None , { \"audio_signal\" : audio_signal })[ 0 ][ 0 ] def _preprocess_signal ( self , signal ): audio_signal = signal . reshape ([ 1 , - 1 ]) # audio_signal += np.random.rand(*audio_signal.shape) * 1e-5 audio_signal = np . concatenate ( ( audio_signal [:, 0 ] . reshape ([ - 1 , 1 ]), audio_signal [:, 1 :] - 0.97 * audio_signal [:, : - 1 ], ), axis = 1 , ) audio_signal = audio_signal . reshape ([ - 1 ]) spectogram = librosa . stft ( audio_signal , n_fft = 512 , hop_length = 160 , win_length = 320 , window = self . stft_window , center = True , ) spectogram = np . stack ([ spectogram . real , spectogram . imag ], - 1 ) spectogram = np . sqrt (( spectogram ** 2 ) . sum ( - 1 )) spectogram = spectogram ** 2 spectogram = np . dot ( self . stft_filterbanks , spectogram ) spectogram = np . expand_dims ( spectogram , 0 ) spectogram = np . log ( spectogram + ( 2 ** - 24 )) spectogram = spectogram - np . asarray ( self . mel_mean ) . reshape ([ 1 , 64 , 1 ]) spectogram = spectogram / np . asarray ( self . mel_std ) . reshape ([ 1 , 64 , 1 ]) return spectogram def _apply_punct_capit_predictions ( self , query : str , punct_preds , capit_preds ) -> str : \"\"\"Restores punctuation and capitalization in ``query``. Args: query: a string without punctuation and capitalization punct_preds: ids of predicted punctuation labels capit_preds: ids of predicted capitalization labels Returns: a query with restored punctuation and capitalization \"\"\" query = query . strip () . split () query_with_punct_and_capit = \"\" skip = 0 for j , word in enumerate ( query ): word_enc = self . tokenizer . encode ( word , add_special_tokens = False ) . ids skip += len ( word_enc ) - 1 punct_label = self . punct_labels [ punct_preds [ j + skip ]] capit_label = self . capit_labels [ capit_preds [ j + skip ]] if capit_label != \"O\" : word = word . capitalize () query_with_punct_and_capit += word if punct_label != \"O\" : query_with_punct_and_capit += punct_label query_with_punct_and_capit += \" \" return query_with_punct_and_capit [: - 1 ] def _fixed_normalization ( self ): \"\"\"From https://github.com/NVIDIA/NeMo/blob/stable/tutorials/asr/Online_ASR_Microphone_Demo.ipynb . For whatever reason they use these values to normalize the mel spectogram. \"\"\" mel_fixed_mean = [ - 14.95827016 , - 12.71798736 , - 11.76067913 , - 10.83311182 , - 10.6746914 , - 10.15163465 , - 10.05378331 , - 9.53918999 , - 9.41858904 , - 9.23382904 , - 9.46470918 , - 9.56037 , - 9.57434245 , - 9.47498732 , - 9.7635205 , - 10.08113074 , - 10.05454561 , - 9.81112681 , - 9.68673603 , - 9.83652977 , - 9.90046248 , - 9.85404766 , - 9.92560366 , - 9.95440354 , - 10.17162966 , - 9.90102482 , - 9.47471025 , - 9.54416855 , - 10.07109475 , - 9.98249912 , - 9.74359465 , - 9.55632283 , - 9.23399915 , - 9.36487649 , - 9.81791084 , - 9.56799225 , - 9.70630899 , - 9.85148006 , - 9.8594418 , - 10.01378735 , - 9.98505315 , - 9.62016094 , - 10.342285 , - 10.41070709 , - 10.10687659 , - 10.14536695 , - 10.30828702 , - 10.23542833 , - 10.88546868 , - 11.31723646 , - 11.46087382 , - 11.54877829 , - 11.62400934 , - 11.92190509 , - 12.14063815 , - 11.65130117 , - 11.58308531 , - 12.22214663 , - 12.42927197 , - 12.58039805 , - 13.10098969 , - 13.14345864 , - 13.31835645 , - 14.47345634 , ] mel_fixed_std = [ 3.81402054 , 4.12647781 , 4.05007065 , 3.87790987 , 3.74721178 , 3.68377423 , 3.69344 , 3.54001005 , 3.59530412 , 3.63752368 , 3.62826417 , 3.56488469 , 3.53740577 , 3.68313898 , 3.67138151 , 3.55707266 , 3.54919572 , 3.55721289 , 3.56723346 , 3.46029304 , 3.44119672 , 3.49030548 , 3.39328435 , 3.28244406 , 3.28001423 , 3.26744937 , 3.46692348 , 3.35378948 , 2.96330901 , 2.97663111 , 3.04575148 , 2.89717604 , 2.95659301 , 2.90181116 , 2.7111687 , 2.93041291 , 2.86647897 , 2.73473181 , 2.71495654 , 2.75543763 , 2.79174615 , 2.96076456 , 2.57376336 , 2.68789782 , 2.90930817 , 2.90412004 , 2.76187531 , 2.89905006 , 2.65896173 , 2.81032176 , 2.87769857 , 2.84665271 , 2.80863137 , 2.80707634 , 2.83752184 , 3.01914511 , 2.92046439 , 2.78461139 , 2.90034605 , 2.94599508 , 2.99099718 , 3.0167554 , 3.04649716 , 2.94116777 , ] return mel_fixed_mean , mel_fixed_std","title":"NemoSTT"},{"location":"inference_engine/reference/#npc_engine.services.stt.nemo_stt.NemoSTT.__init__","text":"Create and load biencoder model for semantic similarity. Parameters: Name Type Description Default model_path str A path where model config and weights are stored. required frame_size int Size of the audio frame in milliseconds. 1000 sample_rate int Sample rate of the audio. 16000 predict_punctuation bool Whether to predict punctuation and capitalization. False Source code in npc_engine\\services\\stt\\nemo_stt.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def __init__ ( self , model_path : str , frame_size : int = 1000 , sample_rate : int = 16000 , predict_punctuation : bool = False , * args , ** kwargs , ): \"\"\"Create and load biencoder model for semantic similarity. Args: model_path: A path where model config and weights are stored. frame_size: Size of the audio frame in milliseconds. sample_rate: Sample rate of the audio. predict_punctuation: Whether to predict punctuation and capitalization. \"\"\" super () . __init__ ( sample_rate = sample_rate , model_path = model_path , frame_size = frame_size , * args , ** kwargs , ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_BASIC provider = rt . get_available_providers ()[ 0 ] logger . info ( f \"STT uses { provider } provider\" ) self . stft_filterbanks = librosa . filters . mel ( 16000 , 512 , n_mels = 64 , fmin = 0 , fmax = 8000 ) self . stft_window = librosa . filters . get_window ( \"hann\" , 320 , fftbins = False ) self . mel_mean , self . mel_std = self . _fixed_normalization () self . asr_model = rt . InferenceSession ( path . join ( model_path , \"ctc.onnx\" ), providers = [ provider ], sess_options = sess_options , ) self . predict_punctuation = predict_punctuation if self . predict_punctuation : self . punctuation = rt . InferenceSession ( path . join ( model_path , \"punctuation.onnx\" ), providers = [ provider ], sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( path . join ( model_path , \"tokenizer.json\" ) ) self . asr_vocab = \" abcdefghijklmnopqrstuvwxyz'\" self . asr_vocab = list ( self . asr_vocab ) self . asr_vocab . append ( \"\" ) self . punct_labels = \"O,.?\" self . capit_labels = \"OU\" self . decoder = build_ctcdecoder ( self . asr_vocab ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_ALL self . sentence_model = rt . InferenceSession ( path . join ( model_path , \"sentence_prediction.onnx\" ), providers = [ rt . get_available_providers ()[ 0 ]], sess_options = sess_options , ) self . sentence_tokenizer = Tokenizer . from_file ( path . join ( model_path , \"sentence_tokenizer.json\" ) ) logger . info ( f \"Sentence classifier uses { rt . get_available_providers ()[ 0 ] } provider\" )","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.services.stt.nemo_stt.NemoSTT._apply_punct_capit_predictions","text":"Restores punctuation and capitalization in query . Parameters: Name Type Description Default query str a string without punctuation and capitalization required punct_preds ids of predicted punctuation labels required capit_preds ids of predicted capitalization labels required Returns: Type Description str a query with restored punctuation and capitalization Source code in npc_engine\\services\\stt\\nemo_stt.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 def _apply_punct_capit_predictions ( self , query : str , punct_preds , capit_preds ) -> str : \"\"\"Restores punctuation and capitalization in ``query``. Args: query: a string without punctuation and capitalization punct_preds: ids of predicted punctuation labels capit_preds: ids of predicted capitalization labels Returns: a query with restored punctuation and capitalization \"\"\" query = query . strip () . split () query_with_punct_and_capit = \"\" skip = 0 for j , word in enumerate ( query ): word_enc = self . tokenizer . encode ( word , add_special_tokens = False ) . ids skip += len ( word_enc ) - 1 punct_label = self . punct_labels [ punct_preds [ j + skip ]] capit_label = self . capit_labels [ capit_preds [ j + skip ]] if capit_label != \"O\" : word = word . capitalize () query_with_punct_and_capit += word if punct_label != \"O\" : query_with_punct_and_capit += punct_label query_with_punct_and_capit += \" \" return query_with_punct_and_capit [: - 1 ]","title":"_apply_punct_capit_predictions()"},{"location":"inference_engine/reference/#npc_engine.services.stt.nemo_stt.NemoSTT._fixed_normalization","text":"From https://github.com/NVIDIA/NeMo/blob/stable/tutorials/asr/Online_ASR_Microphone_Demo.ipynb . For whatever reason they use these values to normalize the mel spectogram. Source code in npc_engine\\services\\stt\\nemo_stt.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 def _fixed_normalization ( self ): \"\"\"From https://github.com/NVIDIA/NeMo/blob/stable/tutorials/asr/Online_ASR_Microphone_Demo.ipynb . For whatever reason they use these values to normalize the mel spectogram. \"\"\" mel_fixed_mean = [ - 14.95827016 , - 12.71798736 , - 11.76067913 , - 10.83311182 , - 10.6746914 , - 10.15163465 , - 10.05378331 , - 9.53918999 , - 9.41858904 , - 9.23382904 , - 9.46470918 , - 9.56037 , - 9.57434245 , - 9.47498732 , - 9.7635205 , - 10.08113074 , - 10.05454561 , - 9.81112681 , - 9.68673603 , - 9.83652977 , - 9.90046248 , - 9.85404766 , - 9.92560366 , - 9.95440354 , - 10.17162966 , - 9.90102482 , - 9.47471025 , - 9.54416855 , - 10.07109475 , - 9.98249912 , - 9.74359465 , - 9.55632283 , - 9.23399915 , - 9.36487649 , - 9.81791084 , - 9.56799225 , - 9.70630899 , - 9.85148006 , - 9.8594418 , - 10.01378735 , - 9.98505315 , - 9.62016094 , - 10.342285 , - 10.41070709 , - 10.10687659 , - 10.14536695 , - 10.30828702 , - 10.23542833 , - 10.88546868 , - 11.31723646 , - 11.46087382 , - 11.54877829 , - 11.62400934 , - 11.92190509 , - 12.14063815 , - 11.65130117 , - 11.58308531 , - 12.22214663 , - 12.42927197 , - 12.58039805 , - 13.10098969 , - 13.14345864 , - 13.31835645 , - 14.47345634 , ] mel_fixed_std = [ 3.81402054 , 4.12647781 , 4.05007065 , 3.87790987 , 3.74721178 , 3.68377423 , 3.69344 , 3.54001005 , 3.59530412 , 3.63752368 , 3.62826417 , 3.56488469 , 3.53740577 , 3.68313898 , 3.67138151 , 3.55707266 , 3.54919572 , 3.55721289 , 3.56723346 , 3.46029304 , 3.44119672 , 3.49030548 , 3.39328435 , 3.28244406 , 3.28001423 , 3.26744937 , 3.46692348 , 3.35378948 , 2.96330901 , 2.97663111 , 3.04575148 , 2.89717604 , 2.95659301 , 2.90181116 , 2.7111687 , 2.93041291 , 2.86647897 , 2.73473181 , 2.71495654 , 2.75543763 , 2.79174615 , 2.96076456 , 2.57376336 , 2.68789782 , 2.90930817 , 2.90412004 , 2.76187531 , 2.89905006 , 2.65896173 , 2.81032176 , 2.87769857 , 2.84665271 , 2.80863137 , 2.80707634 , 2.83752184 , 3.01914511 , 2.92046439 , 2.78461139 , 2.90034605 , 2.94599508 , 2.99099718 , 3.0167554 , 3.04649716 , 2.94116777 , ] return mel_fixed_mean , mel_fixed_std","title":"_fixed_normalization()"},{"location":"inference_engine/reference/#npc_engine.services.stt.nemo_stt.NemoSTT.decide_finished","text":"Decide if audio transcription should be finished. Parameters: Name Type Description Default context str Text context of the speech recognized (e.g. a question to which speech recognized is a reply to). required text str Recognized speech so far required Returns: Type Description bool Decision to stop recognition and finalize results. Source code in npc_engine\\services\\stt\\nemo_stt.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def decide_finished ( self , context : str , text : str ) -> bool : \"\"\"Decide if audio transcription should be finished. Args: context: Text context of the speech recognized (e.g. a question to which speech recognized is a reply to). text: Recognized speech so far Returns: Decision to stop recognition and finalize results. \"\"\" decision = self . _decide_sentence_finished ( context , text ) done = bool ( decision ) return done","title":"decide_finished()"},{"location":"inference_engine/reference/#npc_engine.services.stt.nemo_stt.NemoSTT.decode","text":"Decode logits into text. Parameters: Name Type Description Default logits np . ndarray ndarray of float32 of shape (timesteps, vocab_size). required Returns: Type Description str Decoded string. Source code in npc_engine\\services\\stt\\nemo_stt.py 136 137 138 139 140 141 142 143 144 145 def decode ( self , logits : np . ndarray ) -> str : \"\"\"Decode logits into text. Args: logits: ndarray of float32 of shape (timesteps, vocab_size). Returns: Decoded string. \"\"\" return self . decoder . decode ( logits )","title":"decode()"},{"location":"inference_engine/reference/#npc_engine.services.stt.nemo_stt.NemoSTT.postprocess","text":"Add punctuation and capitalization. Parameters: Name Type Description Default text str audio transcription. required Returns: Type Description str Postprocessed text transcribtion. Source code in npc_engine\\services\\stt\\nemo_stt.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def postprocess ( self , text : str ) -> str : \"\"\"Add punctuation and capitalization. Args: text: audio transcription. Returns: Postprocessed text transcribtion. \"\"\" if self . predict_punctuation : enc = self . tokenizer . encode ( text ) punct , capit = self . punctuation . run ( None , { \"input_ids\" : np . asarray ( enc . ids , np . int64 ) . reshape ([ 1 , - 1 ]), \"attention_mask\" : np . asarray ( enc . attention_mask , np . int64 ) . reshape ( [ 1 , - 1 ] ), }, ) punct = punct . argmax ( 2 ) capit = capit . argmax ( 2 ) punctuated_capitalized = self . _apply_punct_capit_predictions ( text , punct [ 0 ][ 1 : - 1 ] . tolist (), capit [ 0 ][ 1 : - 1 ] . tolist () ) return punctuated_capitalized else : return text","title":"postprocess()"},{"location":"inference_engine/reference/#npc_engine.services.stt.nemo_stt.NemoSTT.transcribe","text":"Transcribe audio usign this pipeline. Parameters: Name Type Description Default audio List [ float ] ndarray of int16 of shape (samples,). required Returns: Type Description np . ndarray Transcribed text from the audio. Source code in npc_engine\\services\\stt\\nemo_stt.py 124 125 126 127 128 129 130 131 132 133 134 def transcribe ( self , audio : List [ float ]) -> np . ndarray : \"\"\"Transcribe audio usign this pipeline. Args: audio: ndarray of int16 of shape (samples,). Returns: Transcribed text from the audio. \"\"\" logits = self . _predict ( np . asarray ( audio , dtype = np . float32 )) return logits","title":"transcribe()"},{"location":"inference_engine/reference/#npc_engine.services.stt.stt_base","text":"Module that implements speech to text model API.","title":"stt_base"},{"location":"inference_engine/reference/#npc_engine.services.stt.stt_base.SpeechToTextAPI","text":"Bases: BaseService Abstract base class for speech to text models. Source code in npc_engine\\services\\stt\\stt_base.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 class SpeechToTextAPI ( BaseService ): \"\"\"Abstract base class for speech to text models.\"\"\" API_METHODS : List [ str ] = [ \"listen\" , \"stt\" , \"get_devices\" , \"select_device\" , \"initialize_microphone_input\" , ] def __init__ ( self , min_speech_duration = 100 , max_silence_duration = 1000 , vad_mode = None , sample_rate = 16000 , vad_frame_ms = 10 , pad_size = 1000 , * args , ** kwargs , ): \"\"\"Initialize VAD part of the API.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True sd . default . samplerate = sample_rate self . max_silence_duration = max_silence_duration self . min_speech_duration = min_speech_duration self . vad = webrtcvad . Vad () if vad_mode is not None : self . vad . set_mode ( vad_mode ) self . sample_rate = sample_rate self . listen_queue = Queue ( 10 ) self . vad_frame_ms = vad_frame_ms self . vad_frame_size = int (( vad_frame_ms * sample_rate ) / 1000 ) self . running = False self . microphone_initialized = False self . silence_buffer = np . empty ([ 0 ]) self . pad_size = pad_size @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"SpeechToTextAPI\" def __del__ ( self ): \"\"\"Stop listening on destruction.\"\"\" if self . microphone_initialized : self . stream . stop () def initialize_microphone_input ( self ): \"\"\"Initialize microphone.\"\"\" if self . microphone_initialized : return self . running = False self . microphone_initialized = True def callback ( in_data , frame_count , time_info , status ): if self . running : try : self . listen_queue . put ( in_data . reshape ( - 1 ), block = False ) except Exception : return else : if not self . _vad_frame ( in_data . reshape ( - 1 ) ): # Register only silence for buffer self . silence_buffer = np . append ( self . silence_buffer , in_data . reshape ( - 1 ) ) self . silence_buffer = self . silence_buffer [ - self . pad_size :] self . stream = sd . InputStream ( samplerate = self . sample_rate , channels = 1 , blocksize = self . vad_frame_size , callback = callback , ) self . stream . start () while self . silence_buffer . shape [ 0 ] < self . pad_size : pass def listen ( self , context : str = None ) -> str : # pragma: no cover \"\"\"Listen for speech input and return text from speech when done. Listens for speech, if speech is active for longer than self.frame_size in milliseconds then starts transcribing it. On each voice activity detection (VAD) pause uses context to decide if transcribed text is a finished response to a context. If it is, applies preprocessing and returns the result. If transcribed text is not a response to a context but VAD pause persists through max_silence_duration then returns the results anyway. Requires a microphone input to be initialized. Args: context: A last line of the dialogue used to decide when to stop listening. It allows our STT system to not wait for a VAD timeout (max_silence_duration in ms). Returns: Recognized text from the audio. \"\"\" if not self . microphone_initialized : raise RuntimeError ( \"Microphone not initialized.\" ) self . listen_queue . queue . clear () context = re . sub ( r \"[^A-Za-z0-9 ]+\" , \"\" , context ) . lower () if context else None text = self . _transcribe_vad_pause ( context ) processed = self . postprocess ( text ) self . listen_queue . queue . clear () return processed def _transcribe_vad_pause ( self , context ) -> str : done = False total_pause_ms = 0 total_speech_ms = 0 speech_appeared = False tested_pause = False logits = None signal = self . silence_buffer self . running = True while not done : try : vad_frame = self . listen_queue . get ( block = False ) except Exception : continue is_speech = self . _vad_frame ( vad_frame ) total_speech_ms , total_pause_ms = self . _update_vad_stats ( is_speech , total_speech_ms , total_pause_ms ) if total_speech_ms > self . min_speech_duration : speech_appeared = True if total_pause_ms == 0 : tested_pause = False signal = np . append ( signal , vad_frame ) if not speech_appeared and total_speech_ms < self . min_speech_duration : # Keep only last minimum detectable speech duration + buffer signal = signal [ - self . _ms_to_samplenum ( self . min_speech_duration * 2 ) :] if signal . shape [ 0 ] >= self . _ms_to_samplenum ( self . min_speech_duration ): if speech_appeared and total_pause_ms > self . max_silence_duration : wrapped_signal = self . _wrap_signal ( signal ) logits = self . transcribe ( wrapped_signal ) text = self . decode ( logits ) done = True self . running = False return text elif ( speech_appeared and total_pause_ms > self . min_speech_duration and not tested_pause ): tested_pause = True logits = self . transcribe ( np . pad ( signal , ( 0 , 1000 ), \"wrap\" )) text = self . decode ( logits ) done = self . decide_finished ( context , text ) self . running = not done self . running = False return text def stt ( self , audio : List [ int ]) -> str : \"\"\"Transcribe speech. Args: audio: PMC data with bit depth 16. Returns: Recognized text from the audio. \"\"\" logits = self . transcribe ( audio ) text = self . decode ( logits ) text = self . postprocess ( text ) return text def get_devices ( self ) -> Dict [ int , str ]: # pragma: no cover \"\"\"Get available audio devices.\"\"\" return [ device [ \"name\" ] for device in sd . query_devices ()] def select_device ( self , device_id : int ): # pragma: no cover \"\"\"Get available audio devices.\"\"\" device_id = int ( device_id ) if device_id >= len ( sd . query_devices ()) or device_id < 0 : raise ValueError ( f \"Bad device id, valid device ids in range [0; { len ( sd . query_devices ()) } )\" ) sd . default . device = device_id def _wrap_signal ( self , signal : np . ndarray ) -> np . ndarray : \"\"\"Append silence buffer at both ends of the signal.\"\"\" signal_with_silence = np . append ( np . append ( self . silence_buffer , signal ), self . silence_buffer ) return signal_with_silence def _vad_frame ( self , frame ): # pragma: no cover \"\"\"Detect voice activity in a frame.\"\"\" is_speech = self . vad . is_speech ( np . frombuffer (( frame * 32767 ) . astype ( np . int16 ) . tobytes (), np . int8 ), sample_rate = 16000 , ) return is_speech def _ms_to_samplenum ( self , time ): return int ( time * self . sample_rate / 1000 ) def _samples_to_ms ( self , samples ): return samples * 1000 / self . sample_rate def _update_vad_stats ( self , is_speech , total_speech_ms , total_pause_ms ): \"\"\"Increase total_speech_ms if is_speech is true and appears for min_speech_duration frames consequently.\"\"\" if is_speech : total_speech_ms += self . vad_frame_ms total_pause_ms = 0 else : total_pause_ms += self . vad_frame_ms total_speech_ms = 0 return total_speech_ms , total_pause_ms @abstractmethod def transcribe ( self , audio : np . ndarray ) -> np . ndarray : \"\"\"Abstract method for audio transcription. Should be implemented by the specific model. Args: audio: ndarray of int16 of shape (samples,). Returns: Transcribed logits from the audio. \"\"\" return None @abstractmethod def decode ( self , logits : np . ndarray ) -> str : \"\"\"Decode logits into text. Args: logits: ndarray of float32 of shape (timesteps, vocab_size). Returns: Decoded string. \"\"\" return None @abstractmethod def decide_finished ( self , context : str , text : str ) -> bool : \"\"\"Abstract method for deciding if audio transcription should be finished. Should be implemented by the specific model. Args: context: Text context of the speech recognized (e.g. a question to which speech recognized is a reply to). text: Recognized speech so far. Returns: Decision to stop recognition and finalize results. \"\"\" return None @abstractmethod def postprocess ( self , text : str ) -> str : \"\"\"Abstract method for audio transcription postprocessing. Should be implemented by the specific model. Args: text: audio transcription. Returns: Postprocessed text transcribtion. \"\"\" return None","title":"SpeechToTextAPI"},{"location":"inference_engine/reference/#npc_engine.services.stt.stt_base.SpeechToTextAPI.__del__","text":"Stop listening on destruction. Source code in npc_engine\\services\\stt\\stt_base.py 62 63 64 65 def __del__ ( self ): \"\"\"Stop listening on destruction.\"\"\" if self . microphone_initialized : self . stream . stop ()","title":"__del__()"},{"location":"inference_engine/reference/#npc_engine.services.stt.stt_base.SpeechToTextAPI.__init__","text":"Initialize VAD part of the API. Source code in npc_engine\\services\\stt\\stt_base.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , min_speech_duration = 100 , max_silence_duration = 1000 , vad_mode = None , sample_rate = 16000 , vad_frame_ms = 10 , pad_size = 1000 , * args , ** kwargs , ): \"\"\"Initialize VAD part of the API.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True sd . default . samplerate = sample_rate self . max_silence_duration = max_silence_duration self . min_speech_duration = min_speech_duration self . vad = webrtcvad . Vad () if vad_mode is not None : self . vad . set_mode ( vad_mode ) self . sample_rate = sample_rate self . listen_queue = Queue ( 10 ) self . vad_frame_ms = vad_frame_ms self . vad_frame_size = int (( vad_frame_ms * sample_rate ) / 1000 ) self . running = False self . microphone_initialized = False self . silence_buffer = np . empty ([ 0 ]) self . pad_size = pad_size","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.services.stt.stt_base.SpeechToTextAPI._update_vad_stats","text":"Increase total_speech_ms if is_speech is true and appears for min_speech_duration frames consequently. Source code in npc_engine\\services\\stt\\stt_base.py 231 232 233 234 235 236 237 238 239 def _update_vad_stats ( self , is_speech , total_speech_ms , total_pause_ms ): \"\"\"Increase total_speech_ms if is_speech is true and appears for min_speech_duration frames consequently.\"\"\" if is_speech : total_speech_ms += self . vad_frame_ms total_pause_ms = 0 else : total_pause_ms += self . vad_frame_ms total_speech_ms = 0 return total_speech_ms , total_pause_ms","title":"_update_vad_stats()"},{"location":"inference_engine/reference/#npc_engine.services.stt.stt_base.SpeechToTextAPI._vad_frame","text":"Detect voice activity in a frame. Source code in npc_engine\\services\\stt\\stt_base.py 217 218 219 220 221 222 223 def _vad_frame ( self , frame ): # pragma: no cover \"\"\"Detect voice activity in a frame.\"\"\" is_speech = self . vad . is_speech ( np . frombuffer (( frame * 32767 ) . astype ( np . int16 ) . tobytes (), np . int8 ), sample_rate = 16000 , ) return is_speech","title":"_vad_frame()"},{"location":"inference_engine/reference/#npc_engine.services.stt.stt_base.SpeechToTextAPI._wrap_signal","text":"Append silence buffer at both ends of the signal. Source code in npc_engine\\services\\stt\\stt_base.py 210 211 212 213 214 215 def _wrap_signal ( self , signal : np . ndarray ) -> np . ndarray : \"\"\"Append silence buffer at both ends of the signal.\"\"\" signal_with_silence = np . append ( np . append ( self . silence_buffer , signal ), self . silence_buffer ) return signal_with_silence","title":"_wrap_signal()"},{"location":"inference_engine/reference/#npc_engine.services.stt.stt_base.SpeechToTextAPI.decide_finished","text":"Abstract method for deciding if audio transcription should be finished. Should be implemented by the specific model. Parameters: Name Type Description Default context str Text context of the speech recognized (e.g. a question to which speech recognized is a reply to). required text str Recognized speech so far. required Returns: Type Description bool Decision to stop recognition and finalize results. Source code in npc_engine\\services\\stt\\stt_base.py 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 @abstractmethod def decide_finished ( self , context : str , text : str ) -> bool : \"\"\"Abstract method for deciding if audio transcription should be finished. Should be implemented by the specific model. Args: context: Text context of the speech recognized (e.g. a question to which speech recognized is a reply to). text: Recognized speech so far. Returns: Decision to stop recognition and finalize results. \"\"\" return None","title":"decide_finished()"},{"location":"inference_engine/reference/#npc_engine.services.stt.stt_base.SpeechToTextAPI.decode","text":"Decode logits into text. Parameters: Name Type Description Default logits np . ndarray ndarray of float32 of shape (timesteps, vocab_size). required Returns: Type Description str Decoded string. Source code in npc_engine\\services\\stt\\stt_base.py 255 256 257 258 259 260 261 262 263 264 265 @abstractmethod def decode ( self , logits : np . ndarray ) -> str : \"\"\"Decode logits into text. Args: logits: ndarray of float32 of shape (timesteps, vocab_size). Returns: Decoded string. \"\"\" return None","title":"decode()"},{"location":"inference_engine/reference/#npc_engine.services.stt.stt_base.SpeechToTextAPI.get_api_name","text":"Get the API name. Source code in npc_engine\\services\\stt\\stt_base.py 57 58 59 60 @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"SpeechToTextAPI\"","title":"get_api_name()"},{"location":"inference_engine/reference/#npc_engine.services.stt.stt_base.SpeechToTextAPI.get_devices","text":"Get available audio devices. Source code in npc_engine\\services\\stt\\stt_base.py 197 198 199 def get_devices ( self ) -> Dict [ int , str ]: # pragma: no cover \"\"\"Get available audio devices.\"\"\" return [ device [ \"name\" ] for device in sd . query_devices ()]","title":"get_devices()"},{"location":"inference_engine/reference/#npc_engine.services.stt.stt_base.SpeechToTextAPI.initialize_microphone_input","text":"Initialize microphone. Source code in npc_engine\\services\\stt\\stt_base.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def initialize_microphone_input ( self ): \"\"\"Initialize microphone.\"\"\" if self . microphone_initialized : return self . running = False self . microphone_initialized = True def callback ( in_data , frame_count , time_info , status ): if self . running : try : self . listen_queue . put ( in_data . reshape ( - 1 ), block = False ) except Exception : return else : if not self . _vad_frame ( in_data . reshape ( - 1 ) ): # Register only silence for buffer self . silence_buffer = np . append ( self . silence_buffer , in_data . reshape ( - 1 ) ) self . silence_buffer = self . silence_buffer [ - self . pad_size :] self . stream = sd . InputStream ( samplerate = self . sample_rate , channels = 1 , blocksize = self . vad_frame_size , callback = callback , ) self . stream . start () while self . silence_buffer . shape [ 0 ] < self . pad_size : pass","title":"initialize_microphone_input()"},{"location":"inference_engine/reference/#npc_engine.services.stt.stt_base.SpeechToTextAPI.listen","text":"Listen for speech input and return text from speech when done. Listens for speech, if speech is active for longer than self.frame_size in milliseconds then starts transcribing it. On each voice activity detection (VAD) pause uses context to decide if transcribed text is a finished response to a context. If it is, applies preprocessing and returns the result. If transcribed text is not a response to a context but VAD pause persists through max_silence_duration then returns the results anyway. Requires a microphone input to be initialized. Parameters: Name Type Description Default context str A last line of the dialogue used to decide when to stop listening. It allows our STT system to not wait for a VAD timeout (max_silence_duration in ms). None Returns: Type Description str Recognized text from the audio. Source code in npc_engine\\services\\stt\\stt_base.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def listen ( self , context : str = None ) -> str : # pragma: no cover \"\"\"Listen for speech input and return text from speech when done. Listens for speech, if speech is active for longer than self.frame_size in milliseconds then starts transcribing it. On each voice activity detection (VAD) pause uses context to decide if transcribed text is a finished response to a context. If it is, applies preprocessing and returns the result. If transcribed text is not a response to a context but VAD pause persists through max_silence_duration then returns the results anyway. Requires a microphone input to be initialized. Args: context: A last line of the dialogue used to decide when to stop listening. It allows our STT system to not wait for a VAD timeout (max_silence_duration in ms). Returns: Recognized text from the audio. \"\"\" if not self . microphone_initialized : raise RuntimeError ( \"Microphone not initialized.\" ) self . listen_queue . queue . clear () context = re . sub ( r \"[^A-Za-z0-9 ]+\" , \"\" , context ) . lower () if context else None text = self . _transcribe_vad_pause ( context ) processed = self . postprocess ( text ) self . listen_queue . queue . clear () return processed","title":"listen()"},{"location":"inference_engine/reference/#npc_engine.services.stt.stt_base.SpeechToTextAPI.postprocess","text":"Abstract method for audio transcription postprocessing. Should be implemented by the specific model. Parameters: Name Type Description Default text str audio transcription. required Returns: Type Description str Postprocessed text transcribtion. Source code in npc_engine\\services\\stt\\stt_base.py 283 284 285 286 287 288 289 290 291 292 293 294 295 @abstractmethod def postprocess ( self , text : str ) -> str : \"\"\"Abstract method for audio transcription postprocessing. Should be implemented by the specific model. Args: text: audio transcription. Returns: Postprocessed text transcribtion. \"\"\" return None","title":"postprocess()"},{"location":"inference_engine/reference/#npc_engine.services.stt.stt_base.SpeechToTextAPI.select_device","text":"Get available audio devices. Source code in npc_engine\\services\\stt\\stt_base.py 201 202 203 204 205 206 207 208 def select_device ( self , device_id : int ): # pragma: no cover \"\"\"Get available audio devices.\"\"\" device_id = int ( device_id ) if device_id >= len ( sd . query_devices ()) or device_id < 0 : raise ValueError ( f \"Bad device id, valid device ids in range [0; { len ( sd . query_devices ()) } )\" ) sd . default . device = device_id","title":"select_device()"},{"location":"inference_engine/reference/#npc_engine.services.stt.stt_base.SpeechToTextAPI.stt","text":"Transcribe speech. Parameters: Name Type Description Default audio List [ int ] PMC data with bit depth 16. required Returns: Type Description str Recognized text from the audio. Source code in npc_engine\\services\\stt\\stt_base.py 183 184 185 186 187 188 189 190 191 192 193 194 195 def stt ( self , audio : List [ int ]) -> str : \"\"\"Transcribe speech. Args: audio: PMC data with bit depth 16. Returns: Recognized text from the audio. \"\"\" logits = self . transcribe ( audio ) text = self . decode ( logits ) text = self . postprocess ( text ) return text","title":"stt()"},{"location":"inference_engine/reference/#npc_engine.services.stt.stt_base.SpeechToTextAPI.transcribe","text":"Abstract method for audio transcription. Should be implemented by the specific model. Parameters: Name Type Description Default audio np . ndarray ndarray of int16 of shape (samples,). required Returns: Type Description np . ndarray Transcribed logits from the audio. Source code in npc_engine\\services\\stt\\stt_base.py 241 242 243 244 245 246 247 248 249 250 251 252 253 @abstractmethod def transcribe ( self , audio : np . ndarray ) -> np . ndarray : \"\"\"Abstract method for audio transcription. Should be implemented by the specific model. Args: audio: ndarray of int16 of shape (samples,). Returns: Transcribed logits from the audio. \"\"\" return None","title":"transcribe()"},{"location":"inference_engine/reference/#npc_engine.services.tts","text":"Text to speech specific model implementations. This module implements specific models and wraps them under the common interface for loading and inference. Example from npc_engine.models.tts import TextToSpeechAPI model = TextToSpeechAPI.load(\"path/to/model_dir\") model.run(speaker_id=0, text=\"Hello, world!\")","title":"tts"},{"location":"inference_engine/reference/#npc_engine.services.tts.flowtron","text":"Flowtron (https://github.com/NVIDIA/flowtron) text to speech inference implementation.","title":"flowtron"},{"location":"inference_engine/reference/#npc_engine.services.tts.flowtron.FlowtronTTS","text":"Bases: TextToSpeechAPI Implements Flowtron architecture inference. Paper: arXiv:2005.05957 Code: https://github.com/NVIDIA/flowtron Onnx export script can be found in this fork https://github.com/npc-engine/flowtron. This model class requires four ONNX models encoder.onnx , backward_flow.onnx , forward_flow.onnx and vocoder.onnx where first three are layers from Flowtron architecture ( flow corresponding to one direction pass of affine coupling layers) and vocoder.onnx is neural vocoder. For detailed specs refer to https://github.com/npc-engine/flowtron. Source code in npc_engine\\services\\tts\\flowtron.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 class FlowtronTTS ( TextToSpeechAPI ): \"\"\"Implements Flowtron architecture inference. Paper: [arXiv:2005.05957](https://arxiv.org/abs/2005.05957) Code: https://github.com/NVIDIA/flowtron Onnx export script can be found in this fork https://github.com/npc-engine/flowtron. This model class requires four ONNX models `encoder.onnx`, `backward_flow.onnx`, `forward_flow.onnx` and `vocoder.onnx` where first three are layers from Flowtron architecture (`flow` corresponding to one direction pass of affine coupling layers) and `vocoder.onnx` is neural vocoder. For detailed specs refer to https://github.com/npc-engine/flowtron. \"\"\" def __init__ ( self , model_path , max_frames = 400 , gate_threshold = 0.5 , sigma = 0.8 , smoothing_window = 3 , smoothing_weight = 0.5 , * args , ** kwargs ): \"\"\"Create and load Flowtron and vocoder models.\"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = onnxruntime . SessionOptions () sess_options . graph_optimization_level = ( onnxruntime . GraphOptimizationLevel . ORT_ENABLE_ALL ) provider = onnxruntime . get_available_providers ()[ 0 ] logging . info ( \"FlowtronTTS using provider {} \" . format ( provider )) self . max_frames = max_frames self . gate_threshold = gate_threshold self . sigma = sigma self . smoothing_window = smoothing_window self . smoothing_weight = smoothing_weight self . encoder = onnxruntime . InferenceSession ( path . join ( model_path , \"encoder.onnx\" ), providers = [ provider ], sess_options = sess_options , ) self . backward_flow = onnxruntime . InferenceSession ( path . join ( model_path , \"backward_flow.onnx\" ), providers = [ provider ], sess_options = sess_options , ) self . forward_flow = onnxruntime . InferenceSession ( path . join ( model_path , \"forward_flow.onnx\" ), providers = [ provider ], sess_options = sess_options , ) self . vocoder = onnxruntime . InferenceSession ( path . join ( model_path , \"vocoder.onnx\" ), providers = [ provider ], sess_options = sess_options , ) self . speaker_ids = [ str ( i ) for i in range ( 127 )] self . speaker_ids_map = { idx : i for i , idx in enumerate ( self . speaker_ids )} def get_speaker_ids ( self ) -> List [ str ]: \"\"\"Return available ids of different speakers.\"\"\" return self . speaker_ids def run ( self , speaker_id : str , text : str , n_chunks : int ) -> Iterator [ np . ndarray ]: \"\"\"Create a generator for iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. Returns: Generator that yields next chunk of speech in the form of f32 ndarray. \"\"\" text = self . _get_text ( text ) speaker_id = np . asarray ([[ self . speaker_ids_map [ speaker_id ]]], dtype = np . int64 ) enc_outps_ortvalue = onnxruntime . OrtValue . ortvalue_from_shape_and_type ( [ text . shape [ 1 ], 1 , 640 ], np . float32 , \"cpu\" , 0 ) io_binding = self . encoder . io_binding () io_binding . bind_ortvalue_output ( \"text_emb\" , enc_outps_ortvalue ) io_binding . bind_cpu_input ( \"speaker_vecs\" , speaker_id ) io_binding . bind_cpu_input ( \"text\" , text . reshape ([ 1 , - 1 ])) self . encoder . run_with_iobinding ( io_binding ) residual = np . random . normal ( 0 , self . sigma , size = [ self . max_frames , 1 , 80 ] ) . astype ( np . float32 ) residual = self . _run_backward_flow ( residual , enc_outps_ortvalue ) residual = self . _run_forward_flow ( residual , enc_outps_ortvalue , num_split = self . max_frames // n_chunks ) last_audio = None for residual in residual : residual = np . transpose ( residual , axes = ( 1 , 2 , 0 )) audio = self . vocoder . run ( None , { \"mels\" : residual })[ 0 ] # audio = np.where( # (audio > (audio.mean() - audio.std())) # | (audio < (audio.mean() + audio.std())), # audio, # audio.mean(), # ) tmp = audio if last_audio is None : audio = audio [:, 1000 :] if last_audio is not None : cumsum_vec = np . cumsum ( np . concatenate ([ last_audio , audio ], axis = 1 ), axis = 1 ) ma_vec = ( cumsum_vec [:, self . smoothing_window :] - cumsum_vec [:, : - self . smoothing_window ] ) / self . smoothing_window audio = ( 1 - self . smoothing_weight ) * audio [ :, self . smoothing_window : ] + self . smoothing_weight * ma_vec [:, last_audio . shape [ 1 ] :] last_audio = tmp audio = audio . reshape ( - 1 ) # audio = audio / np.abs(audio).max() yield audio def _get_text ( self , text : str ): text = _clean_text ( text , [ \"flowtron_cleaners\" ]) words = re . findall ( r \"\\S*\\{.*?\\}\\S*|\\S+\" , text ) text = \" \" . join ( words ) text_norm = np . asarray ( text_to_sequence ( text ), dtype = np . int64 ) . reshape ([ 1 , - 1 ]) return text_norm def _run_backward_flow ( self , residual , enc_outps_ortvalue ): residual_o , hidden_att , hidden_lstm = self . _init_states ( residual ) hidden_att_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 0 ], \"cpu\" , 0 ) hidden_att_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 1 ], \"cpu\" , 0 ) hidden_lstm_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 0 ], \"cpu\" , 0 ) hidden_lstm_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 1 ], \"cpu\" , 0 ) hidden_att_o_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 0 ], \"cpu\" , 0 ) hidden_att_o_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 1 ], \"cpu\" , 0 ) hidden_lstm_o_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 0 ], \"cpu\" , 0 ) hidden_lstm_o_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 1 ], \"cpu\" , 0 ) residual_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( residual_o , \"cpu\" , 0 ) residual_outp = [ residual_ortvalue ] for i in range ( residual . shape [ 0 ] - 1 , - 1 , - 1 ): io_binding = self . backward_flow . io_binding () io_binding . bind_cpu_input ( \"residual\" , residual [ i ]) io_binding . bind_ortvalue_input ( \"text\" , enc_outps_ortvalue ) io_binding . bind_ortvalue_input ( \"last_output\" , residual_outp [ 0 ]) io_binding . bind_ortvalue_input ( \"hidden_att\" , hidden_att_ortvalue ) io_binding . bind_ortvalue_input ( \"hidden_att_c\" , hidden_att_c_ortvalue ) io_binding . bind_ortvalue_input ( \"hidden_lstm\" , hidden_lstm_ortvalue ) io_binding . bind_ortvalue_input ( \"hidden_lstm_c\" , hidden_lstm_c_ortvalue ) io_binding . bind_output ( \"output\" , \"cpu\" ) io_binding . bind_output ( \"gate\" , \"cpu\" ) io_binding . bind_ortvalue_output ( \"hidden_att_o\" , hidden_att_o_ortvalue ) io_binding . bind_ortvalue_output ( \"hidden_att_o_c\" , hidden_att_o_c_ortvalue ) io_binding . bind_ortvalue_output ( \"hidden_lstm_o\" , hidden_lstm_o_ortvalue ) io_binding . bind_ortvalue_output ( \"hidden_lstm_o_c\" , hidden_lstm_o_c_ortvalue ) self . backward_flow . run_with_iobinding ( io_binding ) outp = io_binding . get_outputs () gates = outp [ 1 ] . numpy () residual_outp = [ outp [ 0 ]] + residual_outp if ( gates > self . gate_threshold ) . any (): break # Switch input and output to use latest output as input ( hidden_att_ortvalue , hidden_att_o_ortvalue ) = ( hidden_att_o_ortvalue , hidden_att_ortvalue , ) ( hidden_att_c_ortvalue , hidden_att_o_c_ortvalue ) = ( hidden_att_o_c_ortvalue , hidden_att_c_ortvalue , ) ( hidden_lstm_ortvalue , hidden_lstm_o_ortvalue ) = ( hidden_lstm_o_ortvalue , hidden_lstm_ortvalue , ) ( hidden_lstm_c_ortvalue , hidden_lstm_o_c_ortvalue ) = ( hidden_lstm_o_c_ortvalue , hidden_lstm_c_ortvalue , ) residual = np . concatenate ( [ residual_ort . numpy () for residual_ort in residual_outp ], axis = 0 ) return residual def _run_forward_flow ( self , residual , enc_outps_ortvalue , num_split ): residual_o , hidden_att , hidden_lstm = self . _init_states ( residual ) hidden_att_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 0 ], \"cpu\" , 0 ) hidden_att_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 1 ], \"cpu\" , 0 ) hidden_lstm_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 0 ], \"cpu\" , 0 ) hidden_lstm_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 1 ], \"cpu\" , 0 ) hidden_att_o_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 0 ], \"cpu\" , 0 ) hidden_att_o_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 1 ], \"cpu\" , 0 ) hidden_lstm_o_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 0 ], \"cpu\" , 0 ) hidden_lstm_o_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 1 ], \"cpu\" , 0 ) residual_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( residual_o , \"cpu\" , 0 ) residual_outp = [ residual_ortvalue ] last_output = residual_ortvalue for i in range ( residual . shape [ 0 ]): io_binding = self . forward_flow . io_binding () io_binding . bind_cpu_input ( \"residual\" , residual [ i ]) io_binding . bind_ortvalue_input ( \"text\" , enc_outps_ortvalue ) io_binding . bind_ortvalue_input ( \"last_output\" , last_output ) io_binding . bind_ortvalue_input ( \"hidden_att\" , hidden_att_ortvalue ) io_binding . bind_ortvalue_input ( \"hidden_att_c\" , hidden_att_c_ortvalue ) io_binding . bind_ortvalue_input ( \"hidden_lstm\" , hidden_lstm_ortvalue ) io_binding . bind_ortvalue_input ( \"hidden_lstm_c\" , hidden_lstm_c_ortvalue ) io_binding . bind_output ( \"output\" , \"cpu\" ) io_binding . bind_output ( \"gate\" , \"cpu\" ) io_binding . bind_ortvalue_output ( \"hidden_att_o\" , hidden_att_o_ortvalue ) io_binding . bind_ortvalue_output ( \"hidden_att_o_c\" , hidden_att_o_c_ortvalue ) io_binding . bind_ortvalue_output ( \"hidden_lstm_o\" , hidden_lstm_o_ortvalue ) io_binding . bind_ortvalue_output ( \"hidden_lstm_o_c\" , hidden_lstm_o_c_ortvalue ) self . forward_flow . run_with_iobinding ( io_binding ) outp = io_binding . get_outputs () gates = outp [ 1 ] . numpy () residual_outp . append ( outp [ 0 ]) last_output = outp [ 0 ] if ( gates > self . gate_threshold ) . any (): break # Switch input and output to use latest output as input ( hidden_att_ortvalue , hidden_att_o_ortvalue ) = ( hidden_att_o_ortvalue , hidden_att_ortvalue , ) ( hidden_att_c_ortvalue , hidden_att_o_c_ortvalue ) = ( hidden_att_o_c_ortvalue , hidden_att_c_ortvalue , ) ( hidden_lstm_ortvalue , hidden_lstm_o_ortvalue ) = ( hidden_lstm_o_ortvalue , hidden_lstm_ortvalue , ) ( hidden_lstm_c_ortvalue , hidden_lstm_o_c_ortvalue ) = ( hidden_lstm_o_c_ortvalue , hidden_lstm_c_ortvalue , ) if len ( residual_outp ) % num_split == 0 and i != 0 : residual_o = np . concatenate ( [ residual_ort . numpy () for residual_ort in residual_outp ], axis = 0 ) yield residual_o residual_outp = [] if len ( residual_outp ) > 0 : residual_o = np . concatenate ( [ residual_ort . numpy () for residual_ort in residual_outp ], axis = 0 ) yield residual_o def _init_states ( self , residual ): last_outputs = np . zeros ( [ 1 , residual . shape [ 1 ], residual . shape [ 2 ]], dtype = np . float32 ) hidden_att = [ np . zeros ([ 1 , 1 , 1024 ], dtype = np . float32 ), np . zeros ([ 1 , 1 , 1024 ], dtype = np . float32 ), ] hidden_lstm = [ np . zeros ([ 2 , 1 , 1024 ], dtype = np . float32 ), np . zeros ([ 2 , 1 , 1024 ], dtype = np . float32 ), ] return last_outputs , hidden_att , hidden_lstm","title":"FlowtronTTS"},{"location":"inference_engine/reference/#npc_engine.services.tts.flowtron.FlowtronTTS.__init__","text":"Create and load Flowtron and vocoder models. Source code in npc_engine\\services\\tts\\flowtron.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def __init__ ( self , model_path , max_frames = 400 , gate_threshold = 0.5 , sigma = 0.8 , smoothing_window = 3 , smoothing_weight = 0.5 , * args , ** kwargs ): \"\"\"Create and load Flowtron and vocoder models.\"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = onnxruntime . SessionOptions () sess_options . graph_optimization_level = ( onnxruntime . GraphOptimizationLevel . ORT_ENABLE_ALL ) provider = onnxruntime . get_available_providers ()[ 0 ] logging . info ( \"FlowtronTTS using provider {} \" . format ( provider )) self . max_frames = max_frames self . gate_threshold = gate_threshold self . sigma = sigma self . smoothing_window = smoothing_window self . smoothing_weight = smoothing_weight self . encoder = onnxruntime . InferenceSession ( path . join ( model_path , \"encoder.onnx\" ), providers = [ provider ], sess_options = sess_options , ) self . backward_flow = onnxruntime . InferenceSession ( path . join ( model_path , \"backward_flow.onnx\" ), providers = [ provider ], sess_options = sess_options , ) self . forward_flow = onnxruntime . InferenceSession ( path . join ( model_path , \"forward_flow.onnx\" ), providers = [ provider ], sess_options = sess_options , ) self . vocoder = onnxruntime . InferenceSession ( path . join ( model_path , \"vocoder.onnx\" ), providers = [ provider ], sess_options = sess_options , ) self . speaker_ids = [ str ( i ) for i in range ( 127 )] self . speaker_ids_map = { idx : i for i , idx in enumerate ( self . speaker_ids )}","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.services.tts.flowtron.FlowtronTTS.get_speaker_ids","text":"Return available ids of different speakers. Source code in npc_engine\\services\\tts\\flowtron.py 81 82 83 def get_speaker_ids ( self ) -> List [ str ]: \"\"\"Return available ids of different speakers.\"\"\" return self . speaker_ids","title":"get_speaker_ids()"},{"location":"inference_engine/reference/#npc_engine.services.tts.flowtron.FlowtronTTS.run","text":"Create a generator for iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Returns: Type Description Iterator [ np . ndarray ] Generator that yields next chunk of speech in the form of f32 ndarray. Source code in npc_engine\\services\\tts\\flowtron.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def run ( self , speaker_id : str , text : str , n_chunks : int ) -> Iterator [ np . ndarray ]: \"\"\"Create a generator for iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. Returns: Generator that yields next chunk of speech in the form of f32 ndarray. \"\"\" text = self . _get_text ( text ) speaker_id = np . asarray ([[ self . speaker_ids_map [ speaker_id ]]], dtype = np . int64 ) enc_outps_ortvalue = onnxruntime . OrtValue . ortvalue_from_shape_and_type ( [ text . shape [ 1 ], 1 , 640 ], np . float32 , \"cpu\" , 0 ) io_binding = self . encoder . io_binding () io_binding . bind_ortvalue_output ( \"text_emb\" , enc_outps_ortvalue ) io_binding . bind_cpu_input ( \"speaker_vecs\" , speaker_id ) io_binding . bind_cpu_input ( \"text\" , text . reshape ([ 1 , - 1 ])) self . encoder . run_with_iobinding ( io_binding ) residual = np . random . normal ( 0 , self . sigma , size = [ self . max_frames , 1 , 80 ] ) . astype ( np . float32 ) residual = self . _run_backward_flow ( residual , enc_outps_ortvalue ) residual = self . _run_forward_flow ( residual , enc_outps_ortvalue , num_split = self . max_frames // n_chunks ) last_audio = None for residual in residual : residual = np . transpose ( residual , axes = ( 1 , 2 , 0 )) audio = self . vocoder . run ( None , { \"mels\" : residual })[ 0 ] # audio = np.where( # (audio > (audio.mean() - audio.std())) # | (audio < (audio.mean() + audio.std())), # audio, # audio.mean(), # ) tmp = audio if last_audio is None : audio = audio [:, 1000 :] if last_audio is not None : cumsum_vec = np . cumsum ( np . concatenate ([ last_audio , audio ], axis = 1 ), axis = 1 ) ma_vec = ( cumsum_vec [:, self . smoothing_window :] - cumsum_vec [:, : - self . smoothing_window ] ) / self . smoothing_window audio = ( 1 - self . smoothing_weight ) * audio [ :, self . smoothing_window : ] + self . smoothing_weight * ma_vec [:, last_audio . shape [ 1 ] :] last_audio = tmp audio = audio . reshape ( - 1 ) # audio = audio / np.abs(audio).max() yield audio","title":"run()"},{"location":"inference_engine/reference/#npc_engine.services.tts.tts_base","text":"Module that implements text to speech model API.","title":"tts_base"},{"location":"inference_engine/reference/#npc_engine.services.tts.tts_base.TextToSpeechAPI","text":"Bases: BaseService Abstract base class for text-to-speech models. Source code in npc_engine\\services\\tts\\tts_base.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 class TextToSpeechAPI ( BaseService ): \"\"\"Abstract base class for text-to-speech models.\"\"\" #: Methods that are going to be exposed as services. API_METHODS : List [ str ] = [ \"tts_start\" , \"tts_get_results\" , \"get_speaker_ids\" ] def __init__ ( self , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" self . generator = None super () . __init__ ( * args , ** kwargs ) self . initialized = True @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"TextToSpeechAPI\" def tts_start ( self , speaker_id : str , text : str , n_chunks : int ) -> None : \"\"\"Initiate iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. \"\"\" sentences = re . split ( r \"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\" , text ) self . generator = self . _chain_run ( speaker_id , sentences , n_chunks ) def _chain_run ( self , speaker_id , sentences , n_chunks ) -> Iterable [ np . ndarray ]: \"\"\"Chain the run method to be used in the generator.\"\"\" for sentence in sentences : sentence = sentence . strip () if sentence != \"\" : yield from self . run ( speaker_id , sentence , n_chunks ) def tts_get_results ( self ) -> Iterable [ np . ndarray ]: \"\"\"Retrieve the next chunk of generated speech. Returns: Next chunk of speech in the form of f32 ndarray. \"\"\" if self . generator is not None : return next ( self . generator ) . tolist () else : raise ValueError ( \"Speech generation was not started. Use tts_start to start it\" ) @abstractmethod def run ( self , speaker_id : str , text : str , n_chunks : int ) -> Iterable [ np . ndarray ]: \"\"\"Create a generator for iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. Returns: Generator that yields next chunk of speech in the form of f32 ndarray. \"\"\" return None @abstractmethod def get_speaker_ids ( self ) -> List [ str ]: \"\"\"Get list of available speaker ids. Returns: The return value. True for success, False otherwise. \"\"\" return None","title":"TextToSpeechAPI"},{"location":"inference_engine/reference/#npc_engine.services.tts.tts_base.TextToSpeechAPI.__init__","text":"Empty initialization method for API to be similar to other model base classes. Source code in npc_engine\\services\\tts\\tts_base.py 16 17 18 19 20 def __init__ ( self , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" self . generator = None super () . __init__ ( * args , ** kwargs ) self . initialized = True","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.services.tts.tts_base.TextToSpeechAPI._chain_run","text":"Chain the run method to be used in the generator. Source code in npc_engine\\services\\tts\\tts_base.py 39 40 41 42 43 44 def _chain_run ( self , speaker_id , sentences , n_chunks ) -> Iterable [ np . ndarray ]: \"\"\"Chain the run method to be used in the generator.\"\"\" for sentence in sentences : sentence = sentence . strip () if sentence != \"\" : yield from self . run ( speaker_id , sentence , n_chunks )","title":"_chain_run()"},{"location":"inference_engine/reference/#npc_engine.services.tts.tts_base.TextToSpeechAPI.get_api_name","text":"Get the API name. Source code in npc_engine\\services\\tts\\tts_base.py 22 23 24 25 @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"TextToSpeechAPI\"","title":"get_api_name()"},{"location":"inference_engine/reference/#npc_engine.services.tts.tts_base.TextToSpeechAPI.get_speaker_ids","text":"Get list of available speaker ids. Returns: Type Description List [ str ] The return value. True for success, False otherwise. Source code in npc_engine\\services\\tts\\tts_base.py 73 74 75 76 77 78 79 80 @abstractmethod def get_speaker_ids ( self ) -> List [ str ]: \"\"\"Get list of available speaker ids. Returns: The return value. True for success, False otherwise. \"\"\" return None","title":"get_speaker_ids()"},{"location":"inference_engine/reference/#npc_engine.services.tts.tts_base.TextToSpeechAPI.run","text":"Create a generator for iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Returns: Type Description Iterable [ np . ndarray ] Generator that yields next chunk of speech in the form of f32 ndarray. Source code in npc_engine\\services\\tts\\tts_base.py 59 60 61 62 63 64 65 66 67 68 69 70 71 @abstractmethod def run ( self , speaker_id : str , text : str , n_chunks : int ) -> Iterable [ np . ndarray ]: \"\"\"Create a generator for iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. Returns: Generator that yields next chunk of speech in the form of f32 ndarray. \"\"\" return None","title":"run()"},{"location":"inference_engine/reference/#npc_engine.services.tts.tts_base.TextToSpeechAPI.tts_get_results","text":"Retrieve the next chunk of generated speech. Returns: Type Description Iterable [ np . ndarray ] Next chunk of speech in the form of f32 ndarray. Source code in npc_engine\\services\\tts\\tts_base.py 46 47 48 49 50 51 52 53 54 55 56 57 def tts_get_results ( self ) -> Iterable [ np . ndarray ]: \"\"\"Retrieve the next chunk of generated speech. Returns: Next chunk of speech in the form of f32 ndarray. \"\"\" if self . generator is not None : return next ( self . generator ) . tolist () else : raise ValueError ( \"Speech generation was not started. Use tts_start to start it\" )","title":"tts_get_results()"},{"location":"inference_engine/reference/#npc_engine.services.tts.tts_base.TextToSpeechAPI.tts_start","text":"Initiate iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Source code in npc_engine\\services\\tts\\tts_base.py 27 28 29 30 31 32 33 34 35 36 37 def tts_start ( self , speaker_id : str , text : str , n_chunks : int ) -> None : \"\"\"Initiate iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. \"\"\" sentences = re . split ( r \"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\" , text ) self . generator = self . _chain_run ( speaker_id , sentences , n_chunks )","title":"tts_start()"},{"location":"inference_engine/reference/#npc_engine.services.utils","text":"Utility methods for model handling.","title":"utils"},{"location":"inference_engine/reference/#npc_engine.services.utils.config","text":"Config related functions.","title":"config"},{"location":"inference_engine/reference/#npc_engine.services.utils.config.get_model_type_name","text":"Get model type name. Parameters: Name Type Description Default models_path str Path to the models folder. required model_id str Model id (dirname). required Returns: Type Description str Model type name. Source code in npc_engine\\services\\utils\\config.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def get_model_type_name ( models_path : str , model_id : str ) -> str : \"\"\"Get model type name. Args: models_path: Path to the models folder. model_id: Model id (dirname). Returns: Model type name. \"\"\" model_path = os . path . join ( models_path , model_id ) config_path = os . path . join ( model_path , \"config.yml\" ) with open ( config_path , \"r\" ) as f : config = yaml . safe_load ( f ) return get_type_from_dict ( config )","title":"get_model_type_name()"},{"location":"inference_engine/reference/#npc_engine.services.utils.config.get_type_from_dict","text":"Get model type from config dict. Parameters: Name Type Description Default config_dict dict Config dict. required Returns: Type Description str Model type. Source code in npc_engine\\services\\utils\\config.py 8 9 10 11 12 13 14 15 16 17 def get_type_from_dict ( config_dict : dict ) -> str : \"\"\"Get model type from config dict. Args: config_dict: Config dict. Returns: Model type. \"\"\" return config_dict . get ( \"model_type\" , config_dict . get ( \"type\" , \"\" ))","title":"get_type_from_dict()"},{"location":"inference_engine/reference/#npc_engine.services.utils.config.validate_hub_model","text":"Validate huggingface hub model by id. Parameters: Name Type Description Default models_path str Path to the models folder. required model_id str Huggingface hub model id. required Source code in npc_engine\\services\\utils\\config.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def validate_hub_model ( models_path : str , model_id : str ) -> bool : \"\"\"Validate huggingface hub model by id. Args: models_path: Path to the models folder. model_id: Huggingface hub model id. \"\"\" tmp_model_path = os . path . join ( models_path , model_id . replace ( \"/\" , \"_\" )) model_correct = True try : try : hf_hub_download ( repo_id = model_id , filename = \"config.yml\" , cache_dir = tmp_model_path , force_filename = \"config.yml\" , ) except HTTPError : return False config_path = os . path . join ( models_path , model_id . replace ( \"/\" , \"_\" ), \"config.yml\" ) with open ( config_path ) as f : config_dict = yaml . load ( f , Loader = yaml . Loader ) if \"model_type\" not in config_dict : model_correct = False except ValueError : model_correct = False if os . path . exists ( config_path ): os . remove ( config_path ) if os . path . exists ( tmp_model_path ): os . rmdir ( tmp_model_path ) return model_correct","title":"validate_hub_model()"},{"location":"inference_engine/reference/#npc_engine.services.utils.config.validate_local_model","text":"Validate local model by id. Parameters: Name Type Description Default models_path str Path to the models folder. required model_id str Model id. required Source code in npc_engine\\services\\utils\\config.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def validate_local_model ( models_path : str , model_id : str ) -> bool : \"\"\"Validate local model by id. Args: models_path: Path to the models folder. model_id: Model id. \"\"\" model_correct = True model_path = os . path . join ( models_path , model_id ) if not os . path . exists ( model_path ): model_correct = False else : try : _ = get_model_type_name ( models_path , model_id ) except FileNotFoundError : model_correct = False return model_correct","title":"validate_local_model()"},{"location":"inference_engine/reference/#npc_engine.services.utils.config.validate_model","text":"Validate model by id. Parameters: Name Type Description Default models_path str Path to the models folder. required model_id str Model id. required Source code in npc_engine\\services\\utils\\config.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def validate_model ( models_path : str , model_id : str ) -> bool : \"\"\"Validate model by id. Args: models_path: Path to the models folder. model_id: Model id. \"\"\" model_correct = True model_path = os . path . join ( models_path , model_id ) if not os . path . exists ( model_path ): model_correct = validate_hub_model ( models_path , model_id ) else : model_correct = validate_local_model ( models_path , model_id ) return model_correct","title":"validate_model()"},{"location":"inference_engine/reference/#npc_engine.services.utils.lru_cache","text":"LRU cache.","title":"lru_cache"},{"location":"inference_engine/reference/#npc_engine.services.utils.lru_cache.NumpyLRUCache","text":"Dict based LRU cache for numpy arrays. Source code in npc_engine\\services\\utils\\lru_cache.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class NumpyLRUCache : \"\"\"Dict based LRU cache for numpy arrays.\"\"\" def __init__ ( self , size ): \"\"\"Crate cache.\"\"\" self . size = size self . lru_cache = collections . OrderedDict () self . common_dim = None def _get ( self , key : Any , default = None ) -> np . ndarray : try : value = self . lru_cache . pop ( key ) self . lru_cache [ key ] = value return value except KeyError : return default def _put ( self , key : Any , value : np . ndarray ): try : self . lru_cache . pop ( key ) except KeyError : if len ( self . lru_cache ) >= self . size : self . lru_cache . popitem ( last = False ) self . lru_cache [ key ] = value def cache_compute ( self , keys : List [ Any ], function : Callable ) -> Tuple [ np . ndarray , List [ Any ]]: \"\"\"Get batch from cache and compute missing. Args: keys: List of keys Returns: np.ndarray or None: Found entries concatenated over 0 axis. list(_) or None: Keys that were not found. \"\"\" if len ( self . lru_cache ) == 0 : result = function ( keys ) self . put_batch ( keys , result ) return result else : result = np . zeros (( len ( keys ), * self . common_dim )) items = [ self . _get ( key ) for key in keys ] not_found = [ key for item , key in zip ( items , keys ) if item is None ] if len ( not_found ) > 0 : computed = function ( not_found ) computed_idx = 0 for idx , item in enumerate ( items ): if item is None : result_slc = tuple ([ idx ] + [ slice ( None )] * len ( self . common_dim )) computed_slc = tuple ( [ computed_idx ] + [ slice ( None )] * len ( self . common_dim ) ) result [ result_slc ] = computed [ computed_slc ] computed_idx += 1 else : result_slc = tuple ([ idx ] + [ slice ( None )] * len ( self . common_dim )) result [ result_slc ] = item return result def put_batch ( self , keys : List [ Any ], values : np . ndarray ): \"\"\"Put batch to cache. Args: keys: List of keys values: Ndarray of shape (len(keys), *common_dim) \"\"\" self . _validate_shape ( values ) for key , item in zip ( keys , values ): self . _put ( key , item ) def _validate_shape ( self , value ): if self . common_dim is None : self . common_dim = value . shape [ 1 :] else : if self . common_dim != value . shape [ 1 :]: raise ValueError ( f \"\"\"Cached arrays must have the same shape. Shape expected: { self . common_dim } Shape found: { value . shape [ 1 :] } \"\"\" )","title":"NumpyLRUCache"},{"location":"inference_engine/reference/#npc_engine.services.utils.lru_cache.NumpyLRUCache.__init__","text":"Crate cache. Source code in npc_engine\\services\\utils\\lru_cache.py 10 11 12 13 14 def __init__ ( self , size ): \"\"\"Crate cache.\"\"\" self . size = size self . lru_cache = collections . OrderedDict () self . common_dim = None","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.services.utils.lru_cache.NumpyLRUCache.cache_compute","text":"Get batch from cache and compute missing. Parameters: Name Type Description Default keys List [ Any ] List of keys required Returns: Type Description np . ndarray np.ndarray or None: Found entries concatenated over 0 axis. List [ Any ] list(_) or None: Keys that were not found. Source code in npc_engine\\services\\utils\\lru_cache.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def cache_compute ( self , keys : List [ Any ], function : Callable ) -> Tuple [ np . ndarray , List [ Any ]]: \"\"\"Get batch from cache and compute missing. Args: keys: List of keys Returns: np.ndarray or None: Found entries concatenated over 0 axis. list(_) or None: Keys that were not found. \"\"\" if len ( self . lru_cache ) == 0 : result = function ( keys ) self . put_batch ( keys , result ) return result else : result = np . zeros (( len ( keys ), * self . common_dim )) items = [ self . _get ( key ) for key in keys ] not_found = [ key for item , key in zip ( items , keys ) if item is None ] if len ( not_found ) > 0 : computed = function ( not_found ) computed_idx = 0 for idx , item in enumerate ( items ): if item is None : result_slc = tuple ([ idx ] + [ slice ( None )] * len ( self . common_dim )) computed_slc = tuple ( [ computed_idx ] + [ slice ( None )] * len ( self . common_dim ) ) result [ result_slc ] = computed [ computed_slc ] computed_idx += 1 else : result_slc = tuple ([ idx ] + [ slice ( None )] * len ( self . common_dim )) result [ result_slc ] = item return result","title":"cache_compute()"},{"location":"inference_engine/reference/#npc_engine.services.utils.lru_cache.NumpyLRUCache.put_batch","text":"Put batch to cache. Parameters: Name Type Description Default keys List [ Any ] List of keys required values np . ndarray Ndarray of shape (len(keys), *common_dim) required Source code in npc_engine\\services\\utils\\lru_cache.py 68 69 70 71 72 73 74 75 76 77 def put_batch ( self , keys : List [ Any ], values : np . ndarray ): \"\"\"Put batch to cache. Args: keys: List of keys values: Ndarray of shape (len(keys), *common_dim) \"\"\" self . _validate_shape ( values ) for key , item in zip ( keys , values ): self . _put ( key , item )","title":"put_batch()"},{"location":"inference_engine/reference/#npc_engine.services.utils.mock","text":"Utility script to mock models for testing.","title":"mock"},{"location":"inference_engine/reference/#npc_engine.services.utils.mock.build_output_shape_tensor_","text":"Build output shape tensor for dynamic shape models. Parameters: Name Type Description Default graph Graph to add the output shape tensor to. required output Model output. required dynamic_shape_map Map of input names to their dynamic shape indices. required shape_name Name of the output shape tensor. 'dynamic_shape' Source code in npc_engine\\services\\utils\\mock.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def build_output_shape_tensor_ ( graph , output , dynamic_shape_map , shape_name = \"dynamic_shape\" , override = {} ): \"\"\"Build output shape tensor for dynamic shape models. Args: graph: Graph to add the output shape tensor to. output: Model output. dynamic_shape_map: Map of input names to their dynamic shape indices. shape_name: Name of the output shape tensor. \"\"\" dimensions_retrieved = [] for i , dim in enumerate ( output . type . tensor_type . shape . dim ): if \" + \" in dim . dim_param : dim1 , dim2 = dim . dim_param . split ( \" + \" ) create_dim_variable_ ( graph , shape_name , dim1 , override . get ( dim1 , dim . dim_value ), i , dynamic_shape_map , postfix = \"1\" , ) create_dim_variable_ ( graph , shape_name , dim2 , override . get ( dim2 , dim . dim_value ), i , dynamic_shape_map , postfix = \"2\" , ) so . add_node ( graph , so . node ( \"Add\" , inputs = [ f \" { shape_name } _ { i } _1\" , f \" { shape_name } _ { i } _2\" ], outputs = [ f \" { shape_name } _ { i } \" ], ), ) else : create_dim_variable_ ( graph , shape_name , dim . dim_param , override . get ( dim . dim_param , dim . dim_value ), i , dynamic_shape_map , ) dimensions_retrieved . append ( f \" { shape_name } _ { i } \" ) node = so . node ( \"Concat\" , inputs = dimensions_retrieved , outputs = [ f \" { shape_name } \" ], axis = 0 , ) so . add_node ( graph , node )","title":"build_output_shape_tensor_()"},{"location":"inference_engine/reference/#npc_engine.services.utils.mock.create_dim_variable_","text":"Create a dimension variable for a dynamic shape model. Parameters: Name Type Description Default graph Graph to add the dimension variable to. required shape_name Name of the output shape tensor. required dim_param Dimension parameter name. required dim_value Dimension value. required dim_id Index of the dimension variable. required dynamic_shape_map Map of dynamic axes names to their inputs indices. required Source code in npc_engine\\services\\utils\\mock.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def create_dim_variable_ ( graph , shape_name , dim_param , dim_value , dim_id , dynamic_shape_map , postfix = None ): \"\"\"Create a dimension variable for a dynamic shape model. Args: graph: Graph to add the dimension variable to. shape_name: Name of the output shape tensor. dim_param: Dimension parameter name. dim_value: Dimension value. dim_id: Index of the dimension variable. dynamic_shape_map: Map of dynamic axes names to their inputs indices. \"\"\" if dim_param != \"\" and dim_param in dynamic_shape_map : node1 = so . node ( \"Shape\" , inputs = [ dynamic_shape_map [ dim_param ][ 0 ]], outputs = [ f \" { shape_name } _ { dim_id } \" if postfix is None else f \" { shape_name } _ { dim_id } _ { postfix } \" ], start = dynamic_shape_map [ dim_param ][ 1 ], end = dynamic_shape_map [ dim_param ][ 1 ] + 1 , ) so . add_node ( graph , node1 ) else : so . add_constant ( graph , f \" { shape_name } _ { dim_id } \" if postfix is None else f \" { shape_name } _ { dim_id } _ { postfix } \" , np . array ([ dim_value if dim_value != 0 else 1 ], dtype = np . int64 ,), data_type = \"INT64\" , )","title":"create_dim_variable_()"},{"location":"inference_engine/reference/#npc_engine.services.utils.mock.create_stub_onnx_model","text":"Create stub onnx model for tests with correct input and output shapes and names. Parameters: Name Type Description Default onnx_model_path str Path to the onnx model. required output_path str Path to the output mock model. required dynamic_shape_values dict Map to specify dynamic dimension values. required Source code in npc_engine\\services\\utils\\mock.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def create_stub_onnx_model ( onnx_model_path : str , output_path : str , dynamic_shape_values : dict ): \"\"\"Create stub onnx model for tests with correct input and output shapes and names. Args: onnx_model_path: Path to the onnx model. output_path: Path to the output mock model. dynamic_shape_values: Map to specify dynamic dimension values. \"\"\" onnx_model = so . graph_from_file ( onnx_model_path ) inputs = onnx_model . input outputs = onnx_model . output mock_graph = so . empty_graph () inverse_data_dict = { value : key for key , value in glob . DATA_TYPES . items ()} dynamic_shape_map = {} for input_ in inputs : so . add_input ( mock_graph , name = input_ . name , dimensions = [ dim . dim_param if dim . dim_param != \"\" else dim . dim_value for dim in input_ . type . tensor_type . shape . dim ], data_type = inverse_data_dict [ input_ . type . tensor_type . elem_type ], ) for i , dim in enumerate ( input_ . type . tensor_type . shape . dim ): if dim . dim_param != \"\" : dynamic_shape_map [ dim . dim_param ] = ( input_ . name , i ) for output in outputs : so . add_output ( mock_graph , name = output . name , dimensions = [ dim . dim_param if dim . dim_param != \"\" else dim . dim_value for dim in output . type . tensor_type . shape . dim ], data_type = inverse_data_dict [ output . type . tensor_type . elem_type ], ) input_names = [ inp . name for inp in inputs ] if output . name not in input_names : build_output_shape_tensor_ ( mock_graph , output , dynamic_shape_map , f \"dynamic_shape_ { output . name } \" , override = dynamic_shape_values , ) node = so . node ( \"ConstantOfShape\" , inputs = [ f \"dynamic_shape_ { output . name } \" ], outputs = [ output . name ], value = xhelp . make_tensor ( name = f \"dynamic_shape_ { output . name } _value\" , data_type = output . type . tensor_type . elem_type , dims = [ 1 ], vals = [ 0 ], ), name = f \"ConstantOfShape_ { output . name } \" , ) so . add_node ( mock_graph , node ) so . graph_to_file ( mock_graph , output_path , onnx_opset_version = 15 )","title":"create_stub_onnx_model()"},{"location":"inference_engine/reference/#npc_engine.services.utils.mock.main","text":"Create stub onnx model for tests with correct input and output shapes and names. Source code in npc_engine\\services\\utils\\mock.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 @click . command () @click . option ( \"-d\" , \"--dynamic-dim\" , type = str , multiple = True , help = \"Specify dynamic dimension in format `<dim name>:<dim value>`.\" , ) @click . option ( \"-m\" , \"--onnx-model-path\" , required = True , type = str ) @click . option ( \"-o\" , \"--output-path\" , required = True , type = str ) def main ( onnx_model_path : str , output_path : str , dynamic_dim : List [ str ]): \"\"\"Create stub onnx model for tests with correct input and output shapes and names.\"\"\" dynamic_dims_map = {} for dim in dynamic_dim : dim_name , dim_value = dim . split ( \":\" ) dynamic_dims_map [ dim_name ] = int ( dim_value ) create_stub_onnx_model ( onnx_model_path , output_path , dynamic_dims_map )","title":"main()"},{"location":"inference_engine/reference/#npc_engine.text","text":"from https://github.com/keithito/tacotron","title":"text"},{"location":"inference_engine/reference/#npc_engine.text.cleaners","text":"adapted from https://github.com/keithito/tacotron. Cleaners are transformations that run over the input text at both training and eval time. Cleaners can be selected by passing a comma-delimited list of cleaner names as the \"cleaners\" hyperparameter. Some cleaners are English-specific. You'll typically want to use: 1. \"english_cleaners\" for English text 2. \"transliteration_cleaners\" for non-English text that can be transliterated to ASCII using the Unidecode library (https://pypi.python.org/pypi/Unidecode) 3. \"basic_cleaners\" if you do not want to transliterate (in this case, you should also update the symbols in symbols.py to match your data).","title":"cleaners"},{"location":"inference_engine/reference/#npc_engine.text.cleaners.flowtron_cleaners","text":"Clean text with a set of cleaners. Source code in npc_engine\\text\\cleaners.py 98 99 100 101 102 103 104 105 def flowtron_cleaners ( text ): \"\"\"Clean text with a set of cleaners.\"\"\" text = collapse_whitespace ( text ) text = remove_hyphens ( text ) text = expand_datestime ( text ) text = expand_numbers ( text ) text = expand_safe_abbreviations ( text ) return text","title":"flowtron_cleaners()"},{"location":"inference_engine/reference/#npc_engine.text.numbers","text":"from https://github.com/keithito/tacotron","title":"numbers"},{"location":"inference_engine/reference/#npc_engine.text.symbols","text":"from https://github.com/keithito/tacotron","title":"symbols"},{"location":"inference_engine/reference/#npc_engine.version","text":"This module contains project version information.","title":"version"},{"location":"inference_engine/running_server/","text":"First lets get the npc-engine . The simplest way to install npc-engine is to use the pip command. pip install npc-engine[dml] If you are running it on linux you should specify cpu extra instead of dml as DirectML works only on Windows. To be able to ship it with your game you will need pyinstaller packaged version from: Releases page Resulting folder after following build instructions If using packaged version you should run cli.exe inside the npc-engine folder instead of npc-engine command. You can check all the possible commands via: npc-engine --help To start the server create models directory: mkdir models and execute cli.exe with run command npc-engine run --models-path models --port 5555 This will start a server but if no models were added to the folder it will expose only conrol API. You can download default models via npc-engine download-default-models --models-path models See descriptions of the default models in Default Models section. NOTE Model API examples can be found in npc-engine\\tests\\integration . Now lets test npc-engine with this example request from python: First start the server on port 5555: Now run the following python script: import zmq context = zmq.Context() # Socket to talk to server print(\"Connecting to npc-engine server\") socket = context.socket(zmq.REQ) socket.RCVTIMEO = 2000 socket.connect(\"tcp://localhost:5555\") request = { \"jsonrpc\": \"2.0\", \"method\": \"start_service\", \"id\": 0, \"params\": [\"SimilarityAPI\"], } socket.send_json(request) message = socket.recv_json() request = { \"jsonrpc\": \"2.0\", \"method\": \"compare\", \"id\": 0, \"params\": [\"I will help you\", [\"I shall provide you my assistance\"]], } socket.send_json(request) message = socket.recv_json() print(f\"Response message {message}\") # You can also provide socket identity to select a specific service socket = context.socket(zmq.REQ) socket.setsockopt(zmq.IDENTITY, b\"control\") socket.RCVTIMEO = 2000 socket.connect(\"tcp://localhost:5555\") request = { \"jsonrpc\": \"2.0\", \"method\": \"get_services_metadata\", \"id\": 0, \"params\": [], } socket.send_json(request) message = socket.recv_json() print(f\"Services metadata {message}\")","title":"Running The Server"}]}