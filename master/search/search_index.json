{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome NPC Engine is a deep learning and natural language processing toolkit aimed at designing game NPC AI using natural language. It uses state of the art deep learning models and allows you to: Generate NPC lines based on the natural language descriptions of a character (e.g. his persona , location). Voice those lines with multiple voices (current model has 127) Trigger discrete actions based on semantic similarity between an action description and a player or NPC line. Create APIs to your own deep learning models. Use npc-engine Unity Unreal Engine 4 Godot Other Extend npc-engine Export model using existing API Add new API","title":"Home"},{"location":"#welcome","text":"NPC Engine is a deep learning and natural language processing toolkit aimed at designing game NPC AI using natural language. It uses state of the art deep learning models and allows you to: Generate NPC lines based on the natural language descriptions of a character (e.g. his persona , location). Voice those lines with multiple voices (current model has 127) Trigger discrete actions based on semantic similarity between an action description and a player or NPC line. Create APIs to your own deep learning models.","title":"Welcome"},{"location":"#use-npc-engine","text":"Unity Unreal Engine 4 Godot Other","title":"Use npc-engine"},{"location":"#extend-npc-engine","text":"Export model using existing API Add new API","title":"Extend npc-engine"},{"location":"not_ready/","text":"This integration is not ready yet But don't get discouraged, it's quite easy to create! Check out the integration checklist . If you plan to create one, please consider contributing! To contribute a new integration: Create an issue to discuss the integration architecture with us. Develop the integration, preferably with demo scene. Share it with permissive open source license. Create a pull request to npc-engine repo with documentation about your integration.","title":"This integration is not ready yet"},{"location":"not_ready/#this-integration-is-not-ready-yet","text":"But don't get discouraged, it's quite easy to create! Check out the integration checklist . If you plan to create one, please consider contributing! To contribute a new integration: Create an issue to discuss the integration architecture with us. Develop the integration, preferably with demo scene. Share it with permissive open source license. Create a pull request to npc-engine repo with documentation about your integration.","title":"This integration is not ready yet"},{"location":"inference_engine/api_classes/","text":"API class is an abstract class that corresponds to a certain task a service should perform (e.g. text-to-speech or chatbot) and defines interface methods for such a task as well as abstract methods for specific servicess to implement. All API classes are children of the BaseService class that handles registering service implementations and running them. Important It also should list the methods that are to be exposed as API via API_METHODS class variable. Important To be discovered correctly api classes must be imported into npc_engine.services module Existing APIs These are the existing API classes and corresponding API_METHODS: npc_engine.services.sequence_classifier.sequence_classifier_base.SequenceClassifierAPI Bases: BaseService Abstract base class for text classification models. __init__ ( cache_size = 0 , * args , ** kwargs ) Empty initialization method for API to be similar to other model base classes. classify ( texts ) Classify a list of texts. Parameters: Name Type Description Default texts List [ str ] A list of texts to classify. required Returns: Type Description List [ List [ float ]] List of scores for each text. compute_scores_batch ( texts ) abstractmethod Compute scores for a list of texts. Parameters: Name Type Description Default texts List [ str ] A list of texts to compute scores for. required Returns: Type Description np . ndarray List of scores for each text. get_api_name () classmethod Get the API name. npc_engine.services.text_generation.text_generation_base.TextGenerationAPI Bases: BaseService Abstract base class for text generation models. __init__ ( template_string = None , context_template = None , history_template = None , * args , ** kwargs ) Initialize prompt formatting variables. Parameters: Name Type Description Default template_string str Template string to be rendered as prompt. None generate_reply ( context , * args , ** kwargs ) Format the model prompt and generate response. Parameters: Name Type Description Default context Dict [ str , Any ] Prompt context. required Returns: Type Description str Text response to a prompt. get_api_name () classmethod Get the API name. get_context_template () Return context template. Returns: Type Description Dict [ str , Any ] Example context get_prompt_template () Return prompt template string used to render model prompt. Returns: Type Description str A template string. get_special_tokens () abstractmethod Return dictionary mapping for special tokens. To be implemented by child class. Can then be used in template string as fields Returns: Type Description Dict [ str , str ] Dictionary of special tokens run ( prompt , temperature = 1 , topk = None ) abstractmethod Abstract method for concrete implementation of generation. Parameters: Name Type Description Default prompt str Fromatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text string_too_long ( prompt ) abstractmethod Check if prompt is too long. Parameters: Name Type Description Default prompt str Prompt to check. required Returns: Type Description bool True if prompt is too long, False otherwise. npc_engine.services.similarity.similarity_base.SimilarityAPI Bases: BaseService Abstract base class for text similarity models. __init__ ( cache_size = 0 , * args , ** kwargs ) Empty initialization method for API to be similar to other model base classes. cache ( context ) Cache embeddings of given sequences. Parameters: Name Type Description Default context List [ str ] A list of sentences to cache. required compare ( query , context ) Compare a query to the context. Parameters: Name Type Description Default query str A sentence to compare. required context List [ str ] A list of sentences to compare to. This will be cached if caching is enabled required Returns: Type Description List [ float ] List of similarities compute_embedding ( line ) abstractmethod Compute sentence embedding. Parameters: Name Type Description Default line str Sentence to embed required Returns: Type Description np . ndarray Embedding of shape (1, embedding_size) compute_embedding_batch ( lines ) abstractmethod Compute line embeddings in batch. Parameters: Name Type Description Default lines List [ str ] List of sentences to embed required Returns: Type Description np . ndarray Embedding batch of shape (batch_size, embedding_size) get_api_name () classmethod Get the API name. metric ( embedding_a , embedding_b ) abstractmethod Compute distance between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Parameters: Name Type Description Default embedding_a np . ndarray Embedding of shape (1 or batch_size, embedding_size) required embedding_b np . ndarray Embedding of shape (1 or batch_size, embedding_size) required Returns: Type Description np . ndarray Vector of distances (batch_size or 1,) npc_engine.services.tts.tts_base.TextToSpeechAPI Bases: BaseService Abstract base class for text-to-speech models. __init__ ( * args , ** kwargs ) Empty initialization method for API to be similar to other model base classes. _chain_run ( speaker_id , sentences , n_chunks ) Chain the run method to be used in the generator. get_api_name () classmethod Get the API name. get_speaker_ids () abstractmethod Get list of available speaker ids. Returns: Type Description List [ str ] The return value. True for success, False otherwise. run ( speaker_id , text , n_chunks ) abstractmethod Create a generator for iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Returns: Type Description Iterable [ np . ndarray ] Generator that yields next chunk of speech in the form of f32 ndarray. tts_get_results () Retrieve the next chunk of generated speech. Returns: Type Description Iterable [ np . ndarray ] Next chunk of speech in the form of f32 ndarray. tts_start ( speaker_id , text , n_chunks ) Initiate iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Creating new APIs You can use this dummy API example to create your own: from npc_engine.services.base_service import BaseService class EchoAPI(BaseService): API_METHODS: List[str] = [\"echo\"] def __init__(self, *args, **kwargs): pass def echo(self, text): return text Dont forget Import new API to npc-engine.services so that it is discovered. Models that are implemented for the API should appear there too. Calling other APIs There are a set of service clients that can be used to call other APIs that are running on the server. They will block other services so they should be used carefully. Module implementing the clients for services. control_client Control interface client implementation. ControlClient Bases: ServiceClient Json rpc client for control requests. Source code in npc_engine\\service_clients\\control_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class ControlClient ( ServiceClient ): \"\"\"Json rpc client for control requests.\"\"\" def __init__ ( self , zmq_context : zmq . Context ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , \"control\" ) def start_service ( self , service_id ): \"\"\"Send a start service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"start_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) def stop_service ( self , service_id ): \"\"\"Send a stop service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"stop_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) def get_service_status ( self , service_id ) -> str : \"\"\"Send a get service status request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_service_status\" , \"id\" : 0 , \"params\" : [ service_id ], } return self . send_request ( request ) def restart_service ( self , service_id ): \"\"\"Send a restart service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"restart_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) def get_services_metadata ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Send a get services metadata request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_services_metadata\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) def get_service_metadata ( self , service_id : str ) -> Dict [ str , Any ]: \"\"\"Send a get services metadata request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_service_metadata\" , \"id\" : 0 , \"params\" : [ service_id ], } return self . send_request ( request ) def check_dependency ( self , service_id , dependency_id ) -> bool : \"\"\"Send a check dependency request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"check_dependency\" , \"id\" : 0 , \"params\" : [ service_id , dependency_id ], } return self . send_request ( request ) @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"control\" __init__ ( zmq_context ) Connect to the server on the port. Source code in npc_engine\\service_clients\\control_client.py 10 11 12 def __init__ ( self , zmq_context : zmq . Context ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , \"control\" ) check_dependency ( service_id , dependency_id ) Send a check dependency request to the server. Source code in npc_engine\\service_clients\\control_client.py 74 75 76 77 78 79 80 81 82 def check_dependency ( self , service_id , dependency_id ) -> bool : \"\"\"Send a check dependency request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"check_dependency\" , \"id\" : 0 , \"params\" : [ service_id , dependency_id ], } return self . send_request ( request ) get_api_name () classmethod Return the name of the API. Source code in npc_engine\\service_clients\\control_client.py 84 85 86 87 @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"control\" get_service_metadata ( service_id ) Send a get services metadata request to the server. Source code in npc_engine\\service_clients\\control_client.py 64 65 66 67 68 69 70 71 72 def get_service_metadata ( self , service_id : str ) -> Dict [ str , Any ]: \"\"\"Send a get services metadata request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_service_metadata\" , \"id\" : 0 , \"params\" : [ service_id ], } return self . send_request ( request ) get_service_status ( service_id ) Send a get service status request to the server. Source code in npc_engine\\service_clients\\control_client.py 34 35 36 37 38 39 40 41 42 def get_service_status ( self , service_id ) -> str : \"\"\"Send a get service status request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_service_status\" , \"id\" : 0 , \"params\" : [ service_id ], } return self . send_request ( request ) get_services_metadata () Send a get services metadata request to the server. Source code in npc_engine\\service_clients\\control_client.py 54 55 56 57 58 59 60 61 62 def get_services_metadata ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Send a get services metadata request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_services_metadata\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) restart_service ( service_id ) Send a restart service request to the server. Source code in npc_engine\\service_clients\\control_client.py 44 45 46 47 48 49 50 51 52 def restart_service ( self , service_id ): \"\"\"Send a restart service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"restart_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) start_service ( service_id ) Send a start service request to the server. Source code in npc_engine\\service_clients\\control_client.py 14 15 16 17 18 19 20 21 22 def start_service ( self , service_id ): \"\"\"Send a start service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"start_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) stop_service ( service_id ) Send a stop service request to the server. Source code in npc_engine\\service_clients\\control_client.py 24 25 26 27 28 29 30 31 32 def stop_service ( self , service_id ): \"\"\"Send a stop service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"stop_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) sequence_classifier_client Huggingface sequence classifier interface client implementation. SequenceClassifierClient Bases: ServiceClient Json rpc client for sequence classifier service. Source code in npc_engine\\service_clients\\sequence_classifier_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class SequenceClassifierClient ( ServiceClient ): \"\"\"Json rpc client for sequence classifier service.\"\"\" def __init__ ( self , zmq_context : zmq . Context , service_id : str = \"SequenceClassifierAPI\" , ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , service_id ) def classify ( self , texts : List [ Union [ str , Tuple [ str , str ]]]) -> str : \"\"\"Send a classify request to the SequenceClassifierAPI. Args: texts: Batch of texts to classify. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"classify\" , \"id\" : 0 , \"params\" : [ texts ], } reply = self . send_request ( request ) return reply @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"SequenceClassifierAPI\" __init__ ( zmq_context , service_id = 'SequenceClassifierAPI' ) Connect to the server on the port. Source code in npc_engine\\service_clients\\sequence_classifier_client.py 10 11 12 13 14 15 16 def __init__ ( self , zmq_context : zmq . Context , service_id : str = \"SequenceClassifierAPI\" , ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , service_id ) classify ( texts ) Send a classify request to the SequenceClassifierAPI. Parameters: Name Type Description Default texts List [ Union [ str , Tuple [ str , str ]]] Batch of texts to classify. required Source code in npc_engine\\service_clients\\sequence_classifier_client.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def classify ( self , texts : List [ Union [ str , Tuple [ str , str ]]]) -> str : \"\"\"Send a classify request to the SequenceClassifierAPI. Args: texts: Batch of texts to classify. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"classify\" , \"id\" : 0 , \"params\" : [ texts ], } reply = self . send_request ( request ) return reply get_api_name () classmethod Return the name of the API. Source code in npc_engine\\service_clients\\sequence_classifier_client.py 33 34 35 36 @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"SequenceClassifierAPI\" service_client Module that implements ZMQ base client communication over JSON-RPC 2.0 (https://www.jsonrpc.org/specification). ServiceClient Bases: ABC Base json rpc client. Source code in npc_engine\\service_clients\\service_client.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class ServiceClient ( ABC ): \"\"\"Base json rpc client.\"\"\" clients = {} def __init_subclass__ ( cls , ** kwargs ): \"\"\"Init subclass where service classes get registered to be discovered.\"\"\" super () . __init_subclass__ ( ** kwargs ) cls . clients [ cls . get_api_name ()] = cls def __init__ ( self , zmq_context : zmq . Context , service_id : str = None ): \"\"\"Connect to the server on the port.\"\"\" self . context = zmq_context self . socket = self . context . socket ( zmq . REQ ) self . socket . setsockopt ( zmq . LINGER , 0 ) self . socket . setsockopt ( zmq . IDENTITY , service_id . encode ( \"utf-8\" ) if service_id else self . get_api_name () . encode ( \"utf-8\" ), ) self . socket . connect ( build_ipc_uri ( \"self\" )) logger . info ( \"Connected to server\" ) def send_request ( self , request : Dict [ str , Any ]) -> Any : \"\"\"Send request to the server and return the response. Args: request: The request to send to the server. Returns: The result from the server. \"\"\" logger . trace ( f \"Sending request: { request } \" ) self . socket . send_json ( request ) response = self . socket . recv_json () logger . trace ( f \"Received response: { response } \" ) if \"result\" in response : return response [ \"result\" ] elif \"code\" in response : raise RuntimeError ( f \"code: { response [ 'code' ] } . { response [ 'message' ] } \" ) @abstractclassmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" pass @classmethod def get_api_client ( cls , api_name : str ) -> \"ServiceClient\" : \"\"\"Return the client for the api.\"\"\" try : return cls . clients [ api_name ] except KeyError : raise RuntimeError ( f \"Client for the API { api_name } not found. Available clients: { list ( cls . clients . keys ()) } \" ) __init__ ( zmq_context , service_id = None ) Connect to the server on the port. Source code in npc_engine\\service_clients\\service_client.py 22 23 24 25 26 27 28 29 30 31 32 33 34 def __init__ ( self , zmq_context : zmq . Context , service_id : str = None ): \"\"\"Connect to the server on the port.\"\"\" self . context = zmq_context self . socket = self . context . socket ( zmq . REQ ) self . socket . setsockopt ( zmq . LINGER , 0 ) self . socket . setsockopt ( zmq . IDENTITY , service_id . encode ( \"utf-8\" ) if service_id else self . get_api_name () . encode ( \"utf-8\" ), ) self . socket . connect ( build_ipc_uri ( \"self\" )) logger . info ( \"Connected to server\" ) __init_subclass__ ( ** kwargs ) Init subclass where service classes get registered to be discovered. Source code in npc_engine\\service_clients\\service_client.py 17 18 19 20 def __init_subclass__ ( cls , ** kwargs ): \"\"\"Init subclass where service classes get registered to be discovered.\"\"\" super () . __init_subclass__ ( ** kwargs ) cls . clients [ cls . get_api_name ()] = cls get_api_client ( api_name ) classmethod Return the client for the api. Source code in npc_engine\\service_clients\\service_client.py 59 60 61 62 63 64 65 66 67 @classmethod def get_api_client ( cls , api_name : str ) -> \"ServiceClient\" : \"\"\"Return the client for the api.\"\"\" try : return cls . clients [ api_name ] except KeyError : raise RuntimeError ( f \"Client for the API { api_name } not found. Available clients: { list ( cls . clients . keys ()) } \" ) get_api_name () Return the name of the API. Source code in npc_engine\\service_clients\\service_client.py 54 55 56 57 @abstractclassmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" pass send_request ( request ) Send request to the server and return the response. Parameters: Name Type Description Default request Dict [ str , Any ] The request to send to the server. required Returns: Type Description Any The result from the server. Source code in npc_engine\\service_clients\\service_client.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def send_request ( self , request : Dict [ str , Any ]) -> Any : \"\"\"Send request to the server and return the response. Args: request: The request to send to the server. Returns: The result from the server. \"\"\" logger . trace ( f \"Sending request: { request } \" ) self . socket . send_json ( request ) response = self . socket . recv_json () logger . trace ( f \"Received response: { response } \" ) if \"result\" in response : return response [ \"result\" ] elif \"code\" in response : raise RuntimeError ( f \"code: { response [ 'code' ] } . { response [ 'message' ] } \" ) similarity_client Huggingface semantic similarity interface client implementation. SimilarityClient Bases: ServiceClient Json rpc client for semantic similarity service. Source code in npc_engine\\service_clients\\similarity_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class SimilarityClient ( ServiceClient ): \"\"\"Json rpc client for semantic similarity service.\"\"\" def __init__ ( self , zmq_context : zmq . Context , service_id : str = \"SimilarityAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , service_id ) def compare ( self , query : str , context : List [ str ]) -> float : \"\"\"Send a comparison request to the server. Args: query: A string to compute similarity with contexts. context: A list of strings to compute similiarity with query. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"compare\" , \"id\" : 0 , \"params\" : [ query , context ], } reply = self . send_request ( request ) return reply @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"SimilarityAPI\" __init__ ( zmq_context , service_id = 'SimilarityAPI' ) Connect to the server on the port. Source code in npc_engine\\service_clients\\similarity_client.py 10 11 12 def __init__ ( self , zmq_context : zmq . Context , service_id : str = \"SimilarityAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , service_id ) compare ( query , context ) Send a comparison request to the server. Parameters: Name Type Description Default query str A string to compute similarity with contexts. required context List [ str ] A list of strings to compute similiarity with query. required Source code in npc_engine\\service_clients\\similarity_client.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def compare ( self , query : str , context : List [ str ]) -> float : \"\"\"Send a comparison request to the server. Args: query: A string to compute similarity with contexts. context: A list of strings to compute similiarity with query. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"compare\" , \"id\" : 0 , \"params\" : [ query , context ], } reply = self . send_request ( request ) return reply get_api_name () classmethod Return the name of the API. Source code in npc_engine\\service_clients\\similarity_client.py 30 31 32 33 @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"SimilarityAPI\" text_generation_client Huggingface chatbot interface client implementation. TextGenerationClient Bases: ServiceClient Json rpc client for chatbot service. Source code in npc_engine\\service_clients\\text_generation_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 class TextGenerationClient ( ServiceClient ): \"\"\"Json rpc client for chatbot service.\"\"\" def __init__ ( self , zmq_context : zmq . Context , service_id : str = \"TextGenerationAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , service_id ) def generate_reply ( self , context : Dict [ str , Any ]) -> str : \"\"\"Send a chatbot request to the server. Args: context: A dictionary containing the chatbot request. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"generate_reply\" , \"id\" : 0 , \"params\" : [ context ], } reply = self . send_request ( request ) return reply def get_prompt_template ( self ) -> str : \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_prompt_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) def get_context_template ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_context_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) def get_special_tokens ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_special_tokens\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"TextGenerationAPI\" __init__ ( zmq_context , service_id = 'TextGenerationAPI' ) Connect to the server on the port. Source code in npc_engine\\service_clients\\text_generation_client.py 10 11 12 def __init__ ( self , zmq_context : zmq . Context , service_id : str = \"TextGenerationAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , service_id ) generate_reply ( context ) Send a chatbot request to the server. Parameters: Name Type Description Default context Dict [ str , Any ] A dictionary containing the chatbot request. required Source code in npc_engine\\service_clients\\text_generation_client.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def generate_reply ( self , context : Dict [ str , Any ]) -> str : \"\"\"Send a chatbot request to the server. Args: context: A dictionary containing the chatbot request. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"generate_reply\" , \"id\" : 0 , \"params\" : [ context ], } reply = self . send_request ( request ) return reply get_api_name () classmethod Return the name of the API. Source code in npc_engine\\service_clients\\text_generation_client.py 59 60 61 62 @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"TextGenerationAPI\" get_context_template () Send a chatbot request to the server. Source code in npc_engine\\service_clients\\text_generation_client.py 39 40 41 42 43 44 45 46 47 def get_context_template ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_context_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) get_prompt_template () Send a chatbot request to the server. Source code in npc_engine\\service_clients\\text_generation_client.py 29 30 31 32 33 34 35 36 37 def get_prompt_template ( self ) -> str : \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_prompt_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) get_special_tokens () Send a chatbot request to the server. Source code in npc_engine\\service_clients\\text_generation_client.py 49 50 51 52 53 54 55 56 57 def get_special_tokens ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_special_tokens\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request )","title":"API Classes"},{"location":"inference_engine/api_classes/#existing-apis","text":"These are the existing API classes and corresponding API_METHODS:","title":"Existing APIs"},{"location":"inference_engine/api_classes/#npc_engine.services.sequence_classifier.sequence_classifier_base.SequenceClassifierAPI","text":"Bases: BaseService Abstract base class for text classification models.","title":"SequenceClassifierAPI"},{"location":"inference_engine/api_classes/#npc_engine.services.sequence_classifier.sequence_classifier_base.SequenceClassifierAPI.__init__","text":"Empty initialization method for API to be similar to other model base classes.","title":"__init__()"},{"location":"inference_engine/api_classes/#npc_engine.services.sequence_classifier.sequence_classifier_base.SequenceClassifierAPI.classify","text":"Classify a list of texts. Parameters: Name Type Description Default texts List [ str ] A list of texts to classify. required Returns: Type Description List [ List [ float ]] List of scores for each text.","title":"classify()"},{"location":"inference_engine/api_classes/#npc_engine.services.sequence_classifier.sequence_classifier_base.SequenceClassifierAPI.compute_scores_batch","text":"Compute scores for a list of texts. Parameters: Name Type Description Default texts List [ str ] A list of texts to compute scores for. required Returns: Type Description np . ndarray List of scores for each text.","title":"compute_scores_batch()"},{"location":"inference_engine/api_classes/#npc_engine.services.sequence_classifier.sequence_classifier_base.SequenceClassifierAPI.get_api_name","text":"Get the API name.","title":"get_api_name()"},{"location":"inference_engine/api_classes/#npc_engine.services.text_generation.text_generation_base.TextGenerationAPI","text":"Bases: BaseService Abstract base class for text generation models.","title":"TextGenerationAPI"},{"location":"inference_engine/api_classes/#npc_engine.services.text_generation.text_generation_base.TextGenerationAPI.__init__","text":"Initialize prompt formatting variables. Parameters: Name Type Description Default template_string str Template string to be rendered as prompt. None","title":"__init__()"},{"location":"inference_engine/api_classes/#npc_engine.services.text_generation.text_generation_base.TextGenerationAPI.generate_reply","text":"Format the model prompt and generate response. Parameters: Name Type Description Default context Dict [ str , Any ] Prompt context. required Returns: Type Description str Text response to a prompt.","title":"generate_reply()"},{"location":"inference_engine/api_classes/#npc_engine.services.text_generation.text_generation_base.TextGenerationAPI.get_api_name","text":"Get the API name.","title":"get_api_name()"},{"location":"inference_engine/api_classes/#npc_engine.services.text_generation.text_generation_base.TextGenerationAPI.get_context_template","text":"Return context template. Returns: Type Description Dict [ str , Any ] Example context","title":"get_context_template()"},{"location":"inference_engine/api_classes/#npc_engine.services.text_generation.text_generation_base.TextGenerationAPI.get_prompt_template","text":"Return prompt template string used to render model prompt. Returns: Type Description str A template string.","title":"get_prompt_template()"},{"location":"inference_engine/api_classes/#npc_engine.services.text_generation.text_generation_base.TextGenerationAPI.get_special_tokens","text":"Return dictionary mapping for special tokens. To be implemented by child class. Can then be used in template string as fields Returns: Type Description Dict [ str , str ] Dictionary of special tokens","title":"get_special_tokens()"},{"location":"inference_engine/api_classes/#npc_engine.services.text_generation.text_generation_base.TextGenerationAPI.run","text":"Abstract method for concrete implementation of generation. Parameters: Name Type Description Default prompt str Fromatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text","title":"run()"},{"location":"inference_engine/api_classes/#npc_engine.services.text_generation.text_generation_base.TextGenerationAPI.string_too_long","text":"Check if prompt is too long. Parameters: Name Type Description Default prompt str Prompt to check. required Returns: Type Description bool True if prompt is too long, False otherwise.","title":"string_too_long()"},{"location":"inference_engine/api_classes/#npc_engine.services.similarity.similarity_base.SimilarityAPI","text":"Bases: BaseService Abstract base class for text similarity models.","title":"SimilarityAPI"},{"location":"inference_engine/api_classes/#npc_engine.services.similarity.similarity_base.SimilarityAPI.__init__","text":"Empty initialization method for API to be similar to other model base classes.","title":"__init__()"},{"location":"inference_engine/api_classes/#npc_engine.services.similarity.similarity_base.SimilarityAPI.cache","text":"Cache embeddings of given sequences. Parameters: Name Type Description Default context List [ str ] A list of sentences to cache. required","title":"cache()"},{"location":"inference_engine/api_classes/#npc_engine.services.similarity.similarity_base.SimilarityAPI.compare","text":"Compare a query to the context. Parameters: Name Type Description Default query str A sentence to compare. required context List [ str ] A list of sentences to compare to. This will be cached if caching is enabled required Returns: Type Description List [ float ] List of similarities","title":"compare()"},{"location":"inference_engine/api_classes/#npc_engine.services.similarity.similarity_base.SimilarityAPI.compute_embedding","text":"Compute sentence embedding. Parameters: Name Type Description Default line str Sentence to embed required Returns: Type Description np . ndarray Embedding of shape (1, embedding_size)","title":"compute_embedding()"},{"location":"inference_engine/api_classes/#npc_engine.services.similarity.similarity_base.SimilarityAPI.compute_embedding_batch","text":"Compute line embeddings in batch. Parameters: Name Type Description Default lines List [ str ] List of sentences to embed required Returns: Type Description np . ndarray Embedding batch of shape (batch_size, embedding_size)","title":"compute_embedding_batch()"},{"location":"inference_engine/api_classes/#npc_engine.services.similarity.similarity_base.SimilarityAPI.get_api_name","text":"Get the API name.","title":"get_api_name()"},{"location":"inference_engine/api_classes/#npc_engine.services.similarity.similarity_base.SimilarityAPI.metric","text":"Compute distance between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Parameters: Name Type Description Default embedding_a np . ndarray Embedding of shape (1 or batch_size, embedding_size) required embedding_b np . ndarray Embedding of shape (1 or batch_size, embedding_size) required Returns: Type Description np . ndarray Vector of distances (batch_size or 1,)","title":"metric()"},{"location":"inference_engine/api_classes/#npc_engine.services.tts.tts_base.TextToSpeechAPI","text":"Bases: BaseService Abstract base class for text-to-speech models.","title":"TextToSpeechAPI"},{"location":"inference_engine/api_classes/#npc_engine.services.tts.tts_base.TextToSpeechAPI.__init__","text":"Empty initialization method for API to be similar to other model base classes.","title":"__init__()"},{"location":"inference_engine/api_classes/#npc_engine.services.tts.tts_base.TextToSpeechAPI._chain_run","text":"Chain the run method to be used in the generator.","title":"_chain_run()"},{"location":"inference_engine/api_classes/#npc_engine.services.tts.tts_base.TextToSpeechAPI.get_api_name","text":"Get the API name.","title":"get_api_name()"},{"location":"inference_engine/api_classes/#npc_engine.services.tts.tts_base.TextToSpeechAPI.get_speaker_ids","text":"Get list of available speaker ids. Returns: Type Description List [ str ] The return value. True for success, False otherwise.","title":"get_speaker_ids()"},{"location":"inference_engine/api_classes/#npc_engine.services.tts.tts_base.TextToSpeechAPI.run","text":"Create a generator for iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Returns: Type Description Iterable [ np . ndarray ] Generator that yields next chunk of speech in the form of f32 ndarray.","title":"run()"},{"location":"inference_engine/api_classes/#npc_engine.services.tts.tts_base.TextToSpeechAPI.tts_get_results","text":"Retrieve the next chunk of generated speech. Returns: Type Description Iterable [ np . ndarray ] Next chunk of speech in the form of f32 ndarray.","title":"tts_get_results()"},{"location":"inference_engine/api_classes/#npc_engine.services.tts.tts_base.TextToSpeechAPI.tts_start","text":"Initiate iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required","title":"tts_start()"},{"location":"inference_engine/api_classes/#creating-new-apis","text":"You can use this dummy API example to create your own: from npc_engine.services.base_service import BaseService class EchoAPI(BaseService): API_METHODS: List[str] = [\"echo\"] def __init__(self, *args, **kwargs): pass def echo(self, text): return text Dont forget Import new API to npc-engine.services so that it is discovered. Models that are implemented for the API should appear there too.","title":"Creating new APIs"},{"location":"inference_engine/api_classes/#calling-other-apis","text":"There are a set of service clients that can be used to call other APIs that are running on the server. They will block other services so they should be used carefully. Module implementing the clients for services.","title":"Calling other APIs"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.control_client","text":"Control interface client implementation.","title":"control_client"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.control_client.ControlClient","text":"Bases: ServiceClient Json rpc client for control requests. Source code in npc_engine\\service_clients\\control_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class ControlClient ( ServiceClient ): \"\"\"Json rpc client for control requests.\"\"\" def __init__ ( self , zmq_context : zmq . Context ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , \"control\" ) def start_service ( self , service_id ): \"\"\"Send a start service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"start_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) def stop_service ( self , service_id ): \"\"\"Send a stop service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"stop_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) def get_service_status ( self , service_id ) -> str : \"\"\"Send a get service status request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_service_status\" , \"id\" : 0 , \"params\" : [ service_id ], } return self . send_request ( request ) def restart_service ( self , service_id ): \"\"\"Send a restart service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"restart_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) def get_services_metadata ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Send a get services metadata request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_services_metadata\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) def get_service_metadata ( self , service_id : str ) -> Dict [ str , Any ]: \"\"\"Send a get services metadata request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_service_metadata\" , \"id\" : 0 , \"params\" : [ service_id ], } return self . send_request ( request ) def check_dependency ( self , service_id , dependency_id ) -> bool : \"\"\"Send a check dependency request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"check_dependency\" , \"id\" : 0 , \"params\" : [ service_id , dependency_id ], } return self . send_request ( request ) @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"control\"","title":"ControlClient"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.control_client.ControlClient.__init__","text":"Connect to the server on the port. Source code in npc_engine\\service_clients\\control_client.py 10 11 12 def __init__ ( self , zmq_context : zmq . Context ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , \"control\" )","title":"__init__()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.control_client.ControlClient.check_dependency","text":"Send a check dependency request to the server. Source code in npc_engine\\service_clients\\control_client.py 74 75 76 77 78 79 80 81 82 def check_dependency ( self , service_id , dependency_id ) -> bool : \"\"\"Send a check dependency request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"check_dependency\" , \"id\" : 0 , \"params\" : [ service_id , dependency_id ], } return self . send_request ( request )","title":"check_dependency()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.control_client.ControlClient.get_api_name","text":"Return the name of the API. Source code in npc_engine\\service_clients\\control_client.py 84 85 86 87 @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"control\"","title":"get_api_name()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.control_client.ControlClient.get_service_metadata","text":"Send a get services metadata request to the server. Source code in npc_engine\\service_clients\\control_client.py 64 65 66 67 68 69 70 71 72 def get_service_metadata ( self , service_id : str ) -> Dict [ str , Any ]: \"\"\"Send a get services metadata request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_service_metadata\" , \"id\" : 0 , \"params\" : [ service_id ], } return self . send_request ( request )","title":"get_service_metadata()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.control_client.ControlClient.get_service_status","text":"Send a get service status request to the server. Source code in npc_engine\\service_clients\\control_client.py 34 35 36 37 38 39 40 41 42 def get_service_status ( self , service_id ) -> str : \"\"\"Send a get service status request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_service_status\" , \"id\" : 0 , \"params\" : [ service_id ], } return self . send_request ( request )","title":"get_service_status()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.control_client.ControlClient.get_services_metadata","text":"Send a get services metadata request to the server. Source code in npc_engine\\service_clients\\control_client.py 54 55 56 57 58 59 60 61 62 def get_services_metadata ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Send a get services metadata request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_services_metadata\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request )","title":"get_services_metadata()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.control_client.ControlClient.restart_service","text":"Send a restart service request to the server. Source code in npc_engine\\service_clients\\control_client.py 44 45 46 47 48 49 50 51 52 def restart_service ( self , service_id ): \"\"\"Send a restart service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"restart_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request )","title":"restart_service()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.control_client.ControlClient.start_service","text":"Send a start service request to the server. Source code in npc_engine\\service_clients\\control_client.py 14 15 16 17 18 19 20 21 22 def start_service ( self , service_id ): \"\"\"Send a start service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"start_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request )","title":"start_service()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.control_client.ControlClient.stop_service","text":"Send a stop service request to the server. Source code in npc_engine\\service_clients\\control_client.py 24 25 26 27 28 29 30 31 32 def stop_service ( self , service_id ): \"\"\"Send a stop service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"stop_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request )","title":"stop_service()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.sequence_classifier_client","text":"Huggingface sequence classifier interface client implementation.","title":"sequence_classifier_client"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.sequence_classifier_client.SequenceClassifierClient","text":"Bases: ServiceClient Json rpc client for sequence classifier service. Source code in npc_engine\\service_clients\\sequence_classifier_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class SequenceClassifierClient ( ServiceClient ): \"\"\"Json rpc client for sequence classifier service.\"\"\" def __init__ ( self , zmq_context : zmq . Context , service_id : str = \"SequenceClassifierAPI\" , ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , service_id ) def classify ( self , texts : List [ Union [ str , Tuple [ str , str ]]]) -> str : \"\"\"Send a classify request to the SequenceClassifierAPI. Args: texts: Batch of texts to classify. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"classify\" , \"id\" : 0 , \"params\" : [ texts ], } reply = self . send_request ( request ) return reply @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"SequenceClassifierAPI\"","title":"SequenceClassifierClient"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.sequence_classifier_client.SequenceClassifierClient.__init__","text":"Connect to the server on the port. Source code in npc_engine\\service_clients\\sequence_classifier_client.py 10 11 12 13 14 15 16 def __init__ ( self , zmq_context : zmq . Context , service_id : str = \"SequenceClassifierAPI\" , ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , service_id )","title":"__init__()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.sequence_classifier_client.SequenceClassifierClient.classify","text":"Send a classify request to the SequenceClassifierAPI. Parameters: Name Type Description Default texts List [ Union [ str , Tuple [ str , str ]]] Batch of texts to classify. required Source code in npc_engine\\service_clients\\sequence_classifier_client.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def classify ( self , texts : List [ Union [ str , Tuple [ str , str ]]]) -> str : \"\"\"Send a classify request to the SequenceClassifierAPI. Args: texts: Batch of texts to classify. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"classify\" , \"id\" : 0 , \"params\" : [ texts ], } reply = self . send_request ( request ) return reply","title":"classify()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.sequence_classifier_client.SequenceClassifierClient.get_api_name","text":"Return the name of the API. Source code in npc_engine\\service_clients\\sequence_classifier_client.py 33 34 35 36 @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"SequenceClassifierAPI\"","title":"get_api_name()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.service_client","text":"Module that implements ZMQ base client communication over JSON-RPC 2.0 (https://www.jsonrpc.org/specification).","title":"service_client"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.service_client.ServiceClient","text":"Bases: ABC Base json rpc client. Source code in npc_engine\\service_clients\\service_client.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class ServiceClient ( ABC ): \"\"\"Base json rpc client.\"\"\" clients = {} def __init_subclass__ ( cls , ** kwargs ): \"\"\"Init subclass where service classes get registered to be discovered.\"\"\" super () . __init_subclass__ ( ** kwargs ) cls . clients [ cls . get_api_name ()] = cls def __init__ ( self , zmq_context : zmq . Context , service_id : str = None ): \"\"\"Connect to the server on the port.\"\"\" self . context = zmq_context self . socket = self . context . socket ( zmq . REQ ) self . socket . setsockopt ( zmq . LINGER , 0 ) self . socket . setsockopt ( zmq . IDENTITY , service_id . encode ( \"utf-8\" ) if service_id else self . get_api_name () . encode ( \"utf-8\" ), ) self . socket . connect ( build_ipc_uri ( \"self\" )) logger . info ( \"Connected to server\" ) def send_request ( self , request : Dict [ str , Any ]) -> Any : \"\"\"Send request to the server and return the response. Args: request: The request to send to the server. Returns: The result from the server. \"\"\" logger . trace ( f \"Sending request: { request } \" ) self . socket . send_json ( request ) response = self . socket . recv_json () logger . trace ( f \"Received response: { response } \" ) if \"result\" in response : return response [ \"result\" ] elif \"code\" in response : raise RuntimeError ( f \"code: { response [ 'code' ] } . { response [ 'message' ] } \" ) @abstractclassmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" pass @classmethod def get_api_client ( cls , api_name : str ) -> \"ServiceClient\" : \"\"\"Return the client for the api.\"\"\" try : return cls . clients [ api_name ] except KeyError : raise RuntimeError ( f \"Client for the API { api_name } not found. Available clients: { list ( cls . clients . keys ()) } \" )","title":"ServiceClient"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.service_client.ServiceClient.__init__","text":"Connect to the server on the port. Source code in npc_engine\\service_clients\\service_client.py 22 23 24 25 26 27 28 29 30 31 32 33 34 def __init__ ( self , zmq_context : zmq . Context , service_id : str = None ): \"\"\"Connect to the server on the port.\"\"\" self . context = zmq_context self . socket = self . context . socket ( zmq . REQ ) self . socket . setsockopt ( zmq . LINGER , 0 ) self . socket . setsockopt ( zmq . IDENTITY , service_id . encode ( \"utf-8\" ) if service_id else self . get_api_name () . encode ( \"utf-8\" ), ) self . socket . connect ( build_ipc_uri ( \"self\" )) logger . info ( \"Connected to server\" )","title":"__init__()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.service_client.ServiceClient.__init_subclass__","text":"Init subclass where service classes get registered to be discovered. Source code in npc_engine\\service_clients\\service_client.py 17 18 19 20 def __init_subclass__ ( cls , ** kwargs ): \"\"\"Init subclass where service classes get registered to be discovered.\"\"\" super () . __init_subclass__ ( ** kwargs ) cls . clients [ cls . get_api_name ()] = cls","title":"__init_subclass__()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.service_client.ServiceClient.get_api_client","text":"Return the client for the api. Source code in npc_engine\\service_clients\\service_client.py 59 60 61 62 63 64 65 66 67 @classmethod def get_api_client ( cls , api_name : str ) -> \"ServiceClient\" : \"\"\"Return the client for the api.\"\"\" try : return cls . clients [ api_name ] except KeyError : raise RuntimeError ( f \"Client for the API { api_name } not found. Available clients: { list ( cls . clients . keys ()) } \" )","title":"get_api_client()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.service_client.ServiceClient.get_api_name","text":"Return the name of the API. Source code in npc_engine\\service_clients\\service_client.py 54 55 56 57 @abstractclassmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" pass","title":"get_api_name()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.service_client.ServiceClient.send_request","text":"Send request to the server and return the response. Parameters: Name Type Description Default request Dict [ str , Any ] The request to send to the server. required Returns: Type Description Any The result from the server. Source code in npc_engine\\service_clients\\service_client.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def send_request ( self , request : Dict [ str , Any ]) -> Any : \"\"\"Send request to the server and return the response. Args: request: The request to send to the server. Returns: The result from the server. \"\"\" logger . trace ( f \"Sending request: { request } \" ) self . socket . send_json ( request ) response = self . socket . recv_json () logger . trace ( f \"Received response: { response } \" ) if \"result\" in response : return response [ \"result\" ] elif \"code\" in response : raise RuntimeError ( f \"code: { response [ 'code' ] } . { response [ 'message' ] } \" )","title":"send_request()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.similarity_client","text":"Huggingface semantic similarity interface client implementation.","title":"similarity_client"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.similarity_client.SimilarityClient","text":"Bases: ServiceClient Json rpc client for semantic similarity service. Source code in npc_engine\\service_clients\\similarity_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class SimilarityClient ( ServiceClient ): \"\"\"Json rpc client for semantic similarity service.\"\"\" def __init__ ( self , zmq_context : zmq . Context , service_id : str = \"SimilarityAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , service_id ) def compare ( self , query : str , context : List [ str ]) -> float : \"\"\"Send a comparison request to the server. Args: query: A string to compute similarity with contexts. context: A list of strings to compute similiarity with query. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"compare\" , \"id\" : 0 , \"params\" : [ query , context ], } reply = self . send_request ( request ) return reply @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"SimilarityAPI\"","title":"SimilarityClient"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.similarity_client.SimilarityClient.__init__","text":"Connect to the server on the port. Source code in npc_engine\\service_clients\\similarity_client.py 10 11 12 def __init__ ( self , zmq_context : zmq . Context , service_id : str = \"SimilarityAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , service_id )","title":"__init__()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.similarity_client.SimilarityClient.compare","text":"Send a comparison request to the server. Parameters: Name Type Description Default query str A string to compute similarity with contexts. required context List [ str ] A list of strings to compute similiarity with query. required Source code in npc_engine\\service_clients\\similarity_client.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def compare ( self , query : str , context : List [ str ]) -> float : \"\"\"Send a comparison request to the server. Args: query: A string to compute similarity with contexts. context: A list of strings to compute similiarity with query. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"compare\" , \"id\" : 0 , \"params\" : [ query , context ], } reply = self . send_request ( request ) return reply","title":"compare()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.similarity_client.SimilarityClient.get_api_name","text":"Return the name of the API. Source code in npc_engine\\service_clients\\similarity_client.py 30 31 32 33 @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"SimilarityAPI\"","title":"get_api_name()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.text_generation_client","text":"Huggingface chatbot interface client implementation.","title":"text_generation_client"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.text_generation_client.TextGenerationClient","text":"Bases: ServiceClient Json rpc client for chatbot service. Source code in npc_engine\\service_clients\\text_generation_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 class TextGenerationClient ( ServiceClient ): \"\"\"Json rpc client for chatbot service.\"\"\" def __init__ ( self , zmq_context : zmq . Context , service_id : str = \"TextGenerationAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , service_id ) def generate_reply ( self , context : Dict [ str , Any ]) -> str : \"\"\"Send a chatbot request to the server. Args: context: A dictionary containing the chatbot request. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"generate_reply\" , \"id\" : 0 , \"params\" : [ context ], } reply = self . send_request ( request ) return reply def get_prompt_template ( self ) -> str : \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_prompt_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) def get_context_template ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_context_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) def get_special_tokens ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_special_tokens\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"TextGenerationAPI\"","title":"TextGenerationClient"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.text_generation_client.TextGenerationClient.__init__","text":"Connect to the server on the port. Source code in npc_engine\\service_clients\\text_generation_client.py 10 11 12 def __init__ ( self , zmq_context : zmq . Context , service_id : str = \"TextGenerationAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , service_id )","title":"__init__()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.text_generation_client.TextGenerationClient.generate_reply","text":"Send a chatbot request to the server. Parameters: Name Type Description Default context Dict [ str , Any ] A dictionary containing the chatbot request. required Source code in npc_engine\\service_clients\\text_generation_client.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def generate_reply ( self , context : Dict [ str , Any ]) -> str : \"\"\"Send a chatbot request to the server. Args: context: A dictionary containing the chatbot request. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"generate_reply\" , \"id\" : 0 , \"params\" : [ context ], } reply = self . send_request ( request ) return reply","title":"generate_reply()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.text_generation_client.TextGenerationClient.get_api_name","text":"Return the name of the API. Source code in npc_engine\\service_clients\\text_generation_client.py 59 60 61 62 @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"TextGenerationAPI\"","title":"get_api_name()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.text_generation_client.TextGenerationClient.get_context_template","text":"Send a chatbot request to the server. Source code in npc_engine\\service_clients\\text_generation_client.py 39 40 41 42 43 44 45 46 47 def get_context_template ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_context_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request )","title":"get_context_template()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.text_generation_client.TextGenerationClient.get_prompt_template","text":"Send a chatbot request to the server. Source code in npc_engine\\service_clients\\text_generation_client.py 29 30 31 32 33 34 35 36 37 def get_prompt_template ( self ) -> str : \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_prompt_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request )","title":"get_prompt_template()"},{"location":"inference_engine/api_classes/#npc_engine.service_clients.text_generation_client.TextGenerationClient.get_special_tokens","text":"Send a chatbot request to the server. Source code in npc_engine\\service_clients\\text_generation_client.py 49 50 51 52 53 54 55 56 57 def get_special_tokens ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_special_tokens\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request )","title":"get_special_tokens()"},{"location":"inference_engine/benchmarks/","text":"","title":"Benchmarks"},{"location":"inference_engine/building/","text":"Build on Windows Create virtualenv and activate it python3 -m venv npc-engine-venv .\\npc-engine-venv\\activate.bat Install dependencies pip install -e .[dev,dml] (Optional) Compile, build and install your custom ONNX python runtime\" Build instructions can be found here (Optional) Run tests tox -e py38 Compile to exe with > pyinstaller --hidden-import=\"sklearn.utils._cython_blas\" --hidden-import=\"sklearn.neighbors.typedefs\" ^ --hidden-import=\"sklearn.neighbors.quad_tree\" --hidden-import=\"sklearn.tree._utils\" ^ --hidden-import=\"sklearn.neighbors._typedefs\" --hidden-import=\"sklearn.utils._typedefs\" ^ --hidden-import=\"sklearn.neighbors._partition_nodes\" --additional-hooks-dir hooks ^ --exclude-module tkinter --exclude-module matplotlib .\\npc_engine\\cli.py --onedir","title":"Build From Source"},{"location":"inference_engine/building/#build-on-windows","text":"","title":"Build on Windows"},{"location":"inference_engine/building/#create-virtualenv-and-activate-it","text":"python3 -m venv npc-engine-venv .\\npc-engine-venv\\activate.bat","title":"Create virtualenv and activate it"},{"location":"inference_engine/building/#install-dependencies","text":"pip install -e .[dev,dml]","title":"Install dependencies"},{"location":"inference_engine/building/#optional-compile-build-and-install-your-custom-onnx-python-runtime","text":"Build instructions can be found here","title":"(Optional) Compile, build and install your custom ONNX python runtime\""},{"location":"inference_engine/building/#optional-run-tests","text":"tox -e py38","title":"(Optional) Run tests"},{"location":"inference_engine/building/#compile-to-exe-with","text":"> pyinstaller --hidden-import=\"sklearn.utils._cython_blas\" --hidden-import=\"sklearn.neighbors.typedefs\" ^ --hidden-import=\"sklearn.neighbors.quad_tree\" --hidden-import=\"sklearn.tree._utils\" ^ --hidden-import=\"sklearn.neighbors._typedefs\" --hidden-import=\"sklearn.utils._typedefs\" ^ --hidden-import=\"sklearn.neighbors._partition_nodes\" --additional-hooks-dir hooks ^ --exclude-module tkinter --exclude-module matplotlib .\\npc_engine\\cli.py --onedir","title":"Compile to exe with"},{"location":"inference_engine/exporting_models/","text":"It's possible to convert models from popular libraries into npc-engine services via npc-engine-import-wizard . You can find the installation instructions and list of supported libraries in it's README. Most import wizards require some extras installed, here is the list of possible extras: transformers for \ud83e\udd17 Transformers integration. flowtron-tts for NVIDIA's Flowtron integration. espnet for ESPNet2 integration. Some extras might require additional dependencies installed outside of pip, they will usually report them missing via error messages. Tutorials To export model you need to use npc-engine-import-wizard import command. With Huggingface models you can use Huggingface Hub model id as well as path to the model. npc-engine-import-wizard import --models-path <path-to-models-folder> <model_id or folder> This will prompt you to select correct Exporter class and take you through filling in the required parameters. We will try to cover all the import wizards in the upcoming video tutorials that will appear here.","title":"Exporting models"},{"location":"inference_engine/exporting_models/#tutorials","text":"To export model you need to use npc-engine-import-wizard import command. With Huggingface models you can use Huggingface Hub model id as well as path to the model. npc-engine-import-wizard import --models-path <path-to-models-folder> <model_id or folder> This will prompt you to select correct Exporter class and take you through filling in the required parameters. We will try to cover all the import wizards in the upcoming video tutorials that will appear here.","title":"Tutorials"},{"location":"inference_engine/models/","text":"Service classes are specific implementations of API classes. They define the loading process in __init__ function and all the abstract methods required by the API class to function. How services are configured? MetadataManager Scans the models folder. For each discovered subfolder ServiceManager validates the config.yml and creates descriptors with metadata for each service. The mandatory field of config.yml is type (or model_type ) that must contain correct service class that was discovered and registered by BaseService parent class. This service class will be instantiated with parsed dictionary as parameters on ControlService.start_service request to control service. How is their API exposed? When service is started ControlService starts a new process with BaseService message handling loop. BaseService handles ZMQ IPC requests to it's exposed functions from API's API_METHODS class variable to the main process, while main process handles routing requests to this service. Existing service classes npc_engine.services.sequence_classifier.hf_classifier.HfClassifier Bases: SequenceClassifierAPI Huggingface transformers sequence classification. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with sequence-classification feature. Also requires saved tokenizer with huggingface tokenizers. __init__ ( model_path , * args , ** kwargs ) Create and load biencoder model for semantic similarity. Parameters: Name Type Description Default model_path str A path where model config and weights are. required compute_scores_batch ( texts ) Compute scores. Parameters: Name Type Description Default texts List [ Union [ str , Tuple [ str , str ]]] Sentence to embed required Returns: Name Type Description scores np . ndarray Scores for each text npc_engine.services.text_generation.hf_text_generation.HfChatbot Bases: TextGenerationAPI Chatbot that uses Huggingface transformer architectures. ONNX export of Huggingface transformer is required (see https://huggingface.co/docs/transformers/serialization). Features seq2seq-lm, causal-lm, seq2seq-lm-with-past, causal-lm-with-past are supported __init__ ( model_path , max_length = 100 , min_length = 2 , repetition_penalty = 1 , trunc_length = 512 , * args , ** kwargs ) Create the chatbot from config args and kwargs. Parameters: Name Type Description Default model_path str path to scan for model files (weights and configs) required max_length int stop generation at this number of tokens 100 min_length int model can't stop generating text before it's atleast this long in tokens 2 repetition_penalty float probability coef for same tokens to appear multiple times 1 trunc_length int length to truncate model prompt to 512 create_starter_inputs ( prompt = '' ) Create starter inputs for the model. Parameters: Name Type Description Default prompt str Prompt to start generation from. '' Returns: Type Description Dict [ str , Any ] Dict of inputs to the model decode_logit ( logit , temperature , topk ) Decode logit to token. Parameters: Name Type Description Default logit np . ndarray Logit to decode of shape (vocab_size,) required Returns: Type Description int Decoded token of shape get_special_tokens () Return dict of special tokens to be renderable from template. run ( prompt , temperature = 1.0 , topk = None ) Run text generation from given prompt and parameters. Parameters: Name Type Description Default prompt str Formatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1.0 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text string_too_long ( prompt ) Check if prompt is too long for the model. update_inputs_with_results ( inputs , results , decoded_token ) Update inputs with results from model. Parameters: Name Type Description Default inputs Dict [ str , np . ndarray ] Inputs to the model required results Dict [ str , np . ndarray ] Results from the model required Returns: Type Description Dict [ str , np . ndarray ] Updated inputs Dict [ str , np . ndarray ] Finished generation npc_engine.services.text_generation.bart.BartChatbot Bases: TextGenerationAPI BART based chatbot implementation class. This model class requires two ONNX models encoder_bart.onnx and decoder_bart.onnx that correspond to encoder and decoder from transformers EncoderDecoderModel and a tokenizer.json with huggingface tokenizers definition. encoder_bart.onnx spec: - inputs: `input_ids` - outputs: `encoder_hidden_state` decoder_bart.onnx spec: - inputs: `encoder_hidden_state` `decoder_input_ids` - outputs: `logits` __init__ ( model_path , max_steps = 100 , min_length = 2 , repetition_penalty = 1 , bos_token_id = 0 , eos_token_id = 2 , pad_token_id = 1 , sep_token_id = None , trunc_length = 512 , * args , ** kwargs ) Create the chatbot from config args and kwargs. Parameters: Name Type Description Default model_path path to scan for model files (weights and configs) required max_steps stop generation at this number of tokens 100 min_length model can't stop generating text before it's atleast this long in tokens 2 repetition_penalty probability coef for same tokens to appear multiple times 1 bos_token_id beginning of sequence token id 0 eos_token_id end of sequence token id 2 pad_token_id padding token id 1 sep_token_id token id for separating sequence into multiple parts None get_special_tokens () Retrun dict of special tokens to be renderable from template. run ( prompt , temperature = 1.0 , topk = None ) Run text generation from given prompt and parameters. Parameters: Name Type Description Default prompt str Fromatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1.0 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text string_too_long ( prompt ) Check if prompt is too long for the model. npc_engine.services.similarity.similarity_transformers.TransformerSemanticSimilarity Bases: SimilarityAPI Huggingface transformers semantic similarity. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with biencoder architecture. Also requires a tokenizer.json with huggingface tokenizers definition. model.onnx spec: - inputs: `input_ids` of shape `(batch_size, sequence)` `attention_mask` of shape `(batch_size, sequence)` (Optional) `input_type_ids` of shape `(batch_size, sequence)` - outputs: `token_embeddings` of shape `(batch_size, sequence, hidden_size)` __init__ ( model_path , metric = 'dot' , * args , ** kwargs ) Create and load biencoder model for semantic similarity. Parameters: Name Type Description Default model_path str A path where model config and weights are required metric str distance to compute semantic similarity 'dot' compute_embedding ( line ) Compute sentence embedding. Parameters: Name Type Description Default line str Sentence to embed required Returns: Type Description np . ndarray Embedding of shape (1, embedding_size) compute_embedding_batch ( lines ) Compute line embeddings in batch. Parameters: Name Type Description Default lines List [ str ] List of sentences to embed required Returns: Type Description np . ndarray Embedding batch of shape (batch_size, embedding_size) metric ( embedding_a , embedding_b ) Similarity between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Parameters: Name Type Description Default embedding_a np . ndarray Embedding of shape (1 or batch_size, embedding_size) required embedding_b np . ndarray Embedding of shape (1 or batch_size, embedding_size) required Returns: Type Description np . ndarray Vector of distances (batch_size or 1,) npc_engine.services.tts.flowtron.FlowtronTTS Bases: TextToSpeechAPI Implements Flowtron architecture inference. Paper: arXiv:2005.05957 Code: https://github.com/NVIDIA/flowtron Onnx export script can be found in this fork https://github.com/npc-engine/flowtron. This model class requires four ONNX models encoder.onnx , backward_flow.onnx , forward_flow.onnx and vocoder.onnx where first three are layers from Flowtron architecture ( flow corresponding to one direction pass of affine coupling layers) and vocoder.onnx is neural vocoder. For detailed specs refer to https://github.com/npc-engine/flowtron. __init__ ( model_path , max_frames = 400 , gate_threshold = 0.5 , sigma = 0.8 , smoothing_window = 3 , smoothing_weight = 0.5 , * args , ** kwargs ) Create and load Flowtron and vocoder models. get_speaker_ids () Return available ids of different speakers. run ( speaker_id , text , n_chunks ) Create a generator for iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Returns: Type Description Iterator [ np . ndarray ] Generator that yields next chunk of speech in the form of f32 ndarray. Default Models Fantasy Chatbot BartChatbot trained on LIGHT Dataset . Model consumes both self, other personas and location dialogue is happening in. Semantic Similarity sentence-transformers/all-MiniLM-L6-v2 Onnx export of sentence-transformers/all-MiniLM-L6-v2 . FlowtronTTS with Waveglow vocoder Nvidia's FlowtronTTS architecture using Waveglow vocoder. Weights were published by the authors, this model uses Flowtron LibriTTS2K version. Speech to text NeMo models This model is still heavy WIP it is best to use your platform's of choice existing solutions e.g. UnityEngine.Windows.Speech.DictationRecognizer in Unity. This implementation uses several models exported from NeMo toolkit: QuartzNet15x5 for transcription. Punctuation BERT for applying punctuation. Custom transformer for recognizing end of response to the context initialized from all-MiniLM-L6-v2 Creating new Services You can use this dummy model example to create your own: from npc_engine.services.text_generation.text_generation_base import TextGenerationAPI class EchoService(TextGenerationAPI): def __init__(self, model_path:str, *args, **kwargs): print(\"model is in {model_path}\") def get_special_tokens(self): return {} def run(self, prompt, temperature=1, topk=None): return prompt Dont forget Import new model to npc-engine.services so that it is discovered. Using other services from your service You can use other services from your service by using service clients. They can be created from inside the service with self.create_client(name) where name is the name of the dependency. These clients expose the same API as the dependency and can be just called from your service.","title":"Services"},{"location":"inference_engine/models/#how-services-are-configured","text":"MetadataManager Scans the models folder. For each discovered subfolder ServiceManager validates the config.yml and creates descriptors with metadata for each service. The mandatory field of config.yml is type (or model_type ) that must contain correct service class that was discovered and registered by BaseService parent class. This service class will be instantiated with parsed dictionary as parameters on ControlService.start_service request to control service.","title":"How services are configured?"},{"location":"inference_engine/models/#how-is-their-api-exposed","text":"When service is started ControlService starts a new process with BaseService message handling loop. BaseService handles ZMQ IPC requests to it's exposed functions from API's API_METHODS class variable to the main process, while main process handles routing requests to this service.","title":"How is their API exposed?"},{"location":"inference_engine/models/#existing-service-classes","text":"","title":"Existing service classes"},{"location":"inference_engine/models/#npc_engine.services.sequence_classifier.hf_classifier.HfClassifier","text":"Bases: SequenceClassifierAPI Huggingface transformers sequence classification. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with sequence-classification feature. Also requires saved tokenizer with huggingface tokenizers.","title":"HfClassifier"},{"location":"inference_engine/models/#npc_engine.services.sequence_classifier.hf_classifier.HfClassifier.__init__","text":"Create and load biencoder model for semantic similarity. Parameters: Name Type Description Default model_path str A path where model config and weights are. required","title":"__init__()"},{"location":"inference_engine/models/#npc_engine.services.sequence_classifier.hf_classifier.HfClassifier.compute_scores_batch","text":"Compute scores. Parameters: Name Type Description Default texts List [ Union [ str , Tuple [ str , str ]]] Sentence to embed required Returns: Name Type Description scores np . ndarray Scores for each text","title":"compute_scores_batch()"},{"location":"inference_engine/models/#npc_engine.services.text_generation.hf_text_generation.HfChatbot","text":"Bases: TextGenerationAPI Chatbot that uses Huggingface transformer architectures. ONNX export of Huggingface transformer is required (see https://huggingface.co/docs/transformers/serialization). Features seq2seq-lm, causal-lm, seq2seq-lm-with-past, causal-lm-with-past are supported","title":"HfChatbot"},{"location":"inference_engine/models/#npc_engine.services.text_generation.hf_text_generation.HfChatbot.__init__","text":"Create the chatbot from config args and kwargs. Parameters: Name Type Description Default model_path str path to scan for model files (weights and configs) required max_length int stop generation at this number of tokens 100 min_length int model can't stop generating text before it's atleast this long in tokens 2 repetition_penalty float probability coef for same tokens to appear multiple times 1 trunc_length int length to truncate model prompt to 512","title":"__init__()"},{"location":"inference_engine/models/#npc_engine.services.text_generation.hf_text_generation.HfChatbot.create_starter_inputs","text":"Create starter inputs for the model. Parameters: Name Type Description Default prompt str Prompt to start generation from. '' Returns: Type Description Dict [ str , Any ] Dict of inputs to the model","title":"create_starter_inputs()"},{"location":"inference_engine/models/#npc_engine.services.text_generation.hf_text_generation.HfChatbot.decode_logit","text":"Decode logit to token. Parameters: Name Type Description Default logit np . ndarray Logit to decode of shape (vocab_size,) required Returns: Type Description int Decoded token of shape","title":"decode_logit()"},{"location":"inference_engine/models/#npc_engine.services.text_generation.hf_text_generation.HfChatbot.get_special_tokens","text":"Return dict of special tokens to be renderable from template.","title":"get_special_tokens()"},{"location":"inference_engine/models/#npc_engine.services.text_generation.hf_text_generation.HfChatbot.run","text":"Run text generation from given prompt and parameters. Parameters: Name Type Description Default prompt str Formatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1.0 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text","title":"run()"},{"location":"inference_engine/models/#npc_engine.services.text_generation.hf_text_generation.HfChatbot.string_too_long","text":"Check if prompt is too long for the model.","title":"string_too_long()"},{"location":"inference_engine/models/#npc_engine.services.text_generation.hf_text_generation.HfChatbot.update_inputs_with_results","text":"Update inputs with results from model. Parameters: Name Type Description Default inputs Dict [ str , np . ndarray ] Inputs to the model required results Dict [ str , np . ndarray ] Results from the model required Returns: Type Description Dict [ str , np . ndarray ] Updated inputs Dict [ str , np . ndarray ] Finished generation","title":"update_inputs_with_results()"},{"location":"inference_engine/models/#npc_engine.services.text_generation.bart.BartChatbot","text":"Bases: TextGenerationAPI BART based chatbot implementation class. This model class requires two ONNX models encoder_bart.onnx and decoder_bart.onnx that correspond to encoder and decoder from transformers EncoderDecoderModel and a tokenizer.json with huggingface tokenizers definition. encoder_bart.onnx spec: - inputs: `input_ids` - outputs: `encoder_hidden_state` decoder_bart.onnx spec: - inputs: `encoder_hidden_state` `decoder_input_ids` - outputs: `logits`","title":"BartChatbot"},{"location":"inference_engine/models/#npc_engine.services.text_generation.bart.BartChatbot.__init__","text":"Create the chatbot from config args and kwargs. Parameters: Name Type Description Default model_path path to scan for model files (weights and configs) required max_steps stop generation at this number of tokens 100 min_length model can't stop generating text before it's atleast this long in tokens 2 repetition_penalty probability coef for same tokens to appear multiple times 1 bos_token_id beginning of sequence token id 0 eos_token_id end of sequence token id 2 pad_token_id padding token id 1 sep_token_id token id for separating sequence into multiple parts None","title":"__init__()"},{"location":"inference_engine/models/#npc_engine.services.text_generation.bart.BartChatbot.get_special_tokens","text":"Retrun dict of special tokens to be renderable from template.","title":"get_special_tokens()"},{"location":"inference_engine/models/#npc_engine.services.text_generation.bart.BartChatbot.run","text":"Run text generation from given prompt and parameters. Parameters: Name Type Description Default prompt str Fromatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1.0 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text","title":"run()"},{"location":"inference_engine/models/#npc_engine.services.text_generation.bart.BartChatbot.string_too_long","text":"Check if prompt is too long for the model.","title":"string_too_long()"},{"location":"inference_engine/models/#npc_engine.services.similarity.similarity_transformers.TransformerSemanticSimilarity","text":"Bases: SimilarityAPI Huggingface transformers semantic similarity. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with biencoder architecture. Also requires a tokenizer.json with huggingface tokenizers definition. model.onnx spec: - inputs: `input_ids` of shape `(batch_size, sequence)` `attention_mask` of shape `(batch_size, sequence)` (Optional) `input_type_ids` of shape `(batch_size, sequence)` - outputs: `token_embeddings` of shape `(batch_size, sequence, hidden_size)`","title":"TransformerSemanticSimilarity"},{"location":"inference_engine/models/#npc_engine.services.similarity.similarity_transformers.TransformerSemanticSimilarity.__init__","text":"Create and load biencoder model for semantic similarity. Parameters: Name Type Description Default model_path str A path where model config and weights are required metric str distance to compute semantic similarity 'dot'","title":"__init__()"},{"location":"inference_engine/models/#npc_engine.services.similarity.similarity_transformers.TransformerSemanticSimilarity.compute_embedding","text":"Compute sentence embedding. Parameters: Name Type Description Default line str Sentence to embed required Returns: Type Description np . ndarray Embedding of shape (1, embedding_size)","title":"compute_embedding()"},{"location":"inference_engine/models/#npc_engine.services.similarity.similarity_transformers.TransformerSemanticSimilarity.compute_embedding_batch","text":"Compute line embeddings in batch. Parameters: Name Type Description Default lines List [ str ] List of sentences to embed required Returns: Type Description np . ndarray Embedding batch of shape (batch_size, embedding_size)","title":"compute_embedding_batch()"},{"location":"inference_engine/models/#npc_engine.services.similarity.similarity_transformers.TransformerSemanticSimilarity.metric","text":"Similarity between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Parameters: Name Type Description Default embedding_a np . ndarray Embedding of shape (1 or batch_size, embedding_size) required embedding_b np . ndarray Embedding of shape (1 or batch_size, embedding_size) required Returns: Type Description np . ndarray Vector of distances (batch_size or 1,)","title":"metric()"},{"location":"inference_engine/models/#npc_engine.services.tts.flowtron.FlowtronTTS","text":"Bases: TextToSpeechAPI Implements Flowtron architecture inference. Paper: arXiv:2005.05957 Code: https://github.com/NVIDIA/flowtron Onnx export script can be found in this fork https://github.com/npc-engine/flowtron. This model class requires four ONNX models encoder.onnx , backward_flow.onnx , forward_flow.onnx and vocoder.onnx where first three are layers from Flowtron architecture ( flow corresponding to one direction pass of affine coupling layers) and vocoder.onnx is neural vocoder. For detailed specs refer to https://github.com/npc-engine/flowtron.","title":"FlowtronTTS"},{"location":"inference_engine/models/#npc_engine.services.tts.flowtron.flowtron.FlowtronTTS.__init__","text":"Create and load Flowtron and vocoder models.","title":"__init__()"},{"location":"inference_engine/models/#npc_engine.services.tts.flowtron.flowtron.FlowtronTTS.get_speaker_ids","text":"Return available ids of different speakers.","title":"get_speaker_ids()"},{"location":"inference_engine/models/#npc_engine.services.tts.flowtron.flowtron.FlowtronTTS.run","text":"Create a generator for iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Returns: Type Description Iterator [ np . ndarray ] Generator that yields next chunk of speech in the form of f32 ndarray.","title":"run()"},{"location":"inference_engine/models/#default-models","text":"","title":"Default Models"},{"location":"inference_engine/models/#fantasy-chatbot","text":"BartChatbot trained on LIGHT Dataset . Model consumes both self, other personas and location dialogue is happening in.","title":"Fantasy Chatbot"},{"location":"inference_engine/models/#semantic-similarity-sentence-transformersall-minilm-l6-v2","text":"Onnx export of sentence-transformers/all-MiniLM-L6-v2 .","title":"Semantic Similarity sentence-transformers/all-MiniLM-L6-v2"},{"location":"inference_engine/models/#flowtrontts-with-waveglow-vocoder","text":"Nvidia's FlowtronTTS architecture using Waveglow vocoder. Weights were published by the authors, this model uses Flowtron LibriTTS2K version.","title":"FlowtronTTS with Waveglow vocoder"},{"location":"inference_engine/models/#speech-to-text-nemo-models","text":"This model is still heavy WIP it is best to use your platform's of choice existing solutions e.g. UnityEngine.Windows.Speech.DictationRecognizer in Unity. This implementation uses several models exported from NeMo toolkit: QuartzNet15x5 for transcription. Punctuation BERT for applying punctuation. Custom transformer for recognizing end of response to the context initialized from all-MiniLM-L6-v2","title":"Speech to text NeMo models"},{"location":"inference_engine/models/#creating-new-services","text":"You can use this dummy model example to create your own: from npc_engine.services.text_generation.text_generation_base import TextGenerationAPI class EchoService(TextGenerationAPI): def __init__(self, model_path:str, *args, **kwargs): print(\"model is in {model_path}\") def get_special_tokens(self): return {} def run(self, prompt, temperature=1, topk=None): return prompt Dont forget Import new model to npc-engine.services so that it is discovered.","title":"Creating new Services"},{"location":"inference_engine/models/#using-other-services-from-your-service","text":"You can use other services from your service by using service clients. They can be created from inside the service with self.create_client(name) where name is the name of the dependency. These clients expose the same API as the dependency and can be just called from your service.","title":"Using other services from your service"},{"location":"inference_engine/overview/","text":"NPC Engine is a local python JSON-RPC server. It supports using 0MQ TCP sockets and HTTP protocols. The main goal of this server is to provide access to inference services that handle various functionalities required for storytelling and in-game AI. How does it work Each service is defined in it's own folder in provided --models-path . Service's folder name becomes it's unique Id. On start NPC Engine will expose a special service called control that allows you to get services metadata and to control their lifetime. Services are running in separate processes but handle requests sequentially (including situations when services call each-other). To route requests NPC Engine uses 0MQ identity . It can be defined only before connecting, therefore each connected socket will be refering to it's own service. Service resolution rules are simple: - If no identity provided, request is routed to the first service that implements the method called. - If identity is API name, request is routed to the first service that implements the API. - Same logic in case identity is a service class name. - Identity could be a service ID (folder name), then request is routed to the service with that ID. Service APIs are defined in API classes . Service exposes methods that are listed in API_METHODS class variable. You can find a description of default services available in Default Models section. The specifics of how model is loaded and how inference is done is defined in specific service classes Here is an example of JSON RPC request: request with identity \"some_model\" { \"method\": \"do_smth\", \"params\": [\"hello\"], \"jsonrpc\": \"2.0\", \"id\": 0 } will result in call to some_model.do_smth('hello') on the server, or will fail if the service wasn't started. Creating an integration A checklist for a new integration would be to: Create a class that manages npc-engine subprocess (starts, terminates, checks if it's alive). Create a connection class that talks to npc-engine. Review API classes and wrap JSON-RPC requests into the native functions.","title":"Overview"},{"location":"inference_engine/overview/#how-does-it-work","text":"Each service is defined in it's own folder in provided --models-path . Service's folder name becomes it's unique Id. On start NPC Engine will expose a special service called control that allows you to get services metadata and to control their lifetime. Services are running in separate processes but handle requests sequentially (including situations when services call each-other). To route requests NPC Engine uses 0MQ identity . It can be defined only before connecting, therefore each connected socket will be refering to it's own service. Service resolution rules are simple: - If no identity provided, request is routed to the first service that implements the method called. - If identity is API name, request is routed to the first service that implements the API. - Same logic in case identity is a service class name. - Identity could be a service ID (folder name), then request is routed to the service with that ID. Service APIs are defined in API classes . Service exposes methods that are listed in API_METHODS class variable. You can find a description of default services available in Default Models section. The specifics of how model is loaded and how inference is done is defined in specific service classes Here is an example of JSON RPC request: request with identity \"some_model\" { \"method\": \"do_smth\", \"params\": [\"hello\"], \"jsonrpc\": \"2.0\", \"id\": 0 } will result in call to some_model.do_smth('hello') on the server, or will fail if the service wasn't started.","title":"How does it work"},{"location":"inference_engine/overview/#creating-an-integration","text":"A checklist for a new integration would be to: Create a class that manages npc-engine subprocess (starts, terminates, checks if it's alive). Create a connection class that talks to npc-engine. Review API classes and wrap JSON-RPC requests into the native functions.","title":"Creating an integration"},{"location":"inference_engine/reference/","text":"Server for providing onnx runtime predictions for text generation and speech synthesis. Uses 0MQ REP/REQ sockets or HTTP with JSONRPC 2.0 protocol. cli This is the entry point for the command-line interface that starts npc-engine server. cli ( verbose ) NPC engine JSON RPC server CLI. Source code in npc_engine\\cli.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 @click . group () @click . option ( \"--verbose/--silent\" , \"-v\" , default = False , help = \"Enable verbose output.\" ) def cli ( verbose : bool ): \"\"\"NPC engine JSON RPC server CLI.\"\"\" logger . remove () if verbose : logger . add ( sys . stdout , format = \" {time} {level} {message} \" , level = \"DEBUG\" , enqueue = True ) logger . add ( os . path . join ( \"Logs\" , \"npc-engine.log\" ), format = \" {time} {level} {message} \" , level = \"DEBUG\" , enqueue = True , rotation = \"500 MB\" , ) click . echo ( click . style ( \"Verbose logging is enabled. (LEVEL=INFO)\" , fg = \"yellow\" , ) ) else : logger . add ( sys . stdout , format = \" {time} {level} {message} \" , level = \"WARNING\" , enqueue = True ) logger . add ( os . path . join ( \"Logs\" , \"npc-engine.log\" ), format = \" {time} {level} {message} \" , level = \"WARNING\" , enqueue = True , rotation = \"500 MB\" , ) describe ( models_path , model_id ) Show service detailed information. model_id argument follows service resolution rules of the npc-engine. Source code in npc_engine\\cli.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 @cli . command () @click . option ( \"--models-path\" , default = os . environ . get ( \"NPC_ENGINE_MODELS_PATH\" , \"./models\" ), help = \"The path to the folder with service configs.\" , ) @click . argument ( \"model_id\" ) def describe ( models_path : str , model_id : str ): \"\"\"Show service detailed information. model_id argument follows service resolution rules of the npc-engine. \"\"\" model_manager = MetadataManager ( models_path , \"not_used\" ) metadata = model_manager . get_metadata ( model_id ) click . echo ( metadata [ \"id\" ]) click . echo ( get_type_from_dict ( metadata )) click . echo ( \"Service description:\" ) click . echo ( metadata [ \"service_description\" ]) click . echo ( \"Model description:\" ) click . echo ( metadata [ \"readme\" ]) download_default_models ( models_path ) Download default models into the folder. Source code in npc_engine\\cli.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @cli . command () @click . option ( \"--models-path\" , default = os . environ . get ( \"NPC_ENGINE_MODELS_PATH\" , \"./models\" ), help = \"The path to the folder with service configs.\" , ) def download_default_models ( models_path : str ): # pragma: no cover \"\"\"Download default models into the folder.\"\"\" model_names = [ \"npc-engine/exported-paraphrase-MiniLM-L6-v2\" , \"npc-engine/exported-bart-light-gail-chatbot\" , \"npc-engine/exported-nemo-quartznet-ctc-stt\" , \"npc-engine/exported-flowtron-waveglow-librispeech-tts\" , ] revs = [ \"main\" , \"crop-fix\" , \"main\" , \"main\" , ] for model , rev in zip ( model_names , revs ): logger . info ( \"Downloading model {} \" , model ) logger . info ( \"Downloading {} \" , model ) tmp_folder = snapshot_download ( repo_id = model , revision = rev , cache_dir = models_path ) os . rename ( tmp_folder , os . path . join ( models_path , model . split ( \"/\" )[ - 1 ])) download_model ( models_path , revision , model_id ) Download a model from Huggingface Hub. Source code in npc_engine\\cli.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 @cli . command () @click . option ( \"--models-path\" , default = os . environ . get ( \"NPC_ENGINE_MODELS_PATH\" , \"./models\" , ), help = \"The path to the folder with service configs.\" , ) @click . option ( \"--revision\" , default = \"main\" , help = \"The commit/branch to use.\" ) @click . argument ( \"model_id\" ) def download_model ( models_path : str , revision , model_id : str ): # pragma: no cover \"\"\"Download a model from Huggingface Hub.\"\"\" model_correct = validate_hub_model ( models_path , model_id ) if model_correct : logger . info ( \"Downloading model {} \" , model_id ) tmp_folder = snapshot_download ( repo_id = model_id , revision = revision , cache_dir = models_path ) os . rename ( tmp_folder , os . path . join ( models_path , model_id . split ( \"/\" )[ - 1 ])) else : click . echo ( click . style ( f \" { model_id } is not a valid npc-engine model. You first need to import it.\" , fg = \"yellow\" , ) ) list_models ( models_path ) List the models in the folder. Source code in npc_engine\\cli.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 @cli . command () @click . option ( \"--models-path\" , default = os . environ . get ( \"NPC_ENGINE_MODELS_PATH\" , \"./models\" ), help = \"The path to the folder with service configs.\" , ) def list_models ( models_path : str ): \"\"\"List the models in the folder.\"\"\" metadata_manager = MetadataManager ( models_path , \"not_used\" ) metadata_list = metadata_manager . get_services_metadata () for metadata in metadata_list : click . echo ( metadata [ \"id\" ]) click . echo ( metadata [ \"service\" ]) click . echo ( \"Service description:\" ) click . echo ( metadata [ \"service_short_description\" ]) click . echo ( \"Model description:\" ) click . echo ( metadata [ \"readme\" ] . split ( \" \\n\\n \" )[ 0 ]) click . echo ( \"--------------------\" ) run ( port , start_all , models_path , http ) Load the models and start JSONRPC server. Source code in npc_engine\\cli.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def run ( port : str , start_all : bool , models_path : str , http : bool ): \"\"\"Load the models and start JSONRPC server.\"\"\" from npc_engine.server.control_service import ControlService context = zmq . asyncio . Context ( io_threads = 5 ) metadata_manager = MetadataManager ( models_path , port ) metadata_manager . port = port control_service = ControlService ( context , metadata_manager ) if not http : from npc_engine.server.server import ZMQServer server = ZMQServer ( context , control_service , metadata_manager , start_all ) else : from npc_engine.server.server import HTTPServer server = HTTPServer ( context , control_service , metadata_manager , start_all ) server . run () run_command ( port , start_all , models_path , http ) Start the server. Source code in npc_engine\\cli.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 @cli . command ( \"run\" ) @click . option ( \"--port\" , default = \"5555\" , help = \"The port to listen on.\" ) @click . option ( \"--start-all/--dont-start\" , default = True , help = \"Whether to start all services or not.\" , ) @click . option ( \"--models-path\" , default = os . environ . get ( \"NPC_ENGINE_MODELS_PATH\" , \"./models\" ), help = \"The path to the folder with service configs\" , ) @click . option ( \"--http/--zmq\" , default = False , help = \"Whether to use HTTP or ZMQ.\" ) def run_command ( port : str , start_all : bool , models_path : str , http : bool ): \"\"\"Start the server.\"\"\" run ( port , start_all , models_path , http ) set_models_path ( models_path ) Set the default models path. Parameters: Name Type Description Default models_path str The path to the models. required Source code in npc_engine\\cli.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @cli . command () @click . option ( \"--models-path\" , default = os . environ . get ( \"NPC_ENGINE_MODELS_PATH\" , \"./models\" ), help = \"The path to the folder with service configs.\" , ) def set_models_path ( models_path : str ): \"\"\"Set the default models path. Args: models_path (str): The path to the models. \"\"\" os . environ [ \"NPC_ENGINE_MODELS_PATH\" ] = models_path version () Get the npc engine version. Source code in npc_engine\\cli.py 216 217 218 219 @cli . command () def version (): \"\"\"Get the npc engine version.\"\"\" click . echo ( click . style ( f \" { __version__ } \" , bold = True )) server Module with RPC related functionality implementation. control_service Module that implements control service. ControlService Service that manages other services and routes requests. Source code in npc_engine\\server\\control_service.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 class ControlService : \"\"\"Service that manages other services and routes requests.\"\"\" def __init__ ( self , zmq_context : zmq . asyncio . Context , metadata_manager : MetadataManager , ) -> None : \"\"\"Initialize control service. Args: zmq_context: asyncio zmq context server_port: server port that will be passed to services for inter-communication \"\"\" self . server_port = metadata_manager . port self . metadata = metadata_manager self . control_dispatcher = Dispatcher () self . control_dispatcher . update ( { \"get_services_metadata\" : self . metadata . get_services_metadata , \"get_service_metadata\" : self . metadata . get_metadata , \"get_service_status\" : self . get_service_status , \"start_service\" : self . start_service , \"stop_service\" : self . stop_service , \"restart_service\" : self . restart_service , \"check_dependency\" : self . check_dependency , } ) self . zmq_context = zmq_context self . services = { service_id : { \"process\" : None , \"socket\" : None , \"state\" : ServiceState . STOPPED , } for service_id in self . metadata . services . keys () } def __del__ ( self ): \"\"\"Stop all services.\"\"\" if hasattr ( self , \"services\" ): for service_id , service in self . services . items (): if service [ \"state\" ] == ServiceState . RUNNING : try : self . stop_service ( service_id ) except Exception : pass async def handle_request ( self , address : str , request : str ) -> str : \"\"\"Parse request string and route request to correct service. Args: address (str): address of the service (either model name or class name) request (str): jsonRPC string Returns: str: jsonRPC response \"\"\" request_dict = json . loads ( request ) service_id = self . metadata . resolve_service ( address , request_dict [ \"method\" ]) self . check_service ( service_id ) logger . info ( f \"Request from { address } \\n Request: { request } \" ) if service_id == \"control\" : return JSONRPCResponseManager . handle ( request , self . control_dispatcher ) . json else : if self . services [ service_id ][ \"state\" ] != ServiceState . RUNNING : raise ValueError ( f \"Service { service_id } is not running\" ) else : socket = self . services [ service_id ][ \"socket\" ] await socket . send_string ( request ) response = await socket . recv_string () return response def check_service ( self , service_id ): \"\"\"Check if the service process is running.\"\"\" if ( service_id != \"control\" and ( self . services [ service_id ][ \"state\" ] == ServiceState . RUNNING or self . services [ service_id ][ \"state\" ] == ServiceState . STARTING or self . services [ service_id ][ \"state\" ] == ServiceState . AWAITING ) and not self . services [ service_id ][ \"process\" ] . is_alive () ): self . services [ service_id ][ \"state\" ] = ServiceState . ERROR raise ValueError ( f \"Error in service { service_id } . Process is not alive.\" ) def get_service_status ( self , service_id ): \"\"\"Get the status of the service.\"\"\" service_id = self . metadata . resolve_service ( service_id , None ) self . check_service ( service_id ) return self . services [ service_id ][ \"state\" ] def start_service ( self , service_id ): \"\"\"Start the service.\"\"\" service_id = self . metadata . resolve_service ( service_id , None ) self . check_service ( service_id ) if self . services [ service_id ][ \"state\" ] == ServiceState . RUNNING : raise ValueError ( f \"Service { service_id } is already running\" ) process = Process ( target = service_process , args = ( self . metadata , service_id , logger ), daemon = True , ) process . start () self . services [ service_id ][ \"process\" ] = process self . services [ service_id ][ \"state\" ] = ServiceState . STARTING self . services [ service_id ][ \"socket\" ] = self . zmq_context . socket ( zmq . REQ ) self . services [ service_id ][ \"socket\" ] . setsockopt ( zmq . LINGER , 0 ) self . services [ service_id ][ \"socket\" ] . setsockopt ( zmq . RCVTIMEO , 10000 ) self . services [ service_id ][ \"socket\" ] . connect ( self . metadata . services [ service_id ] . uri ) try : asyncio . create_task ( self . confirm_state_coroutine ( service_id )) except RuntimeError : logger . warning ( \"Create task to confirm service state failed.\" + \" Probably asyncio loop is not running.\" + \" Trying to execute it via asyncio.run()\" ) asyncio . run ( self . confirm_state_coroutine ( service_id )) async def confirm_state_coroutine ( self , service_id ): \"\"\"Confirm the state of the service.\"\"\" request = json . dumps ({ \"jsonrpc\" : \"2.0\" , \"method\" : \"status\" , \"id\" : 1 }) socket = self . services [ service_id ][ \"socket\" ] try : await socket . send_string ( request ) except zmq . Again : self . services [ service_id ][ \"state\" ] = ServiceState . STARTING logger . warning ( f \" { service_id } is not responding.\" ) await asyncio . sleep ( 1 ) await self . confirm_state_coroutine ( service_id ) return response = None while response is None : try : response = await socket . recv_string () except zmq . Again : self . services [ service_id ][ \"state\" ] = ServiceState . STARTING logger . warning ( f \" { service_id } is not responding.\" ) await asyncio . sleep ( 1 ) continue resp_dict = json . loads ( response ) if resp_dict [ \"result\" ] == ServiceState . RUNNING : self . services [ service_id ][ \"state\" ] = ServiceState . RUNNING elif resp_dict [ \"result\" ] == ServiceState . STARTING : logger . info ( f \"Service { service_id } responds but still starting\" ) await asyncio . sleep ( 1 ) await self . confirm_state_coroutine ( service_id ) else : logger . warning ( f \"Service { service_id } failed to start and returned incorrect state.\" ) self . services [ service_id ][ \"state\" ] = ServiceState . ERROR def stop_service ( self , service_id ): \"\"\"Stop the service.\"\"\" service_id = self . metadata . resolve_service ( service_id , None ) self . check_service ( service_id ) if self . services [ service_id ][ \"state\" ] != ServiceState . RUNNING : raise ValueError ( f \"Service { service_id } is not running\" ) self . services [ service_id ][ \"socket\" ] . close () self . services [ service_id ][ \"socket\" ] = None self . services [ service_id ][ \"process\" ] . terminate () self . services [ service_id ][ \"process\" ] = None self . services [ service_id ][ \"state\" ] = ServiceState . STOPPED def restart_service ( self , service_id ): \"\"\"Restart the service.\"\"\" self . stop_service ( service_id ) self . start_service ( service_id ) def check_dependency ( self , service_id , dependency ): \"\"\"Check if the service has the dependency.\"\"\" service_id = self . metadata . resolve_service ( service_id , None ) self . metadata . services [ service_id ] . dependencies . append ( dependency ) self . metadata . check_dependency_cycles () __del__ () Stop all services. Source code in npc_engine\\server\\control_service.py 85 86 87 88 89 90 91 92 93 def __del__ ( self ): \"\"\"Stop all services.\"\"\" if hasattr ( self , \"services\" ): for service_id , service in self . services . items (): if service [ \"state\" ] == ServiceState . RUNNING : try : self . stop_service ( service_id ) except Exception : pass __init__ ( zmq_context , metadata_manager ) Initialize control service. Parameters: Name Type Description Default zmq_context zmq . asyncio . Context asyncio zmq context required server_port server port that will be passed to services for inter-communication required Source code in npc_engine\\server\\control_service.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def __init__ ( self , zmq_context : zmq . asyncio . Context , metadata_manager : MetadataManager , ) -> None : \"\"\"Initialize control service. Args: zmq_context: asyncio zmq context server_port: server port that will be passed to services for inter-communication \"\"\" self . server_port = metadata_manager . port self . metadata = metadata_manager self . control_dispatcher = Dispatcher () self . control_dispatcher . update ( { \"get_services_metadata\" : self . metadata . get_services_metadata , \"get_service_metadata\" : self . metadata . get_metadata , \"get_service_status\" : self . get_service_status , \"start_service\" : self . start_service , \"stop_service\" : self . stop_service , \"restart_service\" : self . restart_service , \"check_dependency\" : self . check_dependency , } ) self . zmq_context = zmq_context self . services = { service_id : { \"process\" : None , \"socket\" : None , \"state\" : ServiceState . STOPPED , } for service_id in self . metadata . services . keys () } check_dependency ( service_id , dependency ) Check if the service has the dependency. Source code in npc_engine\\server\\control_service.py 223 224 225 226 227 def check_dependency ( self , service_id , dependency ): \"\"\"Check if the service has the dependency.\"\"\" service_id = self . metadata . resolve_service ( service_id , None ) self . metadata . services [ service_id ] . dependencies . append ( dependency ) self . metadata . check_dependency_cycles () check_service ( service_id ) Check if the service process is running. Source code in npc_engine\\server\\control_service.py 120 121 122 123 124 125 126 127 128 129 130 131 132 def check_service ( self , service_id ): \"\"\"Check if the service process is running.\"\"\" if ( service_id != \"control\" and ( self . services [ service_id ][ \"state\" ] == ServiceState . RUNNING or self . services [ service_id ][ \"state\" ] == ServiceState . STARTING or self . services [ service_id ][ \"state\" ] == ServiceState . AWAITING ) and not self . services [ service_id ][ \"process\" ] . is_alive () ): self . services [ service_id ][ \"state\" ] = ServiceState . ERROR raise ValueError ( f \"Error in service { service_id } . Process is not alive.\" ) confirm_state_coroutine ( service_id ) async Confirm the state of the service. Source code in npc_engine\\server\\control_service.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 async def confirm_state_coroutine ( self , service_id ): \"\"\"Confirm the state of the service.\"\"\" request = json . dumps ({ \"jsonrpc\" : \"2.0\" , \"method\" : \"status\" , \"id\" : 1 }) socket = self . services [ service_id ][ \"socket\" ] try : await socket . send_string ( request ) except zmq . Again : self . services [ service_id ][ \"state\" ] = ServiceState . STARTING logger . warning ( f \" { service_id } is not responding.\" ) await asyncio . sleep ( 1 ) await self . confirm_state_coroutine ( service_id ) return response = None while response is None : try : response = await socket . recv_string () except zmq . Again : self . services [ service_id ][ \"state\" ] = ServiceState . STARTING logger . warning ( f \" { service_id } is not responding.\" ) await asyncio . sleep ( 1 ) continue resp_dict = json . loads ( response ) if resp_dict [ \"result\" ] == ServiceState . RUNNING : self . services [ service_id ][ \"state\" ] = ServiceState . RUNNING elif resp_dict [ \"result\" ] == ServiceState . STARTING : logger . info ( f \"Service { service_id } responds but still starting\" ) await asyncio . sleep ( 1 ) await self . confirm_state_coroutine ( service_id ) else : logger . warning ( f \"Service { service_id } failed to start and returned incorrect state.\" ) self . services [ service_id ][ \"state\" ] = ServiceState . ERROR get_service_status ( service_id ) Get the status of the service. Source code in npc_engine\\server\\control_service.py 134 135 136 137 138 def get_service_status ( self , service_id ): \"\"\"Get the status of the service.\"\"\" service_id = self . metadata . resolve_service ( service_id , None ) self . check_service ( service_id ) return self . services [ service_id ][ \"state\" ] handle_request ( address , request ) async Parse request string and route request to correct service. Parameters: Name Type Description Default address str address of the service (either model name or class name) required request str jsonRPC string required Returns: Name Type Description str str jsonRPC response Source code in npc_engine\\server\\control_service.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 async def handle_request ( self , address : str , request : str ) -> str : \"\"\"Parse request string and route request to correct service. Args: address (str): address of the service (either model name or class name) request (str): jsonRPC string Returns: str: jsonRPC response \"\"\" request_dict = json . loads ( request ) service_id = self . metadata . resolve_service ( address , request_dict [ \"method\" ]) self . check_service ( service_id ) logger . info ( f \"Request from { address } \\n Request: { request } \" ) if service_id == \"control\" : return JSONRPCResponseManager . handle ( request , self . control_dispatcher ) . json else : if self . services [ service_id ][ \"state\" ] != ServiceState . RUNNING : raise ValueError ( f \"Service { service_id } is not running\" ) else : socket = self . services [ service_id ][ \"socket\" ] await socket . send_string ( request ) response = await socket . recv_string () return response restart_service ( service_id ) Restart the service. Source code in npc_engine\\server\\control_service.py 218 219 220 221 def restart_service ( self , service_id ): \"\"\"Restart the service.\"\"\" self . stop_service ( service_id ) self . start_service ( service_id ) start_service ( service_id ) Start the service. Source code in npc_engine\\server\\control_service.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def start_service ( self , service_id ): \"\"\"Start the service.\"\"\" service_id = self . metadata . resolve_service ( service_id , None ) self . check_service ( service_id ) if self . services [ service_id ][ \"state\" ] == ServiceState . RUNNING : raise ValueError ( f \"Service { service_id } is already running\" ) process = Process ( target = service_process , args = ( self . metadata , service_id , logger ), daemon = True , ) process . start () self . services [ service_id ][ \"process\" ] = process self . services [ service_id ][ \"state\" ] = ServiceState . STARTING self . services [ service_id ][ \"socket\" ] = self . zmq_context . socket ( zmq . REQ ) self . services [ service_id ][ \"socket\" ] . setsockopt ( zmq . LINGER , 0 ) self . services [ service_id ][ \"socket\" ] . setsockopt ( zmq . RCVTIMEO , 10000 ) self . services [ service_id ][ \"socket\" ] . connect ( self . metadata . services [ service_id ] . uri ) try : asyncio . create_task ( self . confirm_state_coroutine ( service_id )) except RuntimeError : logger . warning ( \"Create task to confirm service state failed.\" + \" Probably asyncio loop is not running.\" + \" Trying to execute it via asyncio.run()\" ) asyncio . run ( self . confirm_state_coroutine ( service_id )) stop_service ( service_id ) Stop the service. Source code in npc_engine\\server\\control_service.py 206 207 208 209 210 211 212 213 214 215 216 def stop_service ( self , service_id ): \"\"\"Stop the service.\"\"\" service_id = self . metadata . resolve_service ( service_id , None ) self . check_service ( service_id ) if self . services [ service_id ][ \"state\" ] != ServiceState . RUNNING : raise ValueError ( f \"Service { service_id } is not running\" ) self . services [ service_id ][ \"socket\" ] . close () self . services [ service_id ][ \"socket\" ] = None self . services [ service_id ][ \"process\" ] . terminate () self . services [ service_id ][ \"process\" ] = None self . services [ service_id ][ \"state\" ] = ServiceState . STOPPED ServiceState Enum for the state of the service. Source code in npc_engine\\server\\control_service.py 14 15 16 17 18 19 20 21 22 class ServiceState : \"\"\"Enum for the state of the service.\"\"\" STARTING = \"starting\" RUNNING = \"running\" STOPPED = \"stopped\" AWAITING = \"awaiting\" TIMEOUT = \"timeout\" ERROR = \"error\" service_process ( metadata , service_id , logger ) Service subprocess function. Starts the service and runs it's loop. Source code in npc_engine\\server\\control_service.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def service_process ( metadata : MetadataManager , service_id : str , logger ) -> None : \"\"\"Service subprocess function. Starts the service and runs it's loop. \"\"\" set_logger ( logger ) context = zmq . Context () service = services . BaseService . create ( context , metadata . services [ service_id ] . path , metadata . services [ service_id ] . uri , service_id , ) service . loop () set_logger ( logger_ ) Set the logger for the service. Source code in npc_engine\\server\\control_service.py 25 26 27 28 def set_logger ( logger_ ): \"\"\"Set the logger for the service.\"\"\" global logger logger = logger_ metadata_manager Module that implements lifetime and discoverability of the services. MetadataManager Class that manages service name resolution and metadata. Source code in npc_engine\\server\\metadata_manager.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 class MetadataManager : \"\"\"Class that manages service name resolution and metadata.\"\"\" def __init__ ( self , path : str , port : str ): \"\"\"Create model manager and load models from the given path.\"\"\" self . services = self . _scan_path ( path ) self . models_path = path self . port = port def resolve_service ( self , id_or_type , method = None ): \"\"\"Resolve service id or type to service id.\"\"\" service_id = None if id_or_type == \"control\" : service_id = \"control\" else : if id_or_type in self . services : service_id = id_or_type if service_id is None : for service_key , service in self . services . items (): if service . type == id_or_type or service . api_name == id_or_type : service_id = service_key break if service_id is None and method is not None : service_id = self . resolve_by_method ( method ) elif service_id is None : raise ValueError ( f \"Service { id_or_type } not found\" ) return service_id def get_services_metadata ( self ): \"\"\"List the models in the folder.\"\"\" return [ self . get_metadata ( service ) for service in self . services ] def resolve_by_method ( self , method_name ): \"\"\"Resolve service id by method name.\"\"\" for service_id , service in self . services . items (): if method_name in service . api_methods : return service_id raise ValueError ( f \"Service with method { method_name } not found\" ) def check_dependency_cycles ( self ): \"\"\"Check if there are any dependency cycles.\"\"\" # build graph graph = {} for service_id , service in self . services . items (): graph [ service_id ] = [ self . resolve_service ( dep ) for dep in service . dependencies ] # Tarjan's algorithm visited = {} llvs = {} stack = [] sccs = [] self . idx = 0 for node in graph : if node not in visited : self . __scc ( graph , node , llvs , visited , stack , sccs ) cycles = [ scc for scc in sccs if len ( scc ) > 1 ] if len ( cycles ) > 0 : to_str = \" \\n \" . join ([ \" -> \" . join ( cycle + [ cycle [ 0 ]]) for cycle in cycles ]) raise ValueError ( f \"There are dependency cycles: { to_str } \" ) del self . idx def __scc ( self , graph , node , llvs , visited , stack , sccs ): visited [ node ] = self . idx llvs [ node ] = self . idx self . idx += 1 stack . append ( node ) for v in graph [ node ]: if v not in visited : self . __scc ( graph , v , llvs , visited , stack , sccs ) llvs [ node ] = min ( llvs [ node ], llvs [ v ]) elif v in stack : llvs [ node ] = min ( llvs [ node ], visited . get ( v , - 1 )) if llvs [ node ] == visited [ node ]: sccs . append ([ stack . pop ()]) while sccs [ - 1 ][ - 1 ] != node : sccs [ - 1 ] . append ( stack . pop ()) def _scan_path ( self , path : str ) -> Dict [ str , ServiceDescriptor ]: \"\"\"Scan services defined in the given path.\"\"\" norm_path = ntpath . normpath ( path ) . replace ( \" \\\\ \" , os . path . sep ) paths = [ f . path for f in os . scandir ( norm_path ) if f . is_dir () and os . path . exists ( os . path . join ( f , \"config.yml\" )) ] svcs = {} for path in paths : with open ( os . path . join ( path , \"config.yml\" )) as f : config_dict = yaml . safe_load ( f ) uri = build_ipc_uri ( os . path . basename ( path )) cls = getattr ( services , config_dict . get ( \"model_type\" , config_dict . get ( \"type\" , None )), ) svcs [ os . path . basename ( path )] = ServiceDescriptor ( id = os . path . basename ( path ), type = config_dict . get ( \"model_type\" , config_dict . get ( \"type\" , None )), path = path , uri = uri , api_name = cls . get_api_name (), api_methods = cls . API_METHODS , dependencies = [], ) return svcs def get_metadata ( self , service_id : str ) -> Dict [ str , str ]: \"\"\"Print the model from the path.\"\"\" service_id = self . resolve_service ( service_id ) config_path = os . path . join ( self . services [ service_id ] . path , \"config.yml\" ) readme_path = os . path . join ( self . services [ service_id ] . path , \"README.md\" ) with open ( config_path ) as f : config_dict = yaml . load ( f , Loader = yaml . Loader ) try : with open ( readme_path ) as f : readme = f . read () . split ( \"---\" )[ - 1 ] except FileNotFoundError : readme = \"\" cls = getattr ( services , config_dict . get ( \"model_type\" , config_dict . get ( \"type\" , None )), ) return { \"id\" : self . services [ service_id ] . id , \"service\" : self . services [ service_id ] . type , \"api_name\" : self . services [ service_id ] . api_name , \"path\" : self . services [ service_id ] . path , \"service_short_description\" : cls . models [ self . services [ service_id ] . type ] . __doc__ . split ( \" \\n\\n \" )[ 0 ], \"service_description\" : cls . models [ self . services [ service_id ] . type ] . __doc__ , \"readme\" : readme , } __init__ ( path , port ) Create model manager and load models from the given path. Source code in npc_engine\\server\\metadata_manager.py 22 23 24 25 26 def __init__ ( self , path : str , port : str ): \"\"\"Create model manager and load models from the given path.\"\"\" self . services = self . _scan_path ( path ) self . models_path = path self . port = port _scan_path ( path ) Scan services defined in the given path. Source code in npc_engine\\server\\metadata_manager.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def _scan_path ( self , path : str ) -> Dict [ str , ServiceDescriptor ]: \"\"\"Scan services defined in the given path.\"\"\" norm_path = ntpath . normpath ( path ) . replace ( \" \\\\ \" , os . path . sep ) paths = [ f . path for f in os . scandir ( norm_path ) if f . is_dir () and os . path . exists ( os . path . join ( f , \"config.yml\" )) ] svcs = {} for path in paths : with open ( os . path . join ( path , \"config.yml\" )) as f : config_dict = yaml . safe_load ( f ) uri = build_ipc_uri ( os . path . basename ( path )) cls = getattr ( services , config_dict . get ( \"model_type\" , config_dict . get ( \"type\" , None )), ) svcs [ os . path . basename ( path )] = ServiceDescriptor ( id = os . path . basename ( path ), type = config_dict . get ( \"model_type\" , config_dict . get ( \"type\" , None )), path = path , uri = uri , api_name = cls . get_api_name (), api_methods = cls . API_METHODS , dependencies = [], ) return svcs check_dependency_cycles () Check if there are any dependency cycles. Source code in npc_engine\\server\\metadata_manager.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def check_dependency_cycles ( self ): \"\"\"Check if there are any dependency cycles.\"\"\" # build graph graph = {} for service_id , service in self . services . items (): graph [ service_id ] = [ self . resolve_service ( dep ) for dep in service . dependencies ] # Tarjan's algorithm visited = {} llvs = {} stack = [] sccs = [] self . idx = 0 for node in graph : if node not in visited : self . __scc ( graph , node , llvs , visited , stack , sccs ) cycles = [ scc for scc in sccs if len ( scc ) > 1 ] if len ( cycles ) > 0 : to_str = \" \\n \" . join ([ \" -> \" . join ( cycle + [ cycle [ 0 ]]) for cycle in cycles ]) raise ValueError ( f \"There are dependency cycles: { to_str } \" ) del self . idx get_metadata ( service_id ) Print the model from the path. Source code in npc_engine\\server\\metadata_manager.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def get_metadata ( self , service_id : str ) -> Dict [ str , str ]: \"\"\"Print the model from the path.\"\"\" service_id = self . resolve_service ( service_id ) config_path = os . path . join ( self . services [ service_id ] . path , \"config.yml\" ) readme_path = os . path . join ( self . services [ service_id ] . path , \"README.md\" ) with open ( config_path ) as f : config_dict = yaml . load ( f , Loader = yaml . Loader ) try : with open ( readme_path ) as f : readme = f . read () . split ( \"---\" )[ - 1 ] except FileNotFoundError : readme = \"\" cls = getattr ( services , config_dict . get ( \"model_type\" , config_dict . get ( \"type\" , None )), ) return { \"id\" : self . services [ service_id ] . id , \"service\" : self . services [ service_id ] . type , \"api_name\" : self . services [ service_id ] . api_name , \"path\" : self . services [ service_id ] . path , \"service_short_description\" : cls . models [ self . services [ service_id ] . type ] . __doc__ . split ( \" \\n\\n \" )[ 0 ], \"service_description\" : cls . models [ self . services [ service_id ] . type ] . __doc__ , \"readme\" : readme , } get_services_metadata () List the models in the folder. Source code in npc_engine\\server\\metadata_manager.py 48 49 50 def get_services_metadata ( self ): \"\"\"List the models in the folder.\"\"\" return [ self . get_metadata ( service ) for service in self . services ] resolve_by_method ( method_name ) Resolve service id by method name. Source code in npc_engine\\server\\metadata_manager.py 52 53 54 55 56 57 def resolve_by_method ( self , method_name ): \"\"\"Resolve service id by method name.\"\"\" for service_id , service in self . services . items (): if method_name in service . api_methods : return service_id raise ValueError ( f \"Service with method { method_name } not found\" ) resolve_service ( id_or_type , method = None ) Resolve service id or type to service id. Source code in npc_engine\\server\\metadata_manager.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def resolve_service ( self , id_or_type , method = None ): \"\"\"Resolve service id or type to service id.\"\"\" service_id = None if id_or_type == \"control\" : service_id = \"control\" else : if id_or_type in self . services : service_id = id_or_type if service_id is None : for service_key , service in self . services . items (): if service . type == id_or_type or service . api_name == id_or_type : service_id = service_key break if service_id is None and method is not None : service_id = self . resolve_by_method ( method ) elif service_id is None : raise ValueError ( f \"Service { id_or_type } not found\" ) return service_id server Module that implements ZMQ server communication over JSON-RPC 2.0 (https://www.jsonrpc.org/specification). BaseServer Bases: ABC Base JSON RPC server. Source code in npc_engine\\server\\server.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 class BaseServer ( ABC ): \"\"\"Base JSON RPC server.\"\"\" def __init__ ( self , zmq_context : zmq . asyncio . Context , service_manager : ControlService , metadata : MetadataManager , start_services : bool = True , ): \"\"\"Initialize the server. Args: zmq_context: The ZMQ context. service_manager: Control service that manages services. metadata: Metadata manager. start_services: Start services on initialization. \"\"\" self . context = zmq_context if sys . platform == \"win32\" : asyncio . set_event_loop_policy ( asyncio . WindowsSelectorEventLoopPolicy ()) self . socket_ipc = self . context . socket ( zmq . ROUTER ) self . socket_ipc . setsockopt ( zmq . LINGER , 0 ) ipc_uri = build_ipc_uri ( \"self\" ) ipc_path = ipc_uri . replace ( \"ipc://\" , \"\" ) os . makedirs ( os . path . dirname ( ipc_path ), exist_ok = True ) self . socket_ipc . bind ( build_ipc_uri ( \"self\" )) self . metadata = metadata self . service_manager = service_manager self . start_services_flag = start_services @abstractmethod def run ( self ): \"\"\"Run the server.\"\"\" pass async def msg_loop ( self , socket : zmq . asyncio . Socket ): \"\"\"Asynchoriniously handle a request and reply.\"\"\" while True : address = await socket . recv () _ = await socket . recv () message = await socket . recv_string () logger . info ( f \"Received request to { address } : { message } \" ) asyncio . create_task ( self . handle_reply ( socket , address , message )) async def interrupt_loop ( self ): \"\"\"Handle interrupts loop.\"\"\" while True : await asyncio . sleep ( 1 ) async def start_services ( self ): \"\"\"Start all services.\"\"\" logger . info ( \"Starting services\" ) for service in self . service_manager . services : self . service_manager . start_service ( service ) async def handle_reply ( self , socket , address : str , message : str ): \"\"\"Handle message and reply.\"\"\" logging . info ( \"Handling reply\" ) start = time . time () try : address_str = address . decode ( \"utf-8\" ) except UnicodeDecodeError : address_str = address . hex () try : response = await self . service_manager . handle_request ( address_str , message ) except Exception as e : response = { \"code\" : - 32000 , \"message\" : f \"Internal error: { type ( e ) } { e } \" , \"data\" : tb . extract_tb ( e . __traceback__ ) . format () if hasattr ( e , \"__traceback__\" ) else None , } response = json . dumps ( response ) end = time . time () logger . info ( \"Handle message time: %d \" % ( end - start )) logger . info ( \"Message reply: %s \" % ( response )) # Send reply back to client await socket . send ( address , zmq . SNDMORE ) await socket . send_string ( \"\" , zmq . SNDMORE ) await socket . send_string ( response ) __init__ ( zmq_context , service_manager , metadata , start_services = True ) Initialize the server. Parameters: Name Type Description Default zmq_context zmq . asyncio . Context The ZMQ context. required service_manager ControlService Control service that manages services. required metadata MetadataManager Metadata manager. required start_services bool Start services on initialization. True Source code in npc_engine\\server\\server.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , zmq_context : zmq . asyncio . Context , service_manager : ControlService , metadata : MetadataManager , start_services : bool = True , ): \"\"\"Initialize the server. Args: zmq_context: The ZMQ context. service_manager: Control service that manages services. metadata: Metadata manager. start_services: Start services on initialization. \"\"\" self . context = zmq_context if sys . platform == \"win32\" : asyncio . set_event_loop_policy ( asyncio . WindowsSelectorEventLoopPolicy ()) self . socket_ipc = self . context . socket ( zmq . ROUTER ) self . socket_ipc . setsockopt ( zmq . LINGER , 0 ) ipc_uri = build_ipc_uri ( \"self\" ) ipc_path = ipc_uri . replace ( \"ipc://\" , \"\" ) os . makedirs ( os . path . dirname ( ipc_path ), exist_ok = True ) self . socket_ipc . bind ( build_ipc_uri ( \"self\" )) self . metadata = metadata self . service_manager = service_manager self . start_services_flag = start_services handle_reply ( socket , address , message ) async Handle message and reply. Source code in npc_engine\\server\\server.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 async def handle_reply ( self , socket , address : str , message : str ): \"\"\"Handle message and reply.\"\"\" logging . info ( \"Handling reply\" ) start = time . time () try : address_str = address . decode ( \"utf-8\" ) except UnicodeDecodeError : address_str = address . hex () try : response = await self . service_manager . handle_request ( address_str , message ) except Exception as e : response = { \"code\" : - 32000 , \"message\" : f \"Internal error: { type ( e ) } { e } \" , \"data\" : tb . extract_tb ( e . __traceback__ ) . format () if hasattr ( e , \"__traceback__\" ) else None , } response = json . dumps ( response ) end = time . time () logger . info ( \"Handle message time: %d \" % ( end - start )) logger . info ( \"Message reply: %s \" % ( response )) # Send reply back to client await socket . send ( address , zmq . SNDMORE ) await socket . send_string ( \"\" , zmq . SNDMORE ) await socket . send_string ( response ) interrupt_loop () async Handle interrupts loop. Source code in npc_engine\\server\\server.py 66 67 68 69 async def interrupt_loop ( self ): \"\"\"Handle interrupts loop.\"\"\" while True : await asyncio . sleep ( 1 ) msg_loop ( socket ) async Asynchoriniously handle a request and reply. Source code in npc_engine\\server\\server.py 57 58 59 60 61 62 63 64 async def msg_loop ( self , socket : zmq . asyncio . Socket ): \"\"\"Asynchoriniously handle a request and reply.\"\"\" while True : address = await socket . recv () _ = await socket . recv () message = await socket . recv_string () logger . info ( f \"Received request to { address } : { message } \" ) asyncio . create_task ( self . handle_reply ( socket , address , message )) run () abstractmethod Run the server. Source code in npc_engine\\server\\server.py 52 53 54 55 @abstractmethod def run ( self ): \"\"\"Run the server.\"\"\" pass start_services () async Start all services. Source code in npc_engine\\server\\server.py 71 72 73 74 75 async def start_services ( self ): \"\"\"Start all services.\"\"\" logger . info ( \"Starting services\" ) for service in self . service_manager . services : self . service_manager . start_service ( service ) HTTPServer Bases: BaseServer HTTP server to handle requests. Source code in npc_engine\\server\\server.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 class HTTPServer ( BaseServer ): \"\"\"HTTP server to handle requests.\"\"\" def __init__ ( self , zmq_context : zmq . asyncio . Context , service_manager : ControlService , metadata : MetadataManager , start_services : bool = True , ): \"\"\"Create a server on the port.\"\"\" super () . __init__ ( zmq_context , service_manager , metadata , start_services ) self . app = web . Application () self . app . router . add_get ( \"/\" , self . handle_request ) self . app . router . add_get ( \"/ {name} \" , self . handle_request ) self . app . router . add_post ( \"/\" , self . handle_request ) self . app . router . add_post ( \"/ {name} \" , self . handle_request ) async def add_msg_loop_ipc ( self , app ): \"\"\"Add message loop for IPC.\"\"\" app [ \"msg_loop_ipc\" ] = asyncio . create_task ( self . msg_loop ( self . socket_ipc )) async def add_interrupt_loop ( self , app ): \"\"\"Add interrupt loop.\"\"\" app [ \"interrupt_loop\" ] = asyncio . create_task ( self . interrupt_loop ()) def run ( self ): \"\"\"Run an npc-engine json rpc server and start listening.\"\"\" if self . start_services_flag : self . app . on_startup . append ( lambda _ : asyncio . create_task ( self . start_services ()) ) self . app . on_startup . append ( self . add_msg_loop_ipc ) self . app . on_startup . append ( self . add_interrupt_loop ) logger . info ( \"Starting server\" ) web . run_app ( self . app , host = \"localhost\" , port = int ( self . metadata . port )) async def handle_request ( self , request ): \"\"\"Handle request.\"\"\" try : address = request . match_info . get ( \"name\" , \"xxxxxxxxxxxx\" ) message = await request . text () logger . info ( f \"Received request to { address } : { message } \" ) response = await self . service_manager . handle_request ( address , message ) except Exception as e : response = { \"code\" : - 32000 , \"message\" : f \"Internal error: { type ( e ) } { e } \" , \"data\" : tb . extract_tb ( e . __traceback__ ) . format () if hasattr ( e , \"__traceback__\" ) else None , } response = json . dumps ( response ) return web . json_response ( text = response ) __init__ ( zmq_context , service_manager , metadata , start_services = True ) Create a server on the port. Source code in npc_engine\\server\\server.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def __init__ ( self , zmq_context : zmq . asyncio . Context , service_manager : ControlService , metadata : MetadataManager , start_services : bool = True , ): \"\"\"Create a server on the port.\"\"\" super () . __init__ ( zmq_context , service_manager , metadata , start_services ) self . app = web . Application () self . app . router . add_get ( \"/\" , self . handle_request ) self . app . router . add_get ( \"/ {name} \" , self . handle_request ) self . app . router . add_post ( \"/\" , self . handle_request ) self . app . router . add_post ( \"/ {name} \" , self . handle_request ) add_interrupt_loop ( app ) async Add interrupt loop. Source code in npc_engine\\server\\server.py 173 174 175 async def add_interrupt_loop ( self , app ): \"\"\"Add interrupt loop.\"\"\" app [ \"interrupt_loop\" ] = asyncio . create_task ( self . interrupt_loop ()) add_msg_loop_ipc ( app ) async Add message loop for IPC. Source code in npc_engine\\server\\server.py 169 170 171 async def add_msg_loop_ipc ( self , app ): \"\"\"Add message loop for IPC.\"\"\" app [ \"msg_loop_ipc\" ] = asyncio . create_task ( self . msg_loop ( self . socket_ipc )) handle_request ( request ) async Handle request. Source code in npc_engine\\server\\server.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 async def handle_request ( self , request ): \"\"\"Handle request.\"\"\" try : address = request . match_info . get ( \"name\" , \"xxxxxxxxxxxx\" ) message = await request . text () logger . info ( f \"Received request to { address } : { message } \" ) response = await self . service_manager . handle_request ( address , message ) except Exception as e : response = { \"code\" : - 32000 , \"message\" : f \"Internal error: { type ( e ) } { e } \" , \"data\" : tb . extract_tb ( e . __traceback__ ) . format () if hasattr ( e , \"__traceback__\" ) else None , } response = json . dumps ( response ) return web . json_response ( text = response ) run () Run an npc-engine json rpc server and start listening. Source code in npc_engine\\server\\server.py 177 178 179 180 181 182 183 184 185 186 def run ( self ): \"\"\"Run an npc-engine json rpc server and start listening.\"\"\" if self . start_services_flag : self . app . on_startup . append ( lambda _ : asyncio . create_task ( self . start_services ()) ) self . app . on_startup . append ( self . add_msg_loop_ipc ) self . app . on_startup . append ( self . add_interrupt_loop ) logger . info ( \"Starting server\" ) web . run_app ( self . app , host = \"localhost\" , port = int ( self . metadata . port )) ZMQServer Bases: BaseServer Json rpc server over zmq. Source code in npc_engine\\server\\server.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 class ZMQServer ( BaseServer ): \"\"\"Json rpc server over zmq.\"\"\" def __init__ ( self , zmq_context : zmq . asyncio . Context , service_manager : ControlService , metadata : MetadataManager , start_services : bool = True , ): \"\"\"Create a server on the port.\"\"\" super () . __init__ ( zmq_context , service_manager , metadata , start_services ) self . socket = self . context . socket ( zmq . ROUTER ) self . socket . setsockopt ( zmq . LINGER , 0 ) self . socket . bind ( f \"tcp://*: { metadata . port } \" ) def run ( self ): \"\"\"Run an npc-engine json rpc server and start listening.\"\"\" logger . info ( \"Starting server\" ) asyncio . get_event_loop () . run_until_complete ( self . loop ()) async def loop ( self ): \"\"\"Run the server loop.\"\"\" try : if self . start_services_flag : await asyncio . create_task ( self . start_services ()) logger . info ( \"Starting message loop\" ) await asyncio . gather ( self . msg_loop ( self . socket ), self . msg_loop ( self . socket_ipc ), self . interrupt_loop (), ) except Exception as e : logger . error ( f \"Error in message loop: { e } \" ) logger . error ( tb . format_exc ()) raise e finally : logger . info ( \"Closing message loop\" ) self . socket . close () self . socket_ipc . close () self . context . destroy () __init__ ( zmq_context , service_manager , metadata , start_services = True ) Create a server on the port. Source code in npc_engine\\server\\server.py 110 111 112 113 114 115 116 117 118 119 120 121 def __init__ ( self , zmq_context : zmq . asyncio . Context , service_manager : ControlService , metadata : MetadataManager , start_services : bool = True , ): \"\"\"Create a server on the port.\"\"\" super () . __init__ ( zmq_context , service_manager , metadata , start_services ) self . socket = self . context . socket ( zmq . ROUTER ) self . socket . setsockopt ( zmq . LINGER , 0 ) self . socket . bind ( f \"tcp://*: { metadata . port } \" ) loop () async Run the server loop. Source code in npc_engine\\server\\server.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 async def loop ( self ): \"\"\"Run the server loop.\"\"\" try : if self . start_services_flag : await asyncio . create_task ( self . start_services ()) logger . info ( \"Starting message loop\" ) await asyncio . gather ( self . msg_loop ( self . socket ), self . msg_loop ( self . socket_ipc ), self . interrupt_loop (), ) except Exception as e : logger . error ( f \"Error in message loop: { e } \" ) logger . error ( tb . format_exc ()) raise e finally : logger . info ( \"Closing message loop\" ) self . socket . close () self . socket_ipc . close () self . context . destroy () run () Run an npc-engine json rpc server and start listening. Source code in npc_engine\\server\\server.py 123 124 125 126 def run ( self ): \"\"\"Run an npc-engine json rpc server and start listening.\"\"\" logger . info ( \"Starting server\" ) asyncio . get_event_loop () . run_until_complete ( self . loop ()) utils Utility functions for RPC communication. build_ipc_uri ( service_id ) Build ipc uri for the given service. Source code in npc_engine\\server\\utils.py 44 45 46 def build_ipc_uri ( service_id : str ) -> str : \"\"\"Build ipc uri for the given service.\"\"\" return f \"ipc:// { os . path . join ( user_cache_dir ( 'npc-engine' ), service_id ) } \" schema_to_json ( s , fill_value = lambda _ : '' ) Iterate the schema and return simplified dictionary. Source code in npc_engine\\server\\utils.py 9 10 11 12 13 14 15 16 17 18 19 20 def schema_to_json ( s : Dict [ str , Any ], fill_value : Callable [[ str ], Any ] = lambda _ : \"\" ) -> Dict [ str , Any ]: \"\"\"Iterate the schema and return simplified dictionary.\"\"\" if \"type\" not in s and \"anyOf\" in s : return fill_value ( s [ \"title\" ]) elif \"type\" in s and s [ \"type\" ] == \"object\" : return { k : schema_to_json ( v ) for k , v in s [ \"properties\" ] . items ()} elif \"type\" in s and s [ \"type\" ] == \"array\" : return [ schema_to_json ( s [ \"items\" ])] else : raise ValueError ( f \"Unknown schema type: { s } \" ) start_test_server ( port , models_path ) Start the test server. Parameters: Name Type Description Default port str The port to start the server on. required models_path str The path to the models. required Source code in npc_engine\\server\\utils.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def start_test_server ( port : str , models_path : str ): # pragma: no cover \"\"\"Start the test server. Args: port: The port to start the server on. models_path: The path to the models. \"\"\" subprocess . Popen ( [ \"npc-engine\" , \"--verbose\" , \"run\" , \"--port\" , port , \"--models-path\" , models_path , ], creationflags = subprocess . CREATE_NEW_CONSOLE , ) service_clients Module implementing the clients for services. control_client Control interface client implementation. ControlClient Bases: ServiceClient Json rpc client for control requests. Source code in npc_engine\\service_clients\\control_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class ControlClient ( ServiceClient ): \"\"\"Json rpc client for control requests.\"\"\" def __init__ ( self , zmq_context : zmq . Context ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , \"control\" ) def start_service ( self , service_id ): \"\"\"Send a start service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"start_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) def stop_service ( self , service_id ): \"\"\"Send a stop service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"stop_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) def get_service_status ( self , service_id ) -> str : \"\"\"Send a get service status request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_service_status\" , \"id\" : 0 , \"params\" : [ service_id ], } return self . send_request ( request ) def restart_service ( self , service_id ): \"\"\"Send a restart service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"restart_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) def get_services_metadata ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Send a get services metadata request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_services_metadata\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) def get_service_metadata ( self , service_id : str ) -> Dict [ str , Any ]: \"\"\"Send a get services metadata request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_service_metadata\" , \"id\" : 0 , \"params\" : [ service_id ], } return self . send_request ( request ) def check_dependency ( self , service_id , dependency_id ) -> bool : \"\"\"Send a check dependency request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"check_dependency\" , \"id\" : 0 , \"params\" : [ service_id , dependency_id ], } return self . send_request ( request ) @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"control\" __init__ ( zmq_context ) Connect to the server on the port. Source code in npc_engine\\service_clients\\control_client.py 10 11 12 def __init__ ( self , zmq_context : zmq . Context ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , \"control\" ) check_dependency ( service_id , dependency_id ) Send a check dependency request to the server. Source code in npc_engine\\service_clients\\control_client.py 74 75 76 77 78 79 80 81 82 def check_dependency ( self , service_id , dependency_id ) -> bool : \"\"\"Send a check dependency request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"check_dependency\" , \"id\" : 0 , \"params\" : [ service_id , dependency_id ], } return self . send_request ( request ) get_api_name () classmethod Return the name of the API. Source code in npc_engine\\service_clients\\control_client.py 84 85 86 87 @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"control\" get_service_metadata ( service_id ) Send a get services metadata request to the server. Source code in npc_engine\\service_clients\\control_client.py 64 65 66 67 68 69 70 71 72 def get_service_metadata ( self , service_id : str ) -> Dict [ str , Any ]: \"\"\"Send a get services metadata request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_service_metadata\" , \"id\" : 0 , \"params\" : [ service_id ], } return self . send_request ( request ) get_service_status ( service_id ) Send a get service status request to the server. Source code in npc_engine\\service_clients\\control_client.py 34 35 36 37 38 39 40 41 42 def get_service_status ( self , service_id ) -> str : \"\"\"Send a get service status request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_service_status\" , \"id\" : 0 , \"params\" : [ service_id ], } return self . send_request ( request ) get_services_metadata () Send a get services metadata request to the server. Source code in npc_engine\\service_clients\\control_client.py 54 55 56 57 58 59 60 61 62 def get_services_metadata ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Send a get services metadata request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_services_metadata\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) restart_service ( service_id ) Send a restart service request to the server. Source code in npc_engine\\service_clients\\control_client.py 44 45 46 47 48 49 50 51 52 def restart_service ( self , service_id ): \"\"\"Send a restart service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"restart_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) start_service ( service_id ) Send a start service request to the server. Source code in npc_engine\\service_clients\\control_client.py 14 15 16 17 18 19 20 21 22 def start_service ( self , service_id ): \"\"\"Send a start service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"start_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) stop_service ( service_id ) Send a stop service request to the server. Source code in npc_engine\\service_clients\\control_client.py 24 25 26 27 28 29 30 31 32 def stop_service ( self , service_id ): \"\"\"Send a stop service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"stop_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) sequence_classifier_client Huggingface sequence classifier interface client implementation. SequenceClassifierClient Bases: ServiceClient Json rpc client for sequence classifier service. Source code in npc_engine\\service_clients\\sequence_classifier_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class SequenceClassifierClient ( ServiceClient ): \"\"\"Json rpc client for sequence classifier service.\"\"\" def __init__ ( self , zmq_context : zmq . Context , service_id : str = \"SequenceClassifierAPI\" , ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , service_id ) def classify ( self , texts : List [ Union [ str , Tuple [ str , str ]]]) -> str : \"\"\"Send a classify request to the SequenceClassifierAPI. Args: texts: Batch of texts to classify. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"classify\" , \"id\" : 0 , \"params\" : [ texts ], } reply = self . send_request ( request ) return reply @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"SequenceClassifierAPI\" __init__ ( zmq_context , service_id = 'SequenceClassifierAPI' ) Connect to the server on the port. Source code in npc_engine\\service_clients\\sequence_classifier_client.py 10 11 12 13 14 15 16 def __init__ ( self , zmq_context : zmq . Context , service_id : str = \"SequenceClassifierAPI\" , ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , service_id ) classify ( texts ) Send a classify request to the SequenceClassifierAPI. Parameters: Name Type Description Default texts List [ Union [ str , Tuple [ str , str ]]] Batch of texts to classify. required Source code in npc_engine\\service_clients\\sequence_classifier_client.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def classify ( self , texts : List [ Union [ str , Tuple [ str , str ]]]) -> str : \"\"\"Send a classify request to the SequenceClassifierAPI. Args: texts: Batch of texts to classify. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"classify\" , \"id\" : 0 , \"params\" : [ texts ], } reply = self . send_request ( request ) return reply get_api_name () classmethod Return the name of the API. Source code in npc_engine\\service_clients\\sequence_classifier_client.py 33 34 35 36 @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"SequenceClassifierAPI\" service_client Module that implements ZMQ base client communication over JSON-RPC 2.0 (https://www.jsonrpc.org/specification). ServiceClient Bases: ABC Base json rpc client. Source code in npc_engine\\service_clients\\service_client.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class ServiceClient ( ABC ): \"\"\"Base json rpc client.\"\"\" clients = {} def __init_subclass__ ( cls , ** kwargs ): \"\"\"Init subclass where service classes get registered to be discovered.\"\"\" super () . __init_subclass__ ( ** kwargs ) cls . clients [ cls . get_api_name ()] = cls def __init__ ( self , zmq_context : zmq . Context , service_id : str = None ): \"\"\"Connect to the server on the port.\"\"\" self . context = zmq_context self . socket = self . context . socket ( zmq . REQ ) self . socket . setsockopt ( zmq . LINGER , 0 ) self . socket . setsockopt ( zmq . IDENTITY , service_id . encode ( \"utf-8\" ) if service_id else self . get_api_name () . encode ( \"utf-8\" ), ) self . socket . connect ( build_ipc_uri ( \"self\" )) logger . info ( \"Connected to server\" ) def send_request ( self , request : Dict [ str , Any ]) -> Any : \"\"\"Send request to the server and return the response. Args: request: The request to send to the server. Returns: The result from the server. \"\"\" logger . trace ( f \"Sending request: { request } \" ) self . socket . send_json ( request ) response = self . socket . recv_json () logger . trace ( f \"Received response: { response } \" ) if \"result\" in response : return response [ \"result\" ] elif \"code\" in response : raise RuntimeError ( f \"code: { response [ 'code' ] } . { response [ 'message' ] } \" ) @abstractclassmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" pass @classmethod def get_api_client ( cls , api_name : str ) -> \"ServiceClient\" : \"\"\"Return the client for the api.\"\"\" try : return cls . clients [ api_name ] except KeyError : raise RuntimeError ( f \"Client for the API { api_name } not found. Available clients: { list ( cls . clients . keys ()) } \" ) __init__ ( zmq_context , service_id = None ) Connect to the server on the port. Source code in npc_engine\\service_clients\\service_client.py 22 23 24 25 26 27 28 29 30 31 32 33 34 def __init__ ( self , zmq_context : zmq . Context , service_id : str = None ): \"\"\"Connect to the server on the port.\"\"\" self . context = zmq_context self . socket = self . context . socket ( zmq . REQ ) self . socket . setsockopt ( zmq . LINGER , 0 ) self . socket . setsockopt ( zmq . IDENTITY , service_id . encode ( \"utf-8\" ) if service_id else self . get_api_name () . encode ( \"utf-8\" ), ) self . socket . connect ( build_ipc_uri ( \"self\" )) logger . info ( \"Connected to server\" ) __init_subclass__ ( ** kwargs ) Init subclass where service classes get registered to be discovered. Source code in npc_engine\\service_clients\\service_client.py 17 18 19 20 def __init_subclass__ ( cls , ** kwargs ): \"\"\"Init subclass where service classes get registered to be discovered.\"\"\" super () . __init_subclass__ ( ** kwargs ) cls . clients [ cls . get_api_name ()] = cls get_api_client ( api_name ) classmethod Return the client for the api. Source code in npc_engine\\service_clients\\service_client.py 59 60 61 62 63 64 65 66 67 @classmethod def get_api_client ( cls , api_name : str ) -> \"ServiceClient\" : \"\"\"Return the client for the api.\"\"\" try : return cls . clients [ api_name ] except KeyError : raise RuntimeError ( f \"Client for the API { api_name } not found. Available clients: { list ( cls . clients . keys ()) } \" ) get_api_name () Return the name of the API. Source code in npc_engine\\service_clients\\service_client.py 54 55 56 57 @abstractclassmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" pass send_request ( request ) Send request to the server and return the response. Parameters: Name Type Description Default request Dict [ str , Any ] The request to send to the server. required Returns: Type Description Any The result from the server. Source code in npc_engine\\service_clients\\service_client.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def send_request ( self , request : Dict [ str , Any ]) -> Any : \"\"\"Send request to the server and return the response. Args: request: The request to send to the server. Returns: The result from the server. \"\"\" logger . trace ( f \"Sending request: { request } \" ) self . socket . send_json ( request ) response = self . socket . recv_json () logger . trace ( f \"Received response: { response } \" ) if \"result\" in response : return response [ \"result\" ] elif \"code\" in response : raise RuntimeError ( f \"code: { response [ 'code' ] } . { response [ 'message' ] } \" ) similarity_client Huggingface semantic similarity interface client implementation. SimilarityClient Bases: ServiceClient Json rpc client for semantic similarity service. Source code in npc_engine\\service_clients\\similarity_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class SimilarityClient ( ServiceClient ): \"\"\"Json rpc client for semantic similarity service.\"\"\" def __init__ ( self , zmq_context : zmq . Context , service_id : str = \"SimilarityAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , service_id ) def compare ( self , query : str , context : List [ str ]) -> float : \"\"\"Send a comparison request to the server. Args: query: A string to compute similarity with contexts. context: A list of strings to compute similiarity with query. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"compare\" , \"id\" : 0 , \"params\" : [ query , context ], } reply = self . send_request ( request ) return reply @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"SimilarityAPI\" __init__ ( zmq_context , service_id = 'SimilarityAPI' ) Connect to the server on the port. Source code in npc_engine\\service_clients\\similarity_client.py 10 11 12 def __init__ ( self , zmq_context : zmq . Context , service_id : str = \"SimilarityAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , service_id ) compare ( query , context ) Send a comparison request to the server. Parameters: Name Type Description Default query str A string to compute similarity with contexts. required context List [ str ] A list of strings to compute similiarity with query. required Source code in npc_engine\\service_clients\\similarity_client.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def compare ( self , query : str , context : List [ str ]) -> float : \"\"\"Send a comparison request to the server. Args: query: A string to compute similarity with contexts. context: A list of strings to compute similiarity with query. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"compare\" , \"id\" : 0 , \"params\" : [ query , context ], } reply = self . send_request ( request ) return reply get_api_name () classmethod Return the name of the API. Source code in npc_engine\\service_clients\\similarity_client.py 30 31 32 33 @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"SimilarityAPI\" text_generation_client Huggingface chatbot interface client implementation. TextGenerationClient Bases: ServiceClient Json rpc client for chatbot service. Source code in npc_engine\\service_clients\\text_generation_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 class TextGenerationClient ( ServiceClient ): \"\"\"Json rpc client for chatbot service.\"\"\" def __init__ ( self , zmq_context : zmq . Context , service_id : str = \"TextGenerationAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , service_id ) def generate_reply ( self , context : Dict [ str , Any ]) -> str : \"\"\"Send a chatbot request to the server. Args: context: A dictionary containing the chatbot request. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"generate_reply\" , \"id\" : 0 , \"params\" : [ context ], } reply = self . send_request ( request ) return reply def get_prompt_template ( self ) -> str : \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_prompt_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) def get_context_template ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_context_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) def get_special_tokens ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_special_tokens\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"TextGenerationAPI\" __init__ ( zmq_context , service_id = 'TextGenerationAPI' ) Connect to the server on the port. Source code in npc_engine\\service_clients\\text_generation_client.py 10 11 12 def __init__ ( self , zmq_context : zmq . Context , service_id : str = \"TextGenerationAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , service_id ) generate_reply ( context ) Send a chatbot request to the server. Parameters: Name Type Description Default context Dict [ str , Any ] A dictionary containing the chatbot request. required Source code in npc_engine\\service_clients\\text_generation_client.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def generate_reply ( self , context : Dict [ str , Any ]) -> str : \"\"\"Send a chatbot request to the server. Args: context: A dictionary containing the chatbot request. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"generate_reply\" , \"id\" : 0 , \"params\" : [ context ], } reply = self . send_request ( request ) return reply get_api_name () classmethod Return the name of the API. Source code in npc_engine\\service_clients\\text_generation_client.py 59 60 61 62 @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"TextGenerationAPI\" get_context_template () Send a chatbot request to the server. Source code in npc_engine\\service_clients\\text_generation_client.py 39 40 41 42 43 44 45 46 47 def get_context_template ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_context_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) get_prompt_template () Send a chatbot request to the server. Source code in npc_engine\\service_clients\\text_generation_client.py 29 30 31 32 33 34 35 36 37 def get_prompt_template ( self ) -> str : \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_prompt_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) get_special_tokens () Send a chatbot request to the server. Source code in npc_engine\\service_clients\\text_generation_client.py 49 50 51 52 53 54 55 56 57 def get_special_tokens ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_special_tokens\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) services Module that contains everything related to deep learning models. For your model API to be discovered it must be imported here base_service Module with Model base class. BaseService Bases: FactoryMixin , ABC Abstract base class for managed services. Source code in npc_engine\\services\\base_service.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 class BaseService ( FactoryMixin , ABC ): \"\"\"Abstract base class for managed services.\"\"\" def __init__ ( self , service_id : str , context : zmq . Context , uri : str , providers : List [ str ] = None , * args , ** kwargs , ): \"\"\"Initialize the service. Args: context (zmq.Context): ZMQ context uri (str): URI to serve requests to dependency_clients (list(ServiceClient)): List of dependency clients \"\"\" super ( BaseService , self ) . __init__ () self . zmq_context = context self . socket = context . socket ( zmq . REP ) self . socket . setsockopt ( zmq . LINGER , 0 ) if uri . startswith ( \"ipc://\" ): os . makedirs ( Path ( uri . replace ( \"ipc://\" , \"\" )) . parent , exist_ok = True ) os . chmod ( Path ( uri . replace ( \"ipc://\" , \"\" )) . parent , 777 ) self . socket . bind ( uri ) self . service_id = service_id self . control_client = None self . _set_and_validate_providers ( providers ) @classmethod @abstractmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" pass def create_client ( self , name : str ): \"\"\"Get a dependency client by name to use it in service logic. Args: name (str): Name of the dependency Returns: ServiceClient: Client for the dependency \"\"\" if self . control_client is None : self . control_client = ControlClient ( self . zmq_context ) if name == \"control\" : return self . control_client self . control_client . check_dependency ( self . service_id , name ) api_name = self . control_client . get_service_metadata ( name )[ \"api_name\" ] return ControlClient . get_api_client ( api_name )( self . zmq_context , name ) def loop ( self ): \"\"\"Run service main loop that accepts json rpc.\"\"\" try : dispatcher = Dispatcher () dispatcher . update ( self . build_api_dict ()) dispatcher . update ({ \"status\" : self . status }) while True : request = self . socket . recv_string () response = JSONRPCResponseManager . handle ( request , dispatcher ) self . socket . send_string ( response . json ) except Exception as e : logger . exception ( e ) raise e finally : self . socket . close () self . zmq_context . destroy () def status ( self ): \"\"\"Return status of the service. Returns: ServiceState \"\"\" from npc_engine.server.control_service import ServiceState return ServiceState . RUNNING def build_api_dict ( self ) -> Dict [ str , Callable ]: \"\"\"Build api dict. Returns: dict(str,str): Mapping \"method_name\" -> callable that will be exposed to API \"\"\" api_dict = {} for method in type ( self ) . API_METHODS : logger . info ( f \"Registering method { method } for service { type ( self ) . __name__ } \" ) api_dict [ method ] = getattr ( self , method ) return api_dict def get_providers ( self ) -> List [ str ]: \"\"\"Return onnxruntime providers to use.\"\"\" return self . providers def _set_and_validate_providers ( self , providers : List [ str ]): if providers is not None : self . providers = providers for provider in self . providers : if provider == \"gpu\" : provider = [ prov for prov in rt . get_available_providers () if \"DML\" in prov or \"CUDA\" in prov or \"Tensorrt\" in prov ][ 0 ] if provider == \"cpu\" : provider = [ prov for prov in rt . get_available_providers () if \"CPU\" in prov ][ 0 ] if provider not in rt . get_available_providers (): raise RuntimeError ( f \"Provider { provider } is not available\" ) else : self . providers = [ \"DmlExecutionProvider\" ] __init__ ( service_id , context , uri , providers = None , * args , ** kwargs ) Initialize the service. Parameters: Name Type Description Default context zmq . Context ZMQ context required uri str URI to serve requests to required dependency_clients list(ServiceClient List of dependency clients required Source code in npc_engine\\services\\base_service.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , service_id : str , context : zmq . Context , uri : str , providers : List [ str ] = None , * args , ** kwargs , ): \"\"\"Initialize the service. Args: context (zmq.Context): ZMQ context uri (str): URI to serve requests to dependency_clients (list(ServiceClient)): List of dependency clients \"\"\" super ( BaseService , self ) . __init__ () self . zmq_context = context self . socket = context . socket ( zmq . REP ) self . socket . setsockopt ( zmq . LINGER , 0 ) if uri . startswith ( \"ipc://\" ): os . makedirs ( Path ( uri . replace ( \"ipc://\" , \"\" )) . parent , exist_ok = True ) os . chmod ( Path ( uri . replace ( \"ipc://\" , \"\" )) . parent , 777 ) self . socket . bind ( uri ) self . service_id = service_id self . control_client = None self . _set_and_validate_providers ( providers ) build_api_dict () Build api dict. Returns: Name Type Description dict str , str Mapping \"method_name\" -> callable that will be exposed to API Source code in npc_engine\\services\\base_service.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def build_api_dict ( self ) -> Dict [ str , Callable ]: \"\"\"Build api dict. Returns: dict(str,str): Mapping \"method_name\" -> callable that will be exposed to API \"\"\" api_dict = {} for method in type ( self ) . API_METHODS : logger . info ( f \"Registering method { method } for service { type ( self ) . __name__ } \" ) api_dict [ method ] = getattr ( self , method ) return api_dict create_client ( name ) Get a dependency client by name to use it in service logic. Parameters: Name Type Description Default name str Name of the dependency required Returns: Name Type Description ServiceClient Client for the dependency Source code in npc_engine\\services\\base_service.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def create_client ( self , name : str ): \"\"\"Get a dependency client by name to use it in service logic. Args: name (str): Name of the dependency Returns: ServiceClient: Client for the dependency \"\"\" if self . control_client is None : self . control_client = ControlClient ( self . zmq_context ) if name == \"control\" : return self . control_client self . control_client . check_dependency ( self . service_id , name ) api_name = self . control_client . get_service_metadata ( name )[ \"api_name\" ] return ControlClient . get_api_client ( api_name )( self . zmq_context , name ) get_api_name () classmethod abstractmethod Return the name of the API. Source code in npc_engine\\services\\base_service.py 45 46 47 48 49 @classmethod @abstractmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" pass get_providers () Return onnxruntime providers to use. Source code in npc_engine\\services\\base_service.py 110 111 112 def get_providers ( self ) -> List [ str ]: \"\"\"Return onnxruntime providers to use.\"\"\" return self . providers loop () Run service main loop that accepts json rpc. Source code in npc_engine\\services\\base_service.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def loop ( self ): \"\"\"Run service main loop that accepts json rpc.\"\"\" try : dispatcher = Dispatcher () dispatcher . update ( self . build_api_dict ()) dispatcher . update ({ \"status\" : self . status }) while True : request = self . socket . recv_string () response = JSONRPCResponseManager . handle ( request , dispatcher ) self . socket . send_string ( response . json ) except Exception as e : logger . exception ( e ) raise e finally : self . socket . close () self . zmq_context . destroy () status () Return status of the service. Returns: Type Description ServiceState Source code in npc_engine\\services\\base_service.py 85 86 87 88 89 90 91 92 93 def status ( self ): \"\"\"Return status of the service. Returns: ServiceState \"\"\" from npc_engine.server.control_service import ServiceState return ServiceState . RUNNING factory_mixin Factory mixin for service. FactoryMixin Mixin base class for services that can be created via create method. Source code in npc_engine\\services\\factory_mixin.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class FactoryMixin : \"\"\"Mixin base class for services that can be created via create method.\"\"\" models = {} def __init_subclass__ ( cls , ** kwargs ): \"\"\"Init subclass where service classes get registered to be discovered.\"\"\" super () . __init_subclass__ ( ** kwargs ) cls . models [ cls . __name__ ] = cls @classmethod def create ( cls , context : zmq . Context , path : str , uri : str , service_id : str ): \"\"\"Create a service from the path. Args: context (zmq.Context): ZMQ context path (str): Path to the service uri (str): URI to serve requests to Returns: Service: Service instance \"\"\" config_path = os . path . join ( path , \"config.yml\" ) with open ( config_path ) as f : config_dict = yaml . load ( f , Loader = yaml . Loader ) config_dict [ \"model_path\" ] = path model_cls = cls . models [ get_type_from_dict ( config_dict )] return model_cls ( ** config_dict , context = context , uri = uri , service_id = service_id ) __init_subclass__ ( ** kwargs ) Init subclass where service classes get registered to be discovered. Source code in npc_engine\\services\\factory_mixin.py 13 14 15 16 def __init_subclass__ ( cls , ** kwargs ): \"\"\"Init subclass where service classes get registered to be discovered.\"\"\" super () . __init_subclass__ ( ** kwargs ) cls . models [ cls . __name__ ] = cls create ( context , path , uri , service_id ) classmethod Create a service from the path. Parameters: Name Type Description Default context zmq . Context ZMQ context required path str Path to the service required uri str URI to serve requests to required Returns: Name Type Description Service Service instance Source code in npc_engine\\services\\factory_mixin.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 @classmethod def create ( cls , context : zmq . Context , path : str , uri : str , service_id : str ): \"\"\"Create a service from the path. Args: context (zmq.Context): ZMQ context path (str): Path to the service uri (str): URI to serve requests to Returns: Service: Service instance \"\"\" config_path = os . path . join ( path , \"config.yml\" ) with open ( config_path ) as f : config_dict = yaml . load ( f , Loader = yaml . Loader ) config_dict [ \"model_path\" ] = path model_cls = cls . models [ get_type_from_dict ( config_dict )] return model_cls ( ** config_dict , context = context , uri = uri , service_id = service_id ) persona_dialogue Module that implements persona dialogue API. persona_dialogue Persona Dialogue implementation using the provided services. PersonaDialogue Bases: PersonaDialogueAPI Persona dialogue API implementation using the provided services. Scripted utterances are matched via semantic similarity. Utterances are generated using the provided text generation service. Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 class PersonaDialogue ( PersonaDialogueAPI ): \"\"\"Persona dialogue API implementation using the provided services. Scripted utterances are matched via semantic similarity. Utterances are generated using the provided text generation service. \"\"\" def __init__ ( self , text_generation_svc : str = \"TextGenerationAPI\" , similarity_svc : str = \"SimilarityAPI\" , * args , ** kwargs , ): \"\"\"Initialize the persona dialogue API. Args: text_generation_service: Name of the text generation service. similarity_api: Name of the similarity API. similarity_threshold: Threshold for semantic similarity. \"\"\" super () . __init__ ( * args , ** kwargs ) self . text_generation_service = self . create_client ( text_generation_svc ) self . similarity_api = self . create_client ( similarity_svc ) self . dialogues = {} self . dialogue_id_counter = 0 self . context_template = self . text_generation_service . get_context_template () self . _validate_context_template ( self . context_template ) def start_dialogue ( self , name1 : str = None , persona1 : str = None , name2 : str = None , persona2 : str = None , location_name : str = None , location_description : str = None , dialogue_id : str = None , * args , ** kwargs , ) -> str : \"\"\"Start a dialogue between two characters. Args: dialogue_id: ID of the dialogue. If None it will be named automatically. name1: Name of the first character. persona1: Persona of the first character. name2: Name of the second character. persona2: Persona of the second character. location_name: Name of the place where dialogue happens. location_description: Description of the place where dialogue happens. Returns: Dialogue id. \"\"\" if dialogue_id in self . dialogues : raise ValueError ( \"Dialogue already exists.\" ) if dialogue_id is None : dialogue_id = f \"dialogue_ { self . dialogue_id_counter } \" self . dialogue_id_counter += 1 while dialogue_id in self . dialogues : dialogue_id = f \"dialogue_ { self . dialogue_id_counter } \" self . dialogue_id_counter += 1 self . dialogues [ dialogue_id ] = { \"characters\" : [ { \"name\" : name1 , \"persona\" : persona1 }, { \"name\" : name2 , \"persona\" : persona2 }, ], \"location\" : { \"name\" : location_name , \"description\" : location_description }, \"history\" : [], \"other\" : kwargs . get ( \"other\" , {}), } return dialogue_id def end_dialogue ( self , dialogue_id : str ): \"\"\"End a dialogue. Args: dialogue_id: ID of the dialogue. \"\"\" del self . dialogues [ dialogue_id ] def generate_utterance ( self , dialogue_id : str , speaker_id : str ) -> str : \"\"\"Generate an utterance for the given dialogue. Args: dialogue_id: ID of the dialogue. speaker: 0 for the first character, 1 for the second character. \"\"\" context = self . _build_context ( dialogue_id , speaker_id ) return self . text_generation_service . generate_utterance ( context ) def check_scripted_utterances ( self , utterance : str , scripted_utterances : List [ str ], threshold : float ) -> int : \"\"\"Check if the given utterance is one of the scripted utterances. Args: utterance: Natural language utterance. scripted_utterances: Natural language utterances. threshold: [0,1] threshold for the similarity between the utterance and the scripted utterances. Returns: id of the utterance, None if the utterance is not one of the scripted utterances. \"\"\" scores = self . similarity_api . check_similarity ( utterance , scripted_utterances , threshold ) scores = [ score if score > threshold else 0 for score in scores ] if max ( scores ) > threshold : return scores . index ( max ( scores )) return None def update_dialogue ( self , dialogue_id : str , speaker_id : str , utterance : str ): \"\"\"Update dialogue state. Args: dialogue_id: ID of the dialogue. speaker_id: 0 for the first character, 1 for the second character. utterance: Natural language utterance. \"\"\" self . dialogues [ dialogue_id ][ \"history\" ] . append ( { \"speaker\" : speaker_id , \"line\" : utterance } ) def get_history ( self , dialogue_id : str ) -> List [ Dict [ str , Any ]]: \"\"\"Get the history of a dialogue. Args: dialogue_id: ID of the dialogue. \"\"\" return self . dialogues [ dialogue_id ][ \"history\" ] def _build_context ( self , dialogue_id : str , speaker_id : str ) -> Dict [ str , Any ]: \"\"\"Build the context for the given dialogue. Args: dialogue_id: ID of the dialogue. \"\"\" speaker = [ c [ \"name\" ] for c in self . dialogues [ dialogue_id ][ \"characters\" ]] . index ( speaker_id ) other_speaker = speaker ^ 1 context = copy ( self . context_template ) context [ \"history\" ] = self . dialogues [ dialogue_id ][ \"history\" ] context [ \"location\" ] = self . dialogues [ dialogue_id ][ \"location\" ][ \"description\" ] context [ \"location_name\" ] = self . dialogues [ dialogue_id ][ \"location\" ][ \"name\" ] context [ \"name\" ] = self . dialogues [ dialogue_id ][ \"characters\" ][ speaker ][ \"name\" ] context [ \"persona\" ] = self . dialogues [ dialogue_id ][ \"characters\" ][ speaker ][ \"persona\" ] context [ \"other_name\" ] = self . dialogues [ dialogue_id ][ \"characters\" ][ other_speaker ][ \"name\" ] context [ \"other_persona\" ] = self . dialogues [ dialogue_id ][ \"characters\" ][ other_speaker ][ \"persona\" ] return context def _validate_context_template ( self , context_template : Dict [ str , Any ]): \"\"\"Validate the context template. Args: context_template: Context template. \"\"\" if \"persona\" not in context_template : raise ValueError ( \"Context template must contain a persona.\" ) if \"name\" not in context_template : raise ValueError ( \"Context template must contain a name.\" ) if \"location\" not in context_template : raise ValueError ( \"Context template must contain a location.\" ) if \"location_name\" not in context_template : raise ValueError ( \"Context template must contain a location name.\" ) if \"other_name\" not in context_template : raise ValueError ( \"Context template must contain a other_name field.\" ) if \"other_persona\" not in context_template : raise ValueError ( \"Context template must contain a other_persona field.\" ) if \"history\" not in context_template : raise ValueError ( \"Context template must contain a history field.\" ) __init__ ( text_generation_svc = 'TextGenerationAPI' , similarity_svc = 'SimilarityAPI' , * args , ** kwargs ) Initialize the persona dialogue API. Parameters: Name Type Description Default text_generation_service Name of the text generation service. required similarity_api Name of the similarity API. required similarity_threshold Threshold for semantic similarity. required Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def __init__ ( self , text_generation_svc : str = \"TextGenerationAPI\" , similarity_svc : str = \"SimilarityAPI\" , * args , ** kwargs , ): \"\"\"Initialize the persona dialogue API. Args: text_generation_service: Name of the text generation service. similarity_api: Name of the similarity API. similarity_threshold: Threshold for semantic similarity. \"\"\" super () . __init__ ( * args , ** kwargs ) self . text_generation_service = self . create_client ( text_generation_svc ) self . similarity_api = self . create_client ( similarity_svc ) self . dialogues = {} self . dialogue_id_counter = 0 self . context_template = self . text_generation_service . get_context_template () self . _validate_context_template ( self . context_template ) _build_context ( dialogue_id , speaker_id ) Build the context for the given dialogue. Parameters: Name Type Description Default dialogue_id str ID of the dialogue. required Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def _build_context ( self , dialogue_id : str , speaker_id : str ) -> Dict [ str , Any ]: \"\"\"Build the context for the given dialogue. Args: dialogue_id: ID of the dialogue. \"\"\" speaker = [ c [ \"name\" ] for c in self . dialogues [ dialogue_id ][ \"characters\" ]] . index ( speaker_id ) other_speaker = speaker ^ 1 context = copy ( self . context_template ) context [ \"history\" ] = self . dialogues [ dialogue_id ][ \"history\" ] context [ \"location\" ] = self . dialogues [ dialogue_id ][ \"location\" ][ \"description\" ] context [ \"location_name\" ] = self . dialogues [ dialogue_id ][ \"location\" ][ \"name\" ] context [ \"name\" ] = self . dialogues [ dialogue_id ][ \"characters\" ][ speaker ][ \"name\" ] context [ \"persona\" ] = self . dialogues [ dialogue_id ][ \"characters\" ][ speaker ][ \"persona\" ] context [ \"other_name\" ] = self . dialogues [ dialogue_id ][ \"characters\" ][ other_speaker ][ \"name\" ] context [ \"other_persona\" ] = self . dialogues [ dialogue_id ][ \"characters\" ][ other_speaker ][ \"persona\" ] return context _validate_context_template ( context_template ) Validate the context template. Parameters: Name Type Description Default context_template Dict [ str , Any ] Context template. required Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def _validate_context_template ( self , context_template : Dict [ str , Any ]): \"\"\"Validate the context template. Args: context_template: Context template. \"\"\" if \"persona\" not in context_template : raise ValueError ( \"Context template must contain a persona.\" ) if \"name\" not in context_template : raise ValueError ( \"Context template must contain a name.\" ) if \"location\" not in context_template : raise ValueError ( \"Context template must contain a location.\" ) if \"location_name\" not in context_template : raise ValueError ( \"Context template must contain a location name.\" ) if \"other_name\" not in context_template : raise ValueError ( \"Context template must contain a other_name field.\" ) if \"other_persona\" not in context_template : raise ValueError ( \"Context template must contain a other_persona field.\" ) if \"history\" not in context_template : raise ValueError ( \"Context template must contain a history field.\" ) check_scripted_utterances ( utterance , scripted_utterances , threshold ) Check if the given utterance is one of the scripted utterances. Parameters: Name Type Description Default utterance str Natural language utterance. required scripted_utterances List [ str ] Natural language utterances. required threshold float [0,1] threshold for the similarity between the utterance and the scripted utterances. required Returns: Type Description int id of the utterance, None if the utterance is not one of the scripted utterances. Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def check_scripted_utterances ( self , utterance : str , scripted_utterances : List [ str ], threshold : float ) -> int : \"\"\"Check if the given utterance is one of the scripted utterances. Args: utterance: Natural language utterance. scripted_utterances: Natural language utterances. threshold: [0,1] threshold for the similarity between the utterance and the scripted utterances. Returns: id of the utterance, None if the utterance is not one of the scripted utterances. \"\"\" scores = self . similarity_api . check_similarity ( utterance , scripted_utterances , threshold ) scores = [ score if score > threshold else 0 for score in scores ] if max ( scores ) > threshold : return scores . index ( max ( scores )) return None end_dialogue ( dialogue_id ) End a dialogue. Parameters: Name Type Description Default dialogue_id str ID of the dialogue. required Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue.py 81 82 83 84 85 86 87 def end_dialogue ( self , dialogue_id : str ): \"\"\"End a dialogue. Args: dialogue_id: ID of the dialogue. \"\"\" del self . dialogues [ dialogue_id ] generate_utterance ( dialogue_id , speaker_id ) Generate an utterance for the given dialogue. Parameters: Name Type Description Default dialogue_id str ID of the dialogue. required speaker 0 for the first character, 1 for the second character. required Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue.py 89 90 91 92 93 94 95 96 97 def generate_utterance ( self , dialogue_id : str , speaker_id : str ) -> str : \"\"\"Generate an utterance for the given dialogue. Args: dialogue_id: ID of the dialogue. speaker: 0 for the first character, 1 for the second character. \"\"\" context = self . _build_context ( dialogue_id , speaker_id ) return self . text_generation_service . generate_utterance ( context ) get_history ( dialogue_id ) Get the history of a dialogue. Parameters: Name Type Description Default dialogue_id str ID of the dialogue. required Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue.py 132 133 134 135 136 137 138 def get_history ( self , dialogue_id : str ) -> List [ Dict [ str , Any ]]: \"\"\"Get the history of a dialogue. Args: dialogue_id: ID of the dialogue. \"\"\" return self . dialogues [ dialogue_id ][ \"history\" ] start_dialogue ( name1 = None , persona1 = None , name2 = None , persona2 = None , location_name = None , location_description = None , dialogue_id = None , * args , ** kwargs ) Start a dialogue between two characters. Parameters: Name Type Description Default dialogue_id str ID of the dialogue. If None it will be named automatically. None name1 str Name of the first character. None persona1 str Persona of the first character. None name2 str Name of the second character. None persona2 str Persona of the second character. None location_name str Name of the place where dialogue happens. None location_description str Description of the place where dialogue happens. None Returns: Type Description str Dialogue id. Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def start_dialogue ( self , name1 : str = None , persona1 : str = None , name2 : str = None , persona2 : str = None , location_name : str = None , location_description : str = None , dialogue_id : str = None , * args , ** kwargs , ) -> str : \"\"\"Start a dialogue between two characters. Args: dialogue_id: ID of the dialogue. If None it will be named automatically. name1: Name of the first character. persona1: Persona of the first character. name2: Name of the second character. persona2: Persona of the second character. location_name: Name of the place where dialogue happens. location_description: Description of the place where dialogue happens. Returns: Dialogue id. \"\"\" if dialogue_id in self . dialogues : raise ValueError ( \"Dialogue already exists.\" ) if dialogue_id is None : dialogue_id = f \"dialogue_ { self . dialogue_id_counter } \" self . dialogue_id_counter += 1 while dialogue_id in self . dialogues : dialogue_id = f \"dialogue_ { self . dialogue_id_counter } \" self . dialogue_id_counter += 1 self . dialogues [ dialogue_id ] = { \"characters\" : [ { \"name\" : name1 , \"persona\" : persona1 }, { \"name\" : name2 , \"persona\" : persona2 }, ], \"location\" : { \"name\" : location_name , \"description\" : location_description }, \"history\" : [], \"other\" : kwargs . get ( \"other\" , {}), } return dialogue_id update_dialogue ( dialogue_id , speaker_id , utterance ) Update dialogue state. Parameters: Name Type Description Default dialogue_id str ID of the dialogue. required speaker_id str 0 for the first character, 1 for the second character. required utterance str Natural language utterance. required Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue.py 120 121 122 123 124 125 126 127 128 129 130 def update_dialogue ( self , dialogue_id : str , speaker_id : str , utterance : str ): \"\"\"Update dialogue state. Args: dialogue_id: ID of the dialogue. speaker_id: 0 for the first character, 1 for the second character. utterance: Natural language utterance. \"\"\" self . dialogues [ dialogue_id ][ \"history\" ] . append ( { \"speaker\" : speaker_id , \"line\" : utterance } ) persona_dialogue_base Module that implements persona dialogue API. PersonaDialogueAPI Bases: BaseService Abstract base class for persona dialogue models. Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue_base.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 class PersonaDialogueAPI ( BaseService ): \"\"\"Abstract base class for persona dialogue models.\"\"\" API_METHODS : List [ str ] = [ \"start_dialogue\" , \"step_dialogue\" , \"get_history\" ] def __init__ ( self , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"PersonaDialogueAPI\" @abstractmethod def start_dialogue ( self , name1 : str = None , persona1 : str = None , name2 : str = None , persona2 : str = None , location_name : str = None , location_description : str = None , items_of_interest : List [ str ] = None , dialogue_id : str = None , other : Dict [ str , Any ] = None , ) -> str : \"\"\"Start a dialogue between two characters. All arguments are supposed to be natural language descriptions. Args: name1: Name of the first character. persona1: Persona of the first character. name2: Name of the second character. persona2: Persona of the second character. location_name: Name of the place where dialogue happens. location_description: Description of the place where dialogue happens. items_of_interest: List of items of interest that could be mentioned in the dialogue. dialogue_id: ID of the dialogue. If None it will be named automatically. other: Other information that could be used to start the dialogue. Returns: Dialogue id. \"\"\" pass @abstractmethod def end_dialogue ( self , dialogue_id : str ): \"\"\"End a dialogue between two characters. Args: dialogue_id: ID of the dialogue. \"\"\" pass def step_dialogue ( self , dialogue_id : str , speaker_id : str , utterance : str = None , scripted_utterances : List [ str ] = None , scripted_threshold : float = 0.5 , update_history : bool = True , ) -> Tuple [ str , bool ]: \"\"\"Step a dialogue between two characters. Args: dialogue_id: ID of the dialogue. speaker: 0 for the first character, 1 for the second character. utterance: Natural language utterance. If None it will be generated. scripted_utterances: List of natural language utterances that will be matched against utterance. scripted_threshold: Threshold for matching scripted utterances. update_history: If True, the dialogue history will be updated. Returns: str: Next utterance. bool: scripted utterance triggered \"\"\" if utterance is None : utterance = self . generate_utterance ( dialogue_id , speaker_id ) scripted = False if scripted_utterances is not None : idx = self . check_scripted_utterances ( utterance , scripted_utterances , scripted_threshold ) if idx is not None : utterance = scripted_utterances [ idx ] scripted = True if update_history : self . update_dialogue ( dialogue_id , speaker_id , utterance ) return utterance , scripted @abstractmethod def generate_utterance ( self , dialogue_id : str , speaker_id : str ) -> str : \"\"\"Generate an utterance for the given speaker. Args: dialogue_id: ID of the dialogue. speaker: 0 for the first character, 1 for the second character. \"\"\" pass @abstractmethod def check_scripted_utterances ( self , utterance : str , scripted_utterances : List [ str ], threshold : float ) -> int : \"\"\"Check if the given utterance is one of the scripted utterances. Args: utterance: Natural language utterance. scripted_utterances: Natural language utterances. threshold: [0,1] threshold for the similarity between the utterance and the scripted utterances. Returns: id of the utterance, None if the utterance is not one of the scripted utterances. \"\"\" pass @abstractmethod def update_dialogue ( self , dialogue_id : str , speaker_id : str , utterance : str ): \"\"\"Update dialogue state. Args: dialogue_id: ID of the dialogue. speaker_id: 0 for the first character, 1 for the second character. utterance: Natural language utterance. \"\"\" pass @abstractmethod def get_history ( self , dialogue_id : str ) -> List [ Dict [ str , Any ]]: \"\"\"Get the history of a dialogue. Args: dialogue_id: ID of the dialogue. \"\"\" pass __init__ ( * args , ** kwargs ) Empty initialization method for API to be similar to other model base classes. Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue_base.py 13 14 15 16 def __init__ ( self , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True check_scripted_utterances ( utterance , scripted_utterances , threshold ) abstractmethod Check if the given utterance is one of the scripted utterances. Parameters: Name Type Description Default utterance str Natural language utterance. required scripted_utterances List [ str ] Natural language utterances. required threshold float [0,1] threshold for the similarity between the utterance and the scripted utterances. required Returns: Type Description int id of the utterance, None if the utterance is not one of the scripted utterances. Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue_base.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 @abstractmethod def check_scripted_utterances ( self , utterance : str , scripted_utterances : List [ str ], threshold : float ) -> int : \"\"\"Check if the given utterance is one of the scripted utterances. Args: utterance: Natural language utterance. scripted_utterances: Natural language utterances. threshold: [0,1] threshold for the similarity between the utterance and the scripted utterances. Returns: id of the utterance, None if the utterance is not one of the scripted utterances. \"\"\" pass end_dialogue ( dialogue_id ) abstractmethod End a dialogue between two characters. Parameters: Name Type Description Default dialogue_id str ID of the dialogue. required Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue_base.py 55 56 57 58 59 60 61 62 @abstractmethod def end_dialogue ( self , dialogue_id : str ): \"\"\"End a dialogue between two characters. Args: dialogue_id: ID of the dialogue. \"\"\" pass generate_utterance ( dialogue_id , speaker_id ) abstractmethod Generate an utterance for the given speaker. Parameters: Name Type Description Default dialogue_id str ID of the dialogue. required speaker 0 for the first character, 1 for the second character. required Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue_base.py 101 102 103 104 105 106 107 108 109 @abstractmethod def generate_utterance ( self , dialogue_id : str , speaker_id : str ) -> str : \"\"\"Generate an utterance for the given speaker. Args: dialogue_id: ID of the dialogue. speaker: 0 for the first character, 1 for the second character. \"\"\" pass get_api_name () classmethod Get the API name. Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue_base.py 18 19 20 21 @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"PersonaDialogueAPI\" get_history ( dialogue_id ) abstractmethod Get the history of a dialogue. Parameters: Name Type Description Default dialogue_id str ID of the dialogue. required Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue_base.py 138 139 140 141 142 143 144 145 @abstractmethod def get_history ( self , dialogue_id : str ) -> List [ Dict [ str , Any ]]: \"\"\"Get the history of a dialogue. Args: dialogue_id: ID of the dialogue. \"\"\" pass start_dialogue ( name1 = None , persona1 = None , name2 = None , persona2 = None , location_name = None , location_description = None , items_of_interest = None , dialogue_id = None , other = None ) abstractmethod Start a dialogue between two characters. All arguments are supposed to be natural language descriptions. Parameters: Name Type Description Default name1 str Name of the first character. None persona1 str Persona of the first character. None name2 str Name of the second character. None persona2 str Persona of the second character. None location_name str Name of the place where dialogue happens. None location_description str Description of the place where dialogue happens. None items_of_interest List [ str ] List of items of interest that could be mentioned in the dialogue. None dialogue_id str ID of the dialogue. If None it will be named automatically. None other Dict [ str , Any ] Other information that could be used to start the dialogue. None Returns: Type Description str Dialogue id. Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue_base.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 @abstractmethod def start_dialogue ( self , name1 : str = None , persona1 : str = None , name2 : str = None , persona2 : str = None , location_name : str = None , location_description : str = None , items_of_interest : List [ str ] = None , dialogue_id : str = None , other : Dict [ str , Any ] = None , ) -> str : \"\"\"Start a dialogue between two characters. All arguments are supposed to be natural language descriptions. Args: name1: Name of the first character. persona1: Persona of the first character. name2: Name of the second character. persona2: Persona of the second character. location_name: Name of the place where dialogue happens. location_description: Description of the place where dialogue happens. items_of_interest: List of items of interest that could be mentioned in the dialogue. dialogue_id: ID of the dialogue. If None it will be named automatically. other: Other information that could be used to start the dialogue. Returns: Dialogue id. \"\"\" pass step_dialogue ( dialogue_id , speaker_id , utterance = None , scripted_utterances = None , scripted_threshold = 0.5 , update_history = True ) Step a dialogue between two characters. Parameters: Name Type Description Default dialogue_id str ID of the dialogue. required speaker 0 for the first character, 1 for the second character. required utterance str Natural language utterance. If None it will be generated. None scripted_utterances List [ str ] List of natural language utterances that will be matched against utterance. None scripted_threshold float Threshold for matching scripted utterances. 0.5 update_history bool If True, the dialogue history will be updated. True Returns: Name Type Description str str Next utterance. bool bool scripted utterance triggered Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue_base.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def step_dialogue ( self , dialogue_id : str , speaker_id : str , utterance : str = None , scripted_utterances : List [ str ] = None , scripted_threshold : float = 0.5 , update_history : bool = True , ) -> Tuple [ str , bool ]: \"\"\"Step a dialogue between two characters. Args: dialogue_id: ID of the dialogue. speaker: 0 for the first character, 1 for the second character. utterance: Natural language utterance. If None it will be generated. scripted_utterances: List of natural language utterances that will be matched against utterance. scripted_threshold: Threshold for matching scripted utterances. update_history: If True, the dialogue history will be updated. Returns: str: Next utterance. bool: scripted utterance triggered \"\"\" if utterance is None : utterance = self . generate_utterance ( dialogue_id , speaker_id ) scripted = False if scripted_utterances is not None : idx = self . check_scripted_utterances ( utterance , scripted_utterances , scripted_threshold ) if idx is not None : utterance = scripted_utterances [ idx ] scripted = True if update_history : self . update_dialogue ( dialogue_id , speaker_id , utterance ) return utterance , scripted update_dialogue ( dialogue_id , speaker_id , utterance ) abstractmethod Update dialogue state. Parameters: Name Type Description Default dialogue_id str ID of the dialogue. required speaker_id str 0 for the first character, 1 for the second character. required utterance str Natural language utterance. required Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue_base.py 127 128 129 130 131 132 133 134 135 136 @abstractmethod def update_dialogue ( self , dialogue_id : str , speaker_id : str , utterance : str ): \"\"\"Update dialogue state. Args: dialogue_id: ID of the dialogue. speaker_id: 0 for the first character, 1 for the second character. utterance: Natural language utterance. \"\"\" pass sequence_classifier Sequence classification API module. hf_classifier Module that implements Huggingface transformers classification. HfClassifier Bases: SequenceClassifierAPI Huggingface transformers sequence classification. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with sequence-classification feature. Also requires saved tokenizer with huggingface tokenizers. Source code in npc_engine\\services\\sequence_classifier\\hf_classifier.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class HfClassifier ( SequenceClassifierAPI ): \"\"\"Huggingface transformers sequence classification. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with sequence-classification feature. Also requires saved tokenizer with huggingface tokenizers. \"\"\" def __init__ ( self , model_path : str , * args , ** kwargs ): \"\"\"Create and load biencoder model for semantic similarity. Args: model_path: A path where model config and weights are. \"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_ALL self . model = rt . InferenceSession ( os . path . join ( model_path , \"model.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) input_names = [ inp . name for inp in self . model . get_inputs ()] self . token_type_support = \"token_type_ids\" in input_names special_tokens_map_path = os . path . join ( model_path , \"special_tokens_map.json\" ) with open ( special_tokens_map_path , \"r\" ) as f : self . special_tokens = json . load ( f ) self . pad_token_id = self . tokenizer . encode ( self . special_tokens [ \"pad_token\" ]) . ids [ 0 ] self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) self . tokenizer . enable_padding ( direction = \"right\" , pad_id = self . pad_token_id , pad_type_id = 0 , pad_token = self . tokenizer . decode ( [ self . pad_token_id ], skip_special_tokens = False ), length = None , pad_to_multiple_of = None , ) self . tests = {} def compute_scores_batch ( self , texts : List [ Union [ str , Tuple [ str , str ]]] ) -> np . ndarray : \"\"\"Compute scores. Args: texts: Sentence to embed Returns: scores: Scores for each text \"\"\" tokenized = self . tokenizer . encode_batch ( texts ) ids = np . stack ([ np . asarray ( encoding . ids ) for encoding in tokenized ]) . astype ( np . int64 ) attention_mask = np . stack ( [ np . asarray ( encoding . attention_mask ) for encoding in tokenized ] ) . astype ( np . int64 ) if not self . token_type_support : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask } else : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : np . stack ( [ np . asarray ( encoding . type_ids ) for encoding in tokenized ] ) . astype ( np . int64 ), } outp = self . model . run ( None , input_dict ) return outp [ 0 ] __init__ ( model_path , * args , ** kwargs ) Create and load biencoder model for semantic similarity. Parameters: Name Type Description Default model_path str A path where model config and weights are. required Source code in npc_engine\\services\\sequence_classifier\\hf_classifier.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , model_path : str , * args , ** kwargs ): \"\"\"Create and load biencoder model for semantic similarity. Args: model_path: A path where model config and weights are. \"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_ALL self . model = rt . InferenceSession ( os . path . join ( model_path , \"model.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) input_names = [ inp . name for inp in self . model . get_inputs ()] self . token_type_support = \"token_type_ids\" in input_names special_tokens_map_path = os . path . join ( model_path , \"special_tokens_map.json\" ) with open ( special_tokens_map_path , \"r\" ) as f : self . special_tokens = json . load ( f ) self . pad_token_id = self . tokenizer . encode ( self . special_tokens [ \"pad_token\" ]) . ids [ 0 ] self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) self . tokenizer . enable_padding ( direction = \"right\" , pad_id = self . pad_token_id , pad_type_id = 0 , pad_token = self . tokenizer . decode ( [ self . pad_token_id ], skip_special_tokens = False ), length = None , pad_to_multiple_of = None , ) self . tests = {} compute_scores_batch ( texts ) Compute scores. Parameters: Name Type Description Default texts List [ Union [ str , Tuple [ str , str ]]] Sentence to embed required Returns: Name Type Description scores np . ndarray Scores for each text Source code in npc_engine\\services\\sequence_classifier\\hf_classifier.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def compute_scores_batch ( self , texts : List [ Union [ str , Tuple [ str , str ]]] ) -> np . ndarray : \"\"\"Compute scores. Args: texts: Sentence to embed Returns: scores: Scores for each text \"\"\" tokenized = self . tokenizer . encode_batch ( texts ) ids = np . stack ([ np . asarray ( encoding . ids ) for encoding in tokenized ]) . astype ( np . int64 ) attention_mask = np . stack ( [ np . asarray ( encoding . attention_mask ) for encoding in tokenized ] ) . astype ( np . int64 ) if not self . token_type_support : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask } else : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : np . stack ( [ np . asarray ( encoding . type_ids ) for encoding in tokenized ] ) . astype ( np . int64 ), } outp = self . model . run ( None , input_dict ) return outp [ 0 ] sequence_classifier_base Module that implements sequence classification API. SequenceClassifierAPI Bases: BaseService Abstract base class for text classification models. Source code in npc_engine\\services\\sequence_classifier\\sequence_classifier_base.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class SequenceClassifierAPI ( BaseService ): \"\"\"Abstract base class for text classification models.\"\"\" API_METHODS : List [ str ] = [ \"classify\" ] def __init__ ( self , cache_size = 0 , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True self . cache = NumpyLRUCache ( cache_size ) @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"SequenceClassifierAPI\" def classify ( self , texts : List [ str ]) -> List [ List [ float ]]: \"\"\"Classify a list of texts. Args: texts: A list of texts to classify. Returns: List of scores for each text. \"\"\" texts = [ row if isinstance ( row , str ) else tuple ( row ) for row in texts ] scores = self . cache . cache_compute ( texts , lambda values : self . compute_scores_batch ( values ) ) return scores . tolist () @abstractmethod def compute_scores_batch ( self , texts : List [ str ]) -> np . ndarray : \"\"\"Compute scores for a list of texts. Args: texts: A list of texts to compute scores for. Returns: List of scores for each text. \"\"\" pass __init__ ( cache_size = 0 , * args , ** kwargs ) Empty initialization method for API to be similar to other model base classes. Source code in npc_engine\\services\\sequence_classifier\\sequence_classifier_base.py 15 16 17 18 19 def __init__ ( self , cache_size = 0 , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True self . cache = NumpyLRUCache ( cache_size ) classify ( texts ) Classify a list of texts. Parameters: Name Type Description Default texts List [ str ] A list of texts to classify. required Returns: Type Description List [ List [ float ]] List of scores for each text. Source code in npc_engine\\services\\sequence_classifier\\sequence_classifier_base.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def classify ( self , texts : List [ str ]) -> List [ List [ float ]]: \"\"\"Classify a list of texts. Args: texts: A list of texts to classify. Returns: List of scores for each text. \"\"\" texts = [ row if isinstance ( row , str ) else tuple ( row ) for row in texts ] scores = self . cache . cache_compute ( texts , lambda values : self . compute_scores_batch ( values ) ) return scores . tolist () compute_scores_batch ( texts ) abstractmethod Compute scores for a list of texts. Parameters: Name Type Description Default texts List [ str ] A list of texts to compute scores for. required Returns: Type Description np . ndarray List of scores for each text. Source code in npc_engine\\services\\sequence_classifier\\sequence_classifier_base.py 41 42 43 44 45 46 47 48 49 50 51 @abstractmethod def compute_scores_batch ( self , texts : List [ str ]) -> np . ndarray : \"\"\"Compute scores for a list of texts. Args: texts: A list of texts to compute scores for. Returns: List of scores for each text. \"\"\" pass get_api_name () classmethod Get the API name. Source code in npc_engine\\services\\sequence_classifier\\sequence_classifier_base.py 21 22 23 24 @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"SequenceClassifierAPI\" similarity Similarity model implementations. This module implements specific models and wraps them under the common interface for loading and inference. Example from npc_engine.models.similarity import SimilarityAPI model = SimilarityAPI.load(\"path/to/model_dir\") model.compare(\"hello\", [\"Hello, world!\"]) similarity_base Module that implements semantic similarity model API. SimilarityAPI Bases: BaseService Abstract base class for text similarity models. Source code in npc_engine\\services\\similarity\\similarity_base.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class SimilarityAPI ( BaseService ): \"\"\"Abstract base class for text similarity models.\"\"\" API_METHODS : List [ str ] = [ \"compare\" , \"cache\" ] def __init__ ( self , cache_size = 0 , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True self . lru_cache = NumpyLRUCache ( cache_size ) @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"SimilarityAPI\" def compare ( self , query : str , context : List [ str ]) -> List [ float ]: \"\"\"Compare a query to the context. Args: query: A sentence to compare. context: A list of sentences to compare to. This will be cached if caching is enabled Returns: List of similarities \"\"\" embedding_a = self . compute_embedding ( query ) embedding_b = self . lru_cache . cache_compute ( context , lambda values : self . compute_embedding_batch ( values ) ) similarities = self . metric ( embedding_a , embedding_b ) return similarities . tolist () def cache ( self , context : List [ str ]): \"\"\"Cache embeddings of given sequences. Args: context: A list of sentences to cache. \"\"\" self . lru_cache . cache_compute ( context , lambda values : self . compute_embedding_batch ( values ) ) @abstractmethod def compute_embedding_batch ( self , lines : List [ str ]) -> np . ndarray : \"\"\"Compute line embeddings in batch. Args: lines: List of sentences to embed Returns: Embedding batch of shape (batch_size, embedding_size) \"\"\" return None @abstractmethod def compute_embedding ( self , line : str ) -> np . ndarray : \"\"\"Compute sentence embedding. Args: line: Sentence to embed Returns: Embedding of shape (1, embedding_size) \"\"\" return None @abstractmethod def metric ( self , embedding_a : np . ndarray , embedding_b : np . ndarray ) -> np . ndarray : \"\"\"Compute distance between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Args: embedding_a: Embedding of shape (1 or batch_size, embedding_size) embedding_b: Embedding of shape (1 or batch_size, embedding_size) Returns: Vector of distances (batch_size or 1,) \"\"\" return None __init__ ( cache_size = 0 , * args , ** kwargs ) Empty initialization method for API to be similar to other model base classes. Source code in npc_engine\\services\\similarity\\similarity_base.py 15 16 17 18 19 def __init__ ( self , cache_size = 0 , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True self . lru_cache = NumpyLRUCache ( cache_size ) cache ( context ) Cache embeddings of given sequences. Parameters: Name Type Description Default context List [ str ] A list of sentences to cache. required Source code in npc_engine\\services\\similarity\\similarity_base.py 43 44 45 46 47 48 49 50 51 def cache ( self , context : List [ str ]): \"\"\"Cache embeddings of given sequences. Args: context: A list of sentences to cache. \"\"\" self . lru_cache . cache_compute ( context , lambda values : self . compute_embedding_batch ( values ) ) compare ( query , context ) Compare a query to the context. Parameters: Name Type Description Default query str A sentence to compare. required context List [ str ] A list of sentences to compare to. This will be cached if caching is enabled required Returns: Type Description List [ float ] List of similarities Source code in npc_engine\\services\\similarity\\similarity_base.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def compare ( self , query : str , context : List [ str ]) -> List [ float ]: \"\"\"Compare a query to the context. Args: query: A sentence to compare. context: A list of sentences to compare to. This will be cached if caching is enabled Returns: List of similarities \"\"\" embedding_a = self . compute_embedding ( query ) embedding_b = self . lru_cache . cache_compute ( context , lambda values : self . compute_embedding_batch ( values ) ) similarities = self . metric ( embedding_a , embedding_b ) return similarities . tolist () compute_embedding ( line ) abstractmethod Compute sentence embedding. Parameters: Name Type Description Default line str Sentence to embed required Returns: Type Description np . ndarray Embedding of shape (1, embedding_size) Source code in npc_engine\\services\\similarity\\similarity_base.py 65 66 67 68 69 70 71 72 73 74 75 @abstractmethod def compute_embedding ( self , line : str ) -> np . ndarray : \"\"\"Compute sentence embedding. Args: line: Sentence to embed Returns: Embedding of shape (1, embedding_size) \"\"\" return None compute_embedding_batch ( lines ) abstractmethod Compute line embeddings in batch. Parameters: Name Type Description Default lines List [ str ] List of sentences to embed required Returns: Type Description np . ndarray Embedding batch of shape (batch_size, embedding_size) Source code in npc_engine\\services\\similarity\\similarity_base.py 53 54 55 56 57 58 59 60 61 62 63 @abstractmethod def compute_embedding_batch ( self , lines : List [ str ]) -> np . ndarray : \"\"\"Compute line embeddings in batch. Args: lines: List of sentences to embed Returns: Embedding batch of shape (batch_size, embedding_size) \"\"\" return None get_api_name () classmethod Get the API name. Source code in npc_engine\\services\\similarity\\similarity_base.py 21 22 23 24 @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"SimilarityAPI\" metric ( embedding_a , embedding_b ) abstractmethod Compute distance between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Parameters: Name Type Description Default embedding_a np . ndarray Embedding of shape (1 or batch_size, embedding_size) required embedding_b np . ndarray Embedding of shape (1 or batch_size, embedding_size) required Returns: Type Description np . ndarray Vector of distances (batch_size or 1,) Source code in npc_engine\\services\\similarity\\similarity_base.py 77 78 79 80 81 82 83 84 85 86 87 88 89 @abstractmethod def metric ( self , embedding_a : np . ndarray , embedding_b : np . ndarray ) -> np . ndarray : \"\"\"Compute distance between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Args: embedding_a: Embedding of shape (1 or batch_size, embedding_size) embedding_b: Embedding of shape (1 or batch_size, embedding_size) Returns: Vector of distances (batch_size or 1,) \"\"\" return None similarity_transformers Module that implements Huggingface transformers semantic similarity. TransformerSemanticSimilarity Bases: SimilarityAPI Huggingface transformers semantic similarity. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with biencoder architecture. Also requires a tokenizer.json with huggingface tokenizers definition. model.onnx spec: - inputs: `input_ids` of shape `(batch_size, sequence)` `attention_mask` of shape `(batch_size, sequence)` (Optional) `input_type_ids` of shape `(batch_size, sequence)` - outputs: `token_embeddings` of shape `(batch_size, sequence, hidden_size)` Source code in npc_engine\\services\\similarity\\similarity_transformers.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 class TransformerSemanticSimilarity ( SimilarityAPI ): \"\"\"Huggingface transformers semantic similarity. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with biencoder architecture. Also requires a tokenizer.json with huggingface tokenizers definition. model.onnx spec: - inputs: `input_ids` of shape `(batch_size, sequence)` `attention_mask` of shape `(batch_size, sequence)` (Optional) `input_type_ids` of shape `(batch_size, sequence)` - outputs: `token_embeddings` of shape `(batch_size, sequence, hidden_size)` \"\"\" def __init__ ( self , model_path : str , metric : str = \"dot\" , * args , ** kwargs ): \"\"\"Create and load biencoder model for semantic similarity. Args: model_path: A path where model config and weights are metric: distance to compute semantic similarity \"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_ALL self . model = rt . InferenceSession ( os . path . join ( model_path , \"model.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) input_names = [ inp . name for inp in self . model . get_inputs ()] self . token_type_support = \"token_type_ids\" in input_names special_tokens_map_path = os . path . join ( model_path , \"special_tokens_map.json\" ) with open ( special_tokens_map_path , \"r\" ) as f : self . special_tokens = json . load ( f ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) self . pad_token_id = self . tokenizer . encode ( self . special_tokens [ \"pad_token\" ]) . ids [ 0 ] self . tokenizer . enable_padding ( direction = \"right\" , pad_id = self . pad_token_id , pad_type_id = 0 , pad_token = self . tokenizer . decode ( [ self . pad_token_id ], skip_special_tokens = False ), length = None , pad_to_multiple_of = None , ) self . tests = {} self . metric_type = metric def compute_embedding ( self , line : str ) -> np . ndarray : \"\"\"Compute sentence embedding. Args: line: Sentence to embed Returns: Embedding of shape (1, embedding_size) \"\"\" ids = ( np . asarray ( self . tokenizer . encode ( line ) . ids ) . reshape ([ 1 , - 1 ]) . astype ( np . int64 ) ) attention_mask = np . ones_like ( ids ) if not self . token_type_support : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask } else : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : np . zeros_like ( ids ), } outp = self . model . run ( None , input_dict ) return self . _mean_pooling ( outp , attention_mask ) def compute_embedding_batch ( self , lines : List [ str ]) -> np . ndarray : \"\"\"Compute line embeddings in batch. Args: lines: List of sentences to embed Returns: Embedding batch of shape (batch_size, embedding_size) \"\"\" tokenized = self . tokenizer . encode_batch ( lines ) ids = np . stack ([ np . asarray ( encoding . ids ) for encoding in tokenized ]) . astype ( np . int64 ) attention_mask = np . stack ( [ np . asarray ( encoding . attention_mask ) for encoding in tokenized ] ) . astype ( np . int64 ) if not self . token_type_support : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask } else : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : np . stack ( [ np . asarray ( encoding . type_ids ) for encoding in tokenized ] ) . astype ( np . int64 ), } outp = self . model . run ( None , input_dict ) return self . _mean_pooling ( outp , attention_mask ) def _mean_pooling ( self , model_output , attention_mask ): token_embeddings = model_output [ 0 ] attention_mask = np . expand_dims ( attention_mask , - 1 ) sum_embeddings = np . sum ( token_embeddings * attention_mask , 1 ) sum_mask = np . clip ( attention_mask . sum ( 1 ), a_min = 1e-9 , a_max = None ) return sum_embeddings / sum_mask def metric ( self , embedding_a : np . ndarray , embedding_b : np . ndarray ) -> np . ndarray : \"\"\"Similarity between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Args: embedding_a: Embedding of shape (1 or batch_size, embedding_size) embedding_b: Embedding of shape (1 or batch_size, embedding_size) Returns: Vector of distances (batch_size or 1,) \"\"\" if self . metric_type == \"dot\" : return - np . dot ( embedding_a , embedding_b . T ) . squeeze ( 0 ) elif self . metric_type == \"cosine\" : return 1 - cdist ( embedding_a , embedding_b , metric = \"cosine\" ) . squeeze ( 0 ) __init__ ( model_path , metric = 'dot' , * args , ** kwargs ) Create and load biencoder model for semantic similarity. Parameters: Name Type Description Default model_path str A path where model config and weights are required metric str distance to compute semantic similarity 'dot' Source code in npc_engine\\services\\similarity\\similarity_transformers.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , model_path : str , metric : str = \"dot\" , * args , ** kwargs ): \"\"\"Create and load biencoder model for semantic similarity. Args: model_path: A path where model config and weights are metric: distance to compute semantic similarity \"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_ALL self . model = rt . InferenceSession ( os . path . join ( model_path , \"model.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) input_names = [ inp . name for inp in self . model . get_inputs ()] self . token_type_support = \"token_type_ids\" in input_names special_tokens_map_path = os . path . join ( model_path , \"special_tokens_map.json\" ) with open ( special_tokens_map_path , \"r\" ) as f : self . special_tokens = json . load ( f ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) self . pad_token_id = self . tokenizer . encode ( self . special_tokens [ \"pad_token\" ]) . ids [ 0 ] self . tokenizer . enable_padding ( direction = \"right\" , pad_id = self . pad_token_id , pad_type_id = 0 , pad_token = self . tokenizer . decode ( [ self . pad_token_id ], skip_special_tokens = False ), length = None , pad_to_multiple_of = None , ) self . tests = {} self . metric_type = metric compute_embedding ( line ) Compute sentence embedding. Parameters: Name Type Description Default line str Sentence to embed required Returns: Type Description np . ndarray Embedding of shape (1, embedding_size) Source code in npc_engine\\services\\similarity\\similarity_transformers.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def compute_embedding ( self , line : str ) -> np . ndarray : \"\"\"Compute sentence embedding. Args: line: Sentence to embed Returns: Embedding of shape (1, embedding_size) \"\"\" ids = ( np . asarray ( self . tokenizer . encode ( line ) . ids ) . reshape ([ 1 , - 1 ]) . astype ( np . int64 ) ) attention_mask = np . ones_like ( ids ) if not self . token_type_support : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask } else : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : np . zeros_like ( ids ), } outp = self . model . run ( None , input_dict ) return self . _mean_pooling ( outp , attention_mask ) compute_embedding_batch ( lines ) Compute line embeddings in batch. Parameters: Name Type Description Default lines List [ str ] List of sentences to embed required Returns: Type Description np . ndarray Embedding batch of shape (batch_size, embedding_size) Source code in npc_engine\\services\\similarity\\similarity_transformers.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def compute_embedding_batch ( self , lines : List [ str ]) -> np . ndarray : \"\"\"Compute line embeddings in batch. Args: lines: List of sentences to embed Returns: Embedding batch of shape (batch_size, embedding_size) \"\"\" tokenized = self . tokenizer . encode_batch ( lines ) ids = np . stack ([ np . asarray ( encoding . ids ) for encoding in tokenized ]) . astype ( np . int64 ) attention_mask = np . stack ( [ np . asarray ( encoding . attention_mask ) for encoding in tokenized ] ) . astype ( np . int64 ) if not self . token_type_support : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask } else : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : np . stack ( [ np . asarray ( encoding . type_ids ) for encoding in tokenized ] ) . astype ( np . int64 ), } outp = self . model . run ( None , input_dict ) return self . _mean_pooling ( outp , attention_mask ) metric ( embedding_a , embedding_b ) Similarity between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Parameters: Name Type Description Default embedding_a np . ndarray Embedding of shape (1 or batch_size, embedding_size) required embedding_b np . ndarray Embedding of shape (1 or batch_size, embedding_size) required Returns: Type Description np . ndarray Vector of distances (batch_size or 1,) Source code in npc_engine\\services\\similarity\\similarity_transformers.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def metric ( self , embedding_a : np . ndarray , embedding_b : np . ndarray ) -> np . ndarray : \"\"\"Similarity between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Args: embedding_a: Embedding of shape (1 or batch_size, embedding_size) embedding_b: Embedding of shape (1 or batch_size, embedding_size) Returns: Vector of distances (batch_size or 1,) \"\"\" if self . metric_type == \"dot\" : return - np . dot ( embedding_a , embedding_b . T ) . squeeze ( 0 ) elif self . metric_type == \"cosine\" : return 1 - cdist ( embedding_a , embedding_b , metric = \"cosine\" ) . squeeze ( 0 ) stt Speech to text API. This module implements specific models and wraps them under the common interface for loading and inference. Example from npc_engine.models.stt import SpeechToTextAPI model = SpeechToTextAPI.load(\"path/to/model_dir\") text = model.listen() # Say something nemo_stt Module that implements Huggingface transformers semantic similarity. NemoSTT Bases: SpeechToTextAPI Text to speech pipeline based on Nemo toolkit. Uses ONNX export of EncDecCTCModel from Nemo toolkit. Punctuation distillbert model from Nemo toolkit. (requires tokenizer.json as well) Huggingface transformers model for predicting that sentence is finished (Cropped sentence -> 0 label, finished sentence -> 1 label). OpenSLR Librispeech 3-gram model converted to lowercase https://www.openslr.org/11/ References https://github.com/NVIDIA/NeMo https://catalog.ngc.nvidia.com/orgs/nvidia/models/quartznet15x5 https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/punctuation_and_capitalization.html ctc.onnx spec: - inputs: `audio_signal` mel spectogram of shape `(batch_size, 64, mel_sequence)` - outputs: `tokens` of shape `(batch_size, token_sequence, logits)` punctuation.onnx spec: - inputs: `input_ids` mel spectogram of shape `(batch_size, sequence)` `attention_mask` mel spectogram of shape `(batch_size, sequence)` - outputs: `punctuation` of shape `(batch_size, sequence, 4)` `capitalization` of shape `(batch_size, sequence, 2)` Source code in npc_engine\\services\\stt\\nemo_stt.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 class NemoSTT ( SpeechToTextAPI ): \"\"\"Text to speech pipeline based on Nemo toolkit. Uses: - ONNX export of EncDecCTCModel from Nemo toolkit. - Punctuation distillbert model from Nemo toolkit. (requires tokenizer.json as well) - Huggingface transformers model for predicting that sentence is finished (Cropped sentence -> 0 label, finished sentence -> 1 label). - OpenSLR Librispeech 3-gram model converted to lowercase https://www.openslr.org/11/ References: https://github.com/NVIDIA/NeMo https://catalog.ngc.nvidia.com/orgs/nvidia/models/quartznet15x5 https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/punctuation_and_capitalization.html ctc.onnx spec: - inputs: `audio_signal` mel spectogram of shape `(batch_size, 64, mel_sequence)` - outputs: `tokens` of shape `(batch_size, token_sequence, logits)` punctuation.onnx spec: - inputs: `input_ids` mel spectogram of shape `(batch_size, sequence)` `attention_mask` mel spectogram of shape `(batch_size, sequence)` - outputs: `punctuation` of shape `(batch_size, sequence, 4)` `capitalization` of shape `(batch_size, sequence, 2)` \"\"\" def __init__ ( self , model_path : str , frame_size : int = 1000 , sample_rate : int = 16000 , predict_punctuation : bool = False , * args , ** kwargs , ): \"\"\"Create and load biencoder model for semantic similarity. Args: model_path: A path where model config and weights are stored. frame_size: Size of the audio frame in milliseconds. sample_rate: Sample rate of the audio. predict_punctuation: Whether to predict punctuation and capitalization. \"\"\" super () . __init__ ( sample_rate = sample_rate , model_path = model_path , frame_size = frame_size , * args , ** kwargs , ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_BASIC provider = rt . get_available_providers ()[ 0 ] logger . info ( f \"STT uses { provider } provider\" ) self . stft_filterbanks = librosa . filters . mel ( sr = 16000 , n_fft = 512 , n_mels = 64 , fmin = 0 , fmax = 8000 ) self . stft_window = librosa . filters . get_window ( \"hann\" , 320 , fftbins = False ) self . mel_mean , self . mel_std = self . _fixed_normalization () self . asr_model = rt . InferenceSession ( path . join ( model_path , \"ctc.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . predict_punctuation = predict_punctuation if self . predict_punctuation : self . punctuation = rt . InferenceSession ( path . join ( model_path , \"punctuation.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( path . join ( model_path , \"tokenizer.json\" ) ) self . asr_vocab = \" abcdefghijklmnopqrstuvwxyz'\" self . asr_vocab = list ( self . asr_vocab ) self . asr_vocab . append ( \"\" ) self . punct_labels = \"O,.?\" self . capit_labels = \"OU\" self . decoder = build_ctcdecoder ( self . asr_vocab ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_ALL self . sentence_model = rt . InferenceSession ( path . join ( model_path , \"sentence_prediction.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . sentence_tokenizer = Tokenizer . from_file ( path . join ( model_path , \"sentence_tokenizer.json\" ) ) logger . info ( f \"Sentence classifier uses { rt . get_available_providers ()[ 0 ] } provider\" ) def transcribe ( self , audio : List [ float ]) -> np . ndarray : \"\"\"Transcribe audio usign this pipeline. Args: audio: ndarray of int16 of shape (samples,). Returns: Transcribed text from the audio. \"\"\" logits = self . _predict ( np . asarray ( audio , dtype = np . float32 )) return logits def decode ( self , logits : np . ndarray ) -> str : \"\"\"Decode logits into text. Args: logits: ndarray of float32 of shape (timesteps, vocab_size). Returns: Decoded string. \"\"\" return self . decoder . decode ( logits ) def decide_finished ( self , context : str , text : str ) -> bool : \"\"\"Decide if audio transcription should be finished. Args: context: Text context of the speech recognized (e.g. a question to which speech recognized is a reply to). text: Recognized speech so far Returns: Decision to stop recognition and finalize results. \"\"\" decision = self . _decide_sentence_finished ( context , text ) done = bool ( decision ) return done def postprocess ( self , text : str ) -> str : \"\"\"Add punctuation and capitalization. Args: text: audio transcription. Returns: Postprocessed text transcribtion. \"\"\" if self . predict_punctuation : enc = self . tokenizer . encode ( text ) punct , capit = self . punctuation . run ( None , { \"input_ids\" : np . asarray ( enc . ids , np . int64 ) . reshape ([ 1 , - 1 ]), \"attention_mask\" : np . asarray ( enc . attention_mask , np . int64 ) . reshape ( [ 1 , - 1 ] ), }, ) punct = punct . argmax ( 2 ) capit = capit . argmax ( 2 ) punctuated_capitalized = self . _apply_punct_capit_predictions ( text , punct [ 0 ][ 1 : - 1 ] . tolist (), capit [ 0 ][ 1 : - 1 ] . tolist () ) return punctuated_capitalized else : return text def _decide_sentence_finished ( self , context , text ): tokenized = self . sentence_tokenizer . encode ( context , text ) ids = np . asarray ( tokenized . ids ) . reshape ([ 1 , - 1 ]) . astype ( np . int64 ) type_ids = np . asarray ( tokenized . type_ids ) . reshape ([ 1 , - 1 ]) . astype ( np . int64 ) attention_mask = np . ones_like ( ids ) input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : type_ids , } logits = self . sentence_model . run ( None , input_dict )[ 0 ] return logits . argmax ( - 1 )[ 0 ] def _predict ( self , audio : np . ndarray ) -> np . ndarray : signal = audio . reshape ([ 1 , - 1 ]) audio_signal = self . _preprocess_signal ( signal ) . astype ( np . float32 ) return self . asr_model . run ( None , { \"audio_signal\" : audio_signal })[ 0 ][ 0 ] def _preprocess_signal ( self , signal ): audio_signal = signal . reshape ([ 1 , - 1 ]) # audio_signal += np.random.rand(*audio_signal.shape) * 1e-5 audio_signal = np . concatenate ( ( audio_signal [:, 0 ] . reshape ([ - 1 , 1 ]), audio_signal [:, 1 :] - 0.97 * audio_signal [:, : - 1 ], ), axis = 1 , ) audio_signal = audio_signal . reshape ([ - 1 ]) spectogram = librosa . stft ( audio_signal , n_fft = 512 , hop_length = 160 , win_length = 320 , window = self . stft_window , center = True , ) spectogram = np . stack ([ spectogram . real , spectogram . imag ], - 1 ) spectogram = np . sqrt (( spectogram ** 2 ) . sum ( - 1 )) spectogram = spectogram ** 2 spectogram = np . dot ( self . stft_filterbanks , spectogram ) spectogram = np . expand_dims ( spectogram , 0 ) spectogram = np . log ( spectogram + ( 2 ** - 24 )) spectogram = spectogram - np . asarray ( self . mel_mean ) . reshape ([ 1 , 64 , 1 ]) spectogram = spectogram / np . asarray ( self . mel_std ) . reshape ([ 1 , 64 , 1 ]) return spectogram def _apply_punct_capit_predictions ( self , query : str , punct_preds , capit_preds ) -> str : \"\"\"Restores punctuation and capitalization in ``query``. Args: query: a string without punctuation and capitalization punct_preds: ids of predicted punctuation labels capit_preds: ids of predicted capitalization labels Returns: a query with restored punctuation and capitalization \"\"\" query = query . strip () . split () query_with_punct_and_capit = \"\" skip = 0 for j , word in enumerate ( query ): word_enc = self . tokenizer . encode ( word , add_special_tokens = False ) . ids skip += len ( word_enc ) - 1 punct_label = self . punct_labels [ punct_preds [ j + skip ]] capit_label = self . capit_labels [ capit_preds [ j + skip ]] if capit_label != \"O\" : word = word . capitalize () query_with_punct_and_capit += word if punct_label != \"O\" : query_with_punct_and_capit += punct_label query_with_punct_and_capit += \" \" return query_with_punct_and_capit [: - 1 ] def _fixed_normalization ( self ): \"\"\"From https://github.com/NVIDIA/NeMo/blob/stable/tutorials/asr/Online_ASR_Microphone_Demo.ipynb . For whatever reason they use these values to normalize the mel spectogram. \"\"\" mel_fixed_mean = [ - 14.95827016 , - 12.71798736 , - 11.76067913 , - 10.83311182 , - 10.6746914 , - 10.15163465 , - 10.05378331 , - 9.53918999 , - 9.41858904 , - 9.23382904 , - 9.46470918 , - 9.56037 , - 9.57434245 , - 9.47498732 , - 9.7635205 , - 10.08113074 , - 10.05454561 , - 9.81112681 , - 9.68673603 , - 9.83652977 , - 9.90046248 , - 9.85404766 , - 9.92560366 , - 9.95440354 , - 10.17162966 , - 9.90102482 , - 9.47471025 , - 9.54416855 , - 10.07109475 , - 9.98249912 , - 9.74359465 , - 9.55632283 , - 9.23399915 , - 9.36487649 , - 9.81791084 , - 9.56799225 , - 9.70630899 , - 9.85148006 , - 9.8594418 , - 10.01378735 , - 9.98505315 , - 9.62016094 , - 10.342285 , - 10.41070709 , - 10.10687659 , - 10.14536695 , - 10.30828702 , - 10.23542833 , - 10.88546868 , - 11.31723646 , - 11.46087382 , - 11.54877829 , - 11.62400934 , - 11.92190509 , - 12.14063815 , - 11.65130117 , - 11.58308531 , - 12.22214663 , - 12.42927197 , - 12.58039805 , - 13.10098969 , - 13.14345864 , - 13.31835645 , - 14.47345634 , ] mel_fixed_std = [ 3.81402054 , 4.12647781 , 4.05007065 , 3.87790987 , 3.74721178 , 3.68377423 , 3.69344 , 3.54001005 , 3.59530412 , 3.63752368 , 3.62826417 , 3.56488469 , 3.53740577 , 3.68313898 , 3.67138151 , 3.55707266 , 3.54919572 , 3.55721289 , 3.56723346 , 3.46029304 , 3.44119672 , 3.49030548 , 3.39328435 , 3.28244406 , 3.28001423 , 3.26744937 , 3.46692348 , 3.35378948 , 2.96330901 , 2.97663111 , 3.04575148 , 2.89717604 , 2.95659301 , 2.90181116 , 2.7111687 , 2.93041291 , 2.86647897 , 2.73473181 , 2.71495654 , 2.75543763 , 2.79174615 , 2.96076456 , 2.57376336 , 2.68789782 , 2.90930817 , 2.90412004 , 2.76187531 , 2.89905006 , 2.65896173 , 2.81032176 , 2.87769857 , 2.84665271 , 2.80863137 , 2.80707634 , 2.83752184 , 3.01914511 , 2.92046439 , 2.78461139 , 2.90034605 , 2.94599508 , 2.99099718 , 3.0167554 , 3.04649716 , 2.94116777 , ] return mel_fixed_mean , mel_fixed_std __init__ ( model_path , frame_size = 1000 , sample_rate = 16000 , predict_punctuation = False , * args , ** kwargs ) Create and load biencoder model for semantic similarity. Parameters: Name Type Description Default model_path str A path where model config and weights are stored. required frame_size int Size of the audio frame in milliseconds. 1000 sample_rate int Sample rate of the audio. 16000 predict_punctuation bool Whether to predict punctuation and capitalization. False Source code in npc_engine\\services\\stt\\nemo_stt.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def __init__ ( self , model_path : str , frame_size : int = 1000 , sample_rate : int = 16000 , predict_punctuation : bool = False , * args , ** kwargs , ): \"\"\"Create and load biencoder model for semantic similarity. Args: model_path: A path where model config and weights are stored. frame_size: Size of the audio frame in milliseconds. sample_rate: Sample rate of the audio. predict_punctuation: Whether to predict punctuation and capitalization. \"\"\" super () . __init__ ( sample_rate = sample_rate , model_path = model_path , frame_size = frame_size , * args , ** kwargs , ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_BASIC provider = rt . get_available_providers ()[ 0 ] logger . info ( f \"STT uses { provider } provider\" ) self . stft_filterbanks = librosa . filters . mel ( sr = 16000 , n_fft = 512 , n_mels = 64 , fmin = 0 , fmax = 8000 ) self . stft_window = librosa . filters . get_window ( \"hann\" , 320 , fftbins = False ) self . mel_mean , self . mel_std = self . _fixed_normalization () self . asr_model = rt . InferenceSession ( path . join ( model_path , \"ctc.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . predict_punctuation = predict_punctuation if self . predict_punctuation : self . punctuation = rt . InferenceSession ( path . join ( model_path , \"punctuation.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( path . join ( model_path , \"tokenizer.json\" ) ) self . asr_vocab = \" abcdefghijklmnopqrstuvwxyz'\" self . asr_vocab = list ( self . asr_vocab ) self . asr_vocab . append ( \"\" ) self . punct_labels = \"O,.?\" self . capit_labels = \"OU\" self . decoder = build_ctcdecoder ( self . asr_vocab ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_ALL self . sentence_model = rt . InferenceSession ( path . join ( model_path , \"sentence_prediction.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . sentence_tokenizer = Tokenizer . from_file ( path . join ( model_path , \"sentence_tokenizer.json\" ) ) logger . info ( f \"Sentence classifier uses { rt . get_available_providers ()[ 0 ] } provider\" ) _apply_punct_capit_predictions ( query , punct_preds , capit_preds ) Restores punctuation and capitalization in query . Parameters: Name Type Description Default query str a string without punctuation and capitalization required punct_preds ids of predicted punctuation labels required capit_preds ids of predicted capitalization labels required Returns: Type Description str a query with restored punctuation and capitalization Source code in npc_engine\\services\\stt\\nemo_stt.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 def _apply_punct_capit_predictions ( self , query : str , punct_preds , capit_preds ) -> str : \"\"\"Restores punctuation and capitalization in ``query``. Args: query: a string without punctuation and capitalization punct_preds: ids of predicted punctuation labels capit_preds: ids of predicted capitalization labels Returns: a query with restored punctuation and capitalization \"\"\" query = query . strip () . split () query_with_punct_and_capit = \"\" skip = 0 for j , word in enumerate ( query ): word_enc = self . tokenizer . encode ( word , add_special_tokens = False ) . ids skip += len ( word_enc ) - 1 punct_label = self . punct_labels [ punct_preds [ j + skip ]] capit_label = self . capit_labels [ capit_preds [ j + skip ]] if capit_label != \"O\" : word = word . capitalize () query_with_punct_and_capit += word if punct_label != \"O\" : query_with_punct_and_capit += punct_label query_with_punct_and_capit += \" \" return query_with_punct_and_capit [: - 1 ] _fixed_normalization () From https://github.com/NVIDIA/NeMo/blob/stable/tutorials/asr/Online_ASR_Microphone_Demo.ipynb . For whatever reason they use these values to normalize the mel spectogram. Source code in npc_engine\\services\\stt\\nemo_stt.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 def _fixed_normalization ( self ): \"\"\"From https://github.com/NVIDIA/NeMo/blob/stable/tutorials/asr/Online_ASR_Microphone_Demo.ipynb . For whatever reason they use these values to normalize the mel spectogram. \"\"\" mel_fixed_mean = [ - 14.95827016 , - 12.71798736 , - 11.76067913 , - 10.83311182 , - 10.6746914 , - 10.15163465 , - 10.05378331 , - 9.53918999 , - 9.41858904 , - 9.23382904 , - 9.46470918 , - 9.56037 , - 9.57434245 , - 9.47498732 , - 9.7635205 , - 10.08113074 , - 10.05454561 , - 9.81112681 , - 9.68673603 , - 9.83652977 , - 9.90046248 , - 9.85404766 , - 9.92560366 , - 9.95440354 , - 10.17162966 , - 9.90102482 , - 9.47471025 , - 9.54416855 , - 10.07109475 , - 9.98249912 , - 9.74359465 , - 9.55632283 , - 9.23399915 , - 9.36487649 , - 9.81791084 , - 9.56799225 , - 9.70630899 , - 9.85148006 , - 9.8594418 , - 10.01378735 , - 9.98505315 , - 9.62016094 , - 10.342285 , - 10.41070709 , - 10.10687659 , - 10.14536695 , - 10.30828702 , - 10.23542833 , - 10.88546868 , - 11.31723646 , - 11.46087382 , - 11.54877829 , - 11.62400934 , - 11.92190509 , - 12.14063815 , - 11.65130117 , - 11.58308531 , - 12.22214663 , - 12.42927197 , - 12.58039805 , - 13.10098969 , - 13.14345864 , - 13.31835645 , - 14.47345634 , ] mel_fixed_std = [ 3.81402054 , 4.12647781 , 4.05007065 , 3.87790987 , 3.74721178 , 3.68377423 , 3.69344 , 3.54001005 , 3.59530412 , 3.63752368 , 3.62826417 , 3.56488469 , 3.53740577 , 3.68313898 , 3.67138151 , 3.55707266 , 3.54919572 , 3.55721289 , 3.56723346 , 3.46029304 , 3.44119672 , 3.49030548 , 3.39328435 , 3.28244406 , 3.28001423 , 3.26744937 , 3.46692348 , 3.35378948 , 2.96330901 , 2.97663111 , 3.04575148 , 2.89717604 , 2.95659301 , 2.90181116 , 2.7111687 , 2.93041291 , 2.86647897 , 2.73473181 , 2.71495654 , 2.75543763 , 2.79174615 , 2.96076456 , 2.57376336 , 2.68789782 , 2.90930817 , 2.90412004 , 2.76187531 , 2.89905006 , 2.65896173 , 2.81032176 , 2.87769857 , 2.84665271 , 2.80863137 , 2.80707634 , 2.83752184 , 3.01914511 , 2.92046439 , 2.78461139 , 2.90034605 , 2.94599508 , 2.99099718 , 3.0167554 , 3.04649716 , 2.94116777 , ] return mel_fixed_mean , mel_fixed_std decide_finished ( context , text ) Decide if audio transcription should be finished. Parameters: Name Type Description Default context str Text context of the speech recognized (e.g. a question to which speech recognized is a reply to). required text str Recognized speech so far required Returns: Type Description bool Decision to stop recognition and finalize results. Source code in npc_engine\\services\\stt\\nemo_stt.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def decide_finished ( self , context : str , text : str ) -> bool : \"\"\"Decide if audio transcription should be finished. Args: context: Text context of the speech recognized (e.g. a question to which speech recognized is a reply to). text: Recognized speech so far Returns: Decision to stop recognition and finalize results. \"\"\" decision = self . _decide_sentence_finished ( context , text ) done = bool ( decision ) return done decode ( logits ) Decode logits into text. Parameters: Name Type Description Default logits np . ndarray ndarray of float32 of shape (timesteps, vocab_size). required Returns: Type Description str Decoded string. Source code in npc_engine\\services\\stt\\nemo_stt.py 136 137 138 139 140 141 142 143 144 145 def decode ( self , logits : np . ndarray ) -> str : \"\"\"Decode logits into text. Args: logits: ndarray of float32 of shape (timesteps, vocab_size). Returns: Decoded string. \"\"\" return self . decoder . decode ( logits ) postprocess ( text ) Add punctuation and capitalization. Parameters: Name Type Description Default text str audio transcription. required Returns: Type Description str Postprocessed text transcribtion. Source code in npc_engine\\services\\stt\\nemo_stt.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def postprocess ( self , text : str ) -> str : \"\"\"Add punctuation and capitalization. Args: text: audio transcription. Returns: Postprocessed text transcribtion. \"\"\" if self . predict_punctuation : enc = self . tokenizer . encode ( text ) punct , capit = self . punctuation . run ( None , { \"input_ids\" : np . asarray ( enc . ids , np . int64 ) . reshape ([ 1 , - 1 ]), \"attention_mask\" : np . asarray ( enc . attention_mask , np . int64 ) . reshape ( [ 1 , - 1 ] ), }, ) punct = punct . argmax ( 2 ) capit = capit . argmax ( 2 ) punctuated_capitalized = self . _apply_punct_capit_predictions ( text , punct [ 0 ][ 1 : - 1 ] . tolist (), capit [ 0 ][ 1 : - 1 ] . tolist () ) return punctuated_capitalized else : return text transcribe ( audio ) Transcribe audio usign this pipeline. Parameters: Name Type Description Default audio List [ float ] ndarray of int16 of shape (samples,). required Returns: Type Description np . ndarray Transcribed text from the audio. Source code in npc_engine\\services\\stt\\nemo_stt.py 124 125 126 127 128 129 130 131 132 133 134 def transcribe ( self , audio : List [ float ]) -> np . ndarray : \"\"\"Transcribe audio usign this pipeline. Args: audio: ndarray of int16 of shape (samples,). Returns: Transcribed text from the audio. \"\"\" logits = self . _predict ( np . asarray ( audio , dtype = np . float32 )) return logits stt_base Module that implements speech to text model API. SpeechToTextAPI Bases: BaseService Abstract base class for speech to text models. Source code in npc_engine\\services\\stt\\stt_base.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 class SpeechToTextAPI ( BaseService ): \"\"\"Abstract base class for speech to text models.\"\"\" API_METHODS : List [ str ] = [ \"listen\" , \"stt\" , \"get_devices\" , \"select_device\" , \"initialize_microphone_input\" , ] def __init__ ( self , min_speech_duration = 100 , max_silence_duration = 1000 , vad_mode = None , sample_rate = 16000 , vad_frame_ms = 10 , pad_size = 1000 , * args , ** kwargs , ): \"\"\"Initialize VAD part of the API.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True sd . default . samplerate = sample_rate self . max_silence_duration = max_silence_duration self . min_speech_duration = min_speech_duration self . vad = webrtcvad . Vad () if vad_mode is not None : self . vad . set_mode ( vad_mode ) self . sample_rate = sample_rate self . listen_queue = Queue ( 10 ) self . vad_frame_ms = vad_frame_ms self . vad_frame_size = int (( vad_frame_ms * sample_rate ) / 1000 ) self . running = False self . microphone_initialized = False self . silence_buffer = np . empty ([ 0 ]) self . pad_size = pad_size @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"SpeechToTextAPI\" def __del__ ( self ): \"\"\"Stop listening on destruction.\"\"\" if self . microphone_initialized : self . stream . stop () def initialize_microphone_input ( self ): \"\"\"Initialize microphone.\"\"\" if self . microphone_initialized : return self . running = False self . microphone_initialized = True def callback ( in_data , frame_count , time_info , status ): if self . running : try : self . listen_queue . put ( in_data . reshape ( - 1 ), block = False ) except Exception : return else : if not self . _vad_frame ( in_data . reshape ( - 1 ) ): # Register only silence for buffer self . silence_buffer = np . append ( self . silence_buffer , in_data . reshape ( - 1 ) ) self . silence_buffer = self . silence_buffer [ - self . pad_size :] self . stream = sd . InputStream ( samplerate = self . sample_rate , channels = 1 , blocksize = self . vad_frame_size , callback = callback , ) self . stream . start () while self . silence_buffer . shape [ 0 ] < self . pad_size : pass def listen ( self , context : str = None ) -> str : # pragma: no cover \"\"\"Listen for speech input and return text from speech when done. Listens for speech, if speech is active for longer than self.frame_size in milliseconds then starts transcribing it. On each voice activity detection (VAD) pause uses context to decide if transcribed text is a finished response to a context. If it is, applies preprocessing and returns the result. If transcribed text is not a response to a context but VAD pause persists through max_silence_duration then returns the results anyway. Requires a microphone input to be initialized. Args: context: A last line of the dialogue used to decide when to stop listening. It allows our STT system to not wait for a VAD timeout (max_silence_duration in ms). Returns: Recognized text from the audio. \"\"\" if not self . microphone_initialized : raise RuntimeError ( \"Microphone not initialized.\" ) self . listen_queue . queue . clear () context = re . sub ( r \"[^A-Za-z0-9 ]+\" , \"\" , context ) . lower () if context else None text = self . _transcribe_vad_pause ( context ) processed = self . postprocess ( text ) self . listen_queue . queue . clear () return processed def _transcribe_vad_pause ( self , context ) -> str : done = False total_pause_ms = 0 total_speech_ms = 0 speech_appeared = False tested_pause = False logits = None signal = self . silence_buffer self . running = True while not done : try : vad_frame = self . listen_queue . get ( block = False ) except Exception : continue is_speech = self . _vad_frame ( vad_frame ) total_speech_ms , total_pause_ms = self . _update_vad_stats ( is_speech , total_speech_ms , total_pause_ms ) if total_speech_ms > self . min_speech_duration : speech_appeared = True if total_pause_ms == 0 : tested_pause = False signal = np . append ( signal , vad_frame ) if not speech_appeared and total_speech_ms < self . min_speech_duration : # Keep only last minimum detectable speech duration + buffer signal = signal [ - self . _ms_to_samplenum ( self . min_speech_duration * 2 ) :] if signal . shape [ 0 ] >= self . _ms_to_samplenum ( self . min_speech_duration ): if speech_appeared and total_pause_ms > self . max_silence_duration : wrapped_signal = self . _wrap_signal ( signal ) logits = self . transcribe ( wrapped_signal ) text = self . decode ( logits ) done = True self . running = False return text elif ( speech_appeared and total_pause_ms > self . min_speech_duration and not tested_pause ): tested_pause = True logits = self . transcribe ( np . pad ( signal , ( 0 , 1000 ), \"wrap\" )) text = self . decode ( logits ) done = self . decide_finished ( context , text ) self . running = not done self . running = False return text def stt ( self , audio : List [ int ]) -> str : \"\"\"Transcribe speech. Args: audio: PMC data with bit depth 16. Returns: Recognized text from the audio. \"\"\" logits = self . transcribe ( audio ) text = self . decode ( logits ) text = self . postprocess ( text ) return text def get_devices ( self ) -> Dict [ int , str ]: # pragma: no cover \"\"\"Get available audio devices.\"\"\" return [ device [ \"name\" ] for device in sd . query_devices ()] def select_device ( self , device_id : int ): # pragma: no cover \"\"\"Get available audio devices.\"\"\" device_id = int ( device_id ) if device_id >= len ( sd . query_devices ()) or device_id < 0 : raise ValueError ( f \"Bad device id, valid device ids in range [0; { len ( sd . query_devices ()) } )\" ) sd . default . device = device_id def _wrap_signal ( self , signal : np . ndarray ) -> np . ndarray : \"\"\"Append silence buffer at both ends of the signal.\"\"\" signal_with_silence = np . append ( np . append ( self . silence_buffer , signal ), self . silence_buffer ) return signal_with_silence def _vad_frame ( self , frame ): # pragma: no cover \"\"\"Detect voice activity in a frame.\"\"\" is_speech = self . vad . is_speech ( np . frombuffer (( frame * 32767 ) . astype ( np . int16 ) . tobytes (), np . int8 ), sample_rate = 16000 , ) return is_speech def _ms_to_samplenum ( self , time ): return int ( time * self . sample_rate / 1000 ) def _samples_to_ms ( self , samples ): return samples * 1000 / self . sample_rate def _update_vad_stats ( self , is_speech , total_speech_ms , total_pause_ms ): \"\"\"Increase total_speech_ms if is_speech is true and appears for min_speech_duration frames consequently.\"\"\" if is_speech : total_speech_ms += self . vad_frame_ms total_pause_ms = 0 else : total_pause_ms += self . vad_frame_ms total_speech_ms = 0 return total_speech_ms , total_pause_ms @abstractmethod def transcribe ( self , audio : np . ndarray ) -> np . ndarray : \"\"\"Abstract method for audio transcription. Should be implemented by the specific model. Args: audio: ndarray of int16 of shape (samples,). Returns: Transcribed logits from the audio. \"\"\" return None @abstractmethod def decode ( self , logits : np . ndarray ) -> str : \"\"\"Decode logits into text. Args: logits: ndarray of float32 of shape (timesteps, vocab_size). Returns: Decoded string. \"\"\" return None @abstractmethod def decide_finished ( self , context : str , text : str ) -> bool : \"\"\"Abstract method for deciding if audio transcription should be finished. Should be implemented by the specific model. Args: context: Text context of the speech recognized (e.g. a question to which speech recognized is a reply to). text: Recognized speech so far. Returns: Decision to stop recognition and finalize results. \"\"\" return None @abstractmethod def postprocess ( self , text : str ) -> str : \"\"\"Abstract method for audio transcription postprocessing. Should be implemented by the specific model. Args: text: audio transcription. Returns: Postprocessed text transcribtion. \"\"\" return None __del__ () Stop listening on destruction. Source code in npc_engine\\services\\stt\\stt_base.py 62 63 64 65 def __del__ ( self ): \"\"\"Stop listening on destruction.\"\"\" if self . microphone_initialized : self . stream . stop () __init__ ( min_speech_duration = 100 , max_silence_duration = 1000 , vad_mode = None , sample_rate = 16000 , vad_frame_ms = 10 , pad_size = 1000 , * args , ** kwargs ) Initialize VAD part of the API. Source code in npc_engine\\services\\stt\\stt_base.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , min_speech_duration = 100 , max_silence_duration = 1000 , vad_mode = None , sample_rate = 16000 , vad_frame_ms = 10 , pad_size = 1000 , * args , ** kwargs , ): \"\"\"Initialize VAD part of the API.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True sd . default . samplerate = sample_rate self . max_silence_duration = max_silence_duration self . min_speech_duration = min_speech_duration self . vad = webrtcvad . Vad () if vad_mode is not None : self . vad . set_mode ( vad_mode ) self . sample_rate = sample_rate self . listen_queue = Queue ( 10 ) self . vad_frame_ms = vad_frame_ms self . vad_frame_size = int (( vad_frame_ms * sample_rate ) / 1000 ) self . running = False self . microphone_initialized = False self . silence_buffer = np . empty ([ 0 ]) self . pad_size = pad_size _update_vad_stats ( is_speech , total_speech_ms , total_pause_ms ) Increase total_speech_ms if is_speech is true and appears for min_speech_duration frames consequently. Source code in npc_engine\\services\\stt\\stt_base.py 231 232 233 234 235 236 237 238 239 def _update_vad_stats ( self , is_speech , total_speech_ms , total_pause_ms ): \"\"\"Increase total_speech_ms if is_speech is true and appears for min_speech_duration frames consequently.\"\"\" if is_speech : total_speech_ms += self . vad_frame_ms total_pause_ms = 0 else : total_pause_ms += self . vad_frame_ms total_speech_ms = 0 return total_speech_ms , total_pause_ms _vad_frame ( frame ) Detect voice activity in a frame. Source code in npc_engine\\services\\stt\\stt_base.py 217 218 219 220 221 222 223 def _vad_frame ( self , frame ): # pragma: no cover \"\"\"Detect voice activity in a frame.\"\"\" is_speech = self . vad . is_speech ( np . frombuffer (( frame * 32767 ) . astype ( np . int16 ) . tobytes (), np . int8 ), sample_rate = 16000 , ) return is_speech _wrap_signal ( signal ) Append silence buffer at both ends of the signal. Source code in npc_engine\\services\\stt\\stt_base.py 210 211 212 213 214 215 def _wrap_signal ( self , signal : np . ndarray ) -> np . ndarray : \"\"\"Append silence buffer at both ends of the signal.\"\"\" signal_with_silence = np . append ( np . append ( self . silence_buffer , signal ), self . silence_buffer ) return signal_with_silence decide_finished ( context , text ) abstractmethod Abstract method for deciding if audio transcription should be finished. Should be implemented by the specific model. Parameters: Name Type Description Default context str Text context of the speech recognized (e.g. a question to which speech recognized is a reply to). required text str Recognized speech so far. required Returns: Type Description bool Decision to stop recognition and finalize results. Source code in npc_engine\\services\\stt\\stt_base.py 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 @abstractmethod def decide_finished ( self , context : str , text : str ) -> bool : \"\"\"Abstract method for deciding if audio transcription should be finished. Should be implemented by the specific model. Args: context: Text context of the speech recognized (e.g. a question to which speech recognized is a reply to). text: Recognized speech so far. Returns: Decision to stop recognition and finalize results. \"\"\" return None decode ( logits ) abstractmethod Decode logits into text. Parameters: Name Type Description Default logits np . ndarray ndarray of float32 of shape (timesteps, vocab_size). required Returns: Type Description str Decoded string. Source code in npc_engine\\services\\stt\\stt_base.py 255 256 257 258 259 260 261 262 263 264 265 @abstractmethod def decode ( self , logits : np . ndarray ) -> str : \"\"\"Decode logits into text. Args: logits: ndarray of float32 of shape (timesteps, vocab_size). Returns: Decoded string. \"\"\" return None get_api_name () classmethod Get the API name. Source code in npc_engine\\services\\stt\\stt_base.py 57 58 59 60 @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"SpeechToTextAPI\" get_devices () Get available audio devices. Source code in npc_engine\\services\\stt\\stt_base.py 197 198 199 def get_devices ( self ) -> Dict [ int , str ]: # pragma: no cover \"\"\"Get available audio devices.\"\"\" return [ device [ \"name\" ] for device in sd . query_devices ()] initialize_microphone_input () Initialize microphone. Source code in npc_engine\\services\\stt\\stt_base.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def initialize_microphone_input ( self ): \"\"\"Initialize microphone.\"\"\" if self . microphone_initialized : return self . running = False self . microphone_initialized = True def callback ( in_data , frame_count , time_info , status ): if self . running : try : self . listen_queue . put ( in_data . reshape ( - 1 ), block = False ) except Exception : return else : if not self . _vad_frame ( in_data . reshape ( - 1 ) ): # Register only silence for buffer self . silence_buffer = np . append ( self . silence_buffer , in_data . reshape ( - 1 ) ) self . silence_buffer = self . silence_buffer [ - self . pad_size :] self . stream = sd . InputStream ( samplerate = self . sample_rate , channels = 1 , blocksize = self . vad_frame_size , callback = callback , ) self . stream . start () while self . silence_buffer . shape [ 0 ] < self . pad_size : pass listen ( context = None ) Listen for speech input and return text from speech when done. Listens for speech, if speech is active for longer than self.frame_size in milliseconds then starts transcribing it. On each voice activity detection (VAD) pause uses context to decide if transcribed text is a finished response to a context. If it is, applies preprocessing and returns the result. If transcribed text is not a response to a context but VAD pause persists through max_silence_duration then returns the results anyway. Requires a microphone input to be initialized. Parameters: Name Type Description Default context str A last line of the dialogue used to decide when to stop listening. It allows our STT system to not wait for a VAD timeout (max_silence_duration in ms). None Returns: Type Description str Recognized text from the audio. Source code in npc_engine\\services\\stt\\stt_base.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def listen ( self , context : str = None ) -> str : # pragma: no cover \"\"\"Listen for speech input and return text from speech when done. Listens for speech, if speech is active for longer than self.frame_size in milliseconds then starts transcribing it. On each voice activity detection (VAD) pause uses context to decide if transcribed text is a finished response to a context. If it is, applies preprocessing and returns the result. If transcribed text is not a response to a context but VAD pause persists through max_silence_duration then returns the results anyway. Requires a microphone input to be initialized. Args: context: A last line of the dialogue used to decide when to stop listening. It allows our STT system to not wait for a VAD timeout (max_silence_duration in ms). Returns: Recognized text from the audio. \"\"\" if not self . microphone_initialized : raise RuntimeError ( \"Microphone not initialized.\" ) self . listen_queue . queue . clear () context = re . sub ( r \"[^A-Za-z0-9 ]+\" , \"\" , context ) . lower () if context else None text = self . _transcribe_vad_pause ( context ) processed = self . postprocess ( text ) self . listen_queue . queue . clear () return processed postprocess ( text ) abstractmethod Abstract method for audio transcription postprocessing. Should be implemented by the specific model. Parameters: Name Type Description Default text str audio transcription. required Returns: Type Description str Postprocessed text transcribtion. Source code in npc_engine\\services\\stt\\stt_base.py 283 284 285 286 287 288 289 290 291 292 293 294 295 @abstractmethod def postprocess ( self , text : str ) -> str : \"\"\"Abstract method for audio transcription postprocessing. Should be implemented by the specific model. Args: text: audio transcription. Returns: Postprocessed text transcribtion. \"\"\" return None select_device ( device_id ) Get available audio devices. Source code in npc_engine\\services\\stt\\stt_base.py 201 202 203 204 205 206 207 208 def select_device ( self , device_id : int ): # pragma: no cover \"\"\"Get available audio devices.\"\"\" device_id = int ( device_id ) if device_id >= len ( sd . query_devices ()) or device_id < 0 : raise ValueError ( f \"Bad device id, valid device ids in range [0; { len ( sd . query_devices ()) } )\" ) sd . default . device = device_id stt ( audio ) Transcribe speech. Parameters: Name Type Description Default audio List [ int ] PMC data with bit depth 16. required Returns: Type Description str Recognized text from the audio. Source code in npc_engine\\services\\stt\\stt_base.py 183 184 185 186 187 188 189 190 191 192 193 194 195 def stt ( self , audio : List [ int ]) -> str : \"\"\"Transcribe speech. Args: audio: PMC data with bit depth 16. Returns: Recognized text from the audio. \"\"\" logits = self . transcribe ( audio ) text = self . decode ( logits ) text = self . postprocess ( text ) return text transcribe ( audio ) abstractmethod Abstract method for audio transcription. Should be implemented by the specific model. Parameters: Name Type Description Default audio np . ndarray ndarray of int16 of shape (samples,). required Returns: Type Description np . ndarray Transcribed logits from the audio. Source code in npc_engine\\services\\stt\\stt_base.py 241 242 243 244 245 246 247 248 249 250 251 252 253 @abstractmethod def transcribe ( self , audio : np . ndarray ) -> np . ndarray : \"\"\"Abstract method for audio transcription. Should be implemented by the specific model. Args: audio: ndarray of int16 of shape (samples,). Returns: Transcribed logits from the audio. \"\"\" return None text_generation Text generation API services. This module implements specific models and wraps them under the common interface for loading and inference. Example from npc_engine.services.text_generation import TextGenerationAPI model = TextGenerationAPI.load(\"path/to/model_dir\") model.generate_reply(context, temperature=0.8, topk=None,) bart BART based chatbot implementation. BartChatbot Bases: TextGenerationAPI BART based chatbot implementation class. This model class requires two ONNX models encoder_bart.onnx and decoder_bart.onnx that correspond to encoder and decoder from transformers EncoderDecoderModel and a tokenizer.json with huggingface tokenizers definition. encoder_bart.onnx spec: - inputs: `input_ids` - outputs: `encoder_hidden_state` decoder_bart.onnx spec: - inputs: `encoder_hidden_state` `decoder_input_ids` - outputs: `logits` Source code in npc_engine\\services\\text_generation\\bart.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 class BartChatbot ( TextGenerationAPI ): \"\"\"BART based chatbot implementation class. This model class requires two ONNX models `encoder_bart.onnx` and `decoder_bart.onnx` that correspond to encoder and decoder from transformers [EncoderDecoderModel](https://huggingface.co/transformers/model_doc/encoderdecoder.html) and a tokenizer.json with huggingface tokenizers definition. encoder_bart.onnx spec: - inputs: `input_ids` - outputs: `encoder_hidden_state` decoder_bart.onnx spec: - inputs: `encoder_hidden_state` `decoder_input_ids` - outputs: `logits` \"\"\" def __init__ ( self , model_path , max_steps = 100 , min_length = 2 , repetition_penalty = 1 , bos_token_id = 0 , eos_token_id = 2 , pad_token_id = 1 , sep_token_id = None , trunc_length = 512 , * args , ** kwargs , ): \"\"\"Create the chatbot from config args and kwargs. Args: model_path: path to scan for model files (weights and configs) max_steps: stop generation at this number of tokens min_length: model can't stop generating text before it's atleast this long in tokens repetition_penalty: probability coef for same tokens to appear multiple times bos_token_id: beginning of sequence token id eos_token_id: end of sequence token id pad_token_id: padding token id sep_token_id: token id for separating sequence into multiple parts \"\"\" super () . __init__ ( * args , ** kwargs ) self . bos_token_id = bos_token_id self . eos_token_id = eos_token_id self . sep_token_id = eos_token_id if sep_token_id is None else sep_token_id self . pad_token_id = pad_token_id sess_options = rt . SessionOptions () sess_options . graph_optimization_level = ( rt . GraphOptimizationLevel . ORT_ENABLE_ALL ) self . encoder_model = rt . InferenceSession ( os . path . join ( model_path , \"encoder_bart.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . decoder_model = rt . InferenceSession ( os . path . join ( model_path , \"decoder_bart.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) added_tokens_path = os . path . join ( model_path , \"added_tokens.txt\" ) if os . path . exists ( added_tokens_path ): with open ( added_tokens_path ) as f : added_tokens = json . load ( f ) added_tokens = [ key for key , _ in sorted ( list ( added_tokens . items ()), key = lambda x : x [ 1 ]) ] self . tokenizer . add_tokens ( added_tokens ) self . special_tokens = { \"bos_token\" : self . tokenizer . decode ( [ bos_token_id ], skip_special_tokens = False ), \"eos_token\" : self . tokenizer . decode ( [ eos_token_id ], skip_special_tokens = False ), \"sep_token\" : self . tokenizer . decode ( [ self . sep_token_id ], skip_special_tokens = False ), \"pad_token\" : self . tokenizer . decode ( [ pad_token_id ], skip_special_tokens = False ), ** { f \"added_token { self . tokenizer . token_to_id ( token ) } \" : token for token in added_tokens }, } self . max_steps = max_steps self . min_length = min_length self . repetition_penalty = repetition_penalty self . trunc_length = trunc_length def run ( self , prompt : str , temperature : float = 1.0 , topk : int = None ) -> str : \"\"\"Run text generation from given prompt and parameters. Args: prompt: Fromatted prompt. temperature: Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness topk: If not none selects top n of predictions to sample from during generation. Returns: Generated text \"\"\" tokens = self . tokenizer . encode ( prompt ) total = np . asarray ( tokens . ids , dtype = np . int64 ) . reshape ([ 1 , - 1 ]) total_enc = self . encoder_model . run ( None , { \"input_ids\" : total })[ 0 ] utterance = np . asarray ([ self . eos_token_id ], dtype = np . int64 ) . reshape ([ 1 , 1 ]) for i in range ( self . max_steps ): o = self . decoder_model . run ( None , { \"encoder_hidden_state\" : total_enc , \"decoder_input_ids\" : utterance }, ) logits = o [ 0 ][ 0 , - 1 , :] if i < self . min_length : logits [ self . eos_token_id ] = float ( \"-inf\" ) if topk is not None : ind = np . argpartition ( logits , - topk )[ - topk :] new_logits = np . zeros ( logits . shape ) new_logits [ ind ] = logits [ ind ] logits = new_logits probs = scp . softmax ( logits / temperature , axis = 0 ) token = np . random . choice ( np . arange ( probs . shape [ 0 ]), p = probs ) token = token . reshape ([ 1 , 1 ]) utterance = np . concatenate ([ utterance , token ], axis = 1 ) if token [ 0 , 0 ] == self . eos_token_id : break return self . tokenizer . decode ( utterance [ 0 , :] . tolist (), skip_special_tokens = True ) def get_special_tokens ( self ) -> Dict [ str , str ]: \"\"\"Retrun dict of special tokens to be renderable from template.\"\"\" return self . special_tokens def string_too_long ( self , prompt ): \"\"\"Check if prompt is too long for the model.\"\"\" return len ( self . tokenizer . encode ( prompt )) > self . trunc_length __init__ ( model_path , max_steps = 100 , min_length = 2 , repetition_penalty = 1 , bos_token_id = 0 , eos_token_id = 2 , pad_token_id = 1 , sep_token_id = None , trunc_length = 512 , * args , ** kwargs ) Create the chatbot from config args and kwargs. Parameters: Name Type Description Default model_path path to scan for model files (weights and configs) required max_steps stop generation at this number of tokens 100 min_length model can't stop generating text before it's atleast this long in tokens 2 repetition_penalty probability coef for same tokens to appear multiple times 1 bos_token_id beginning of sequence token id 0 eos_token_id end of sequence token id 2 pad_token_id padding token id 1 sep_token_id token id for separating sequence into multiple parts None Source code in npc_engine\\services\\text_generation\\bart.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def __init__ ( self , model_path , max_steps = 100 , min_length = 2 , repetition_penalty = 1 , bos_token_id = 0 , eos_token_id = 2 , pad_token_id = 1 , sep_token_id = None , trunc_length = 512 , * args , ** kwargs , ): \"\"\"Create the chatbot from config args and kwargs. Args: model_path: path to scan for model files (weights and configs) max_steps: stop generation at this number of tokens min_length: model can't stop generating text before it's atleast this long in tokens repetition_penalty: probability coef for same tokens to appear multiple times bos_token_id: beginning of sequence token id eos_token_id: end of sequence token id pad_token_id: padding token id sep_token_id: token id for separating sequence into multiple parts \"\"\" super () . __init__ ( * args , ** kwargs ) self . bos_token_id = bos_token_id self . eos_token_id = eos_token_id self . sep_token_id = eos_token_id if sep_token_id is None else sep_token_id self . pad_token_id = pad_token_id sess_options = rt . SessionOptions () sess_options . graph_optimization_level = ( rt . GraphOptimizationLevel . ORT_ENABLE_ALL ) self . encoder_model = rt . InferenceSession ( os . path . join ( model_path , \"encoder_bart.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . decoder_model = rt . InferenceSession ( os . path . join ( model_path , \"decoder_bart.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) added_tokens_path = os . path . join ( model_path , \"added_tokens.txt\" ) if os . path . exists ( added_tokens_path ): with open ( added_tokens_path ) as f : added_tokens = json . load ( f ) added_tokens = [ key for key , _ in sorted ( list ( added_tokens . items ()), key = lambda x : x [ 1 ]) ] self . tokenizer . add_tokens ( added_tokens ) self . special_tokens = { \"bos_token\" : self . tokenizer . decode ( [ bos_token_id ], skip_special_tokens = False ), \"eos_token\" : self . tokenizer . decode ( [ eos_token_id ], skip_special_tokens = False ), \"sep_token\" : self . tokenizer . decode ( [ self . sep_token_id ], skip_special_tokens = False ), \"pad_token\" : self . tokenizer . decode ( [ pad_token_id ], skip_special_tokens = False ), ** { f \"added_token { self . tokenizer . token_to_id ( token ) } \" : token for token in added_tokens }, } self . max_steps = max_steps self . min_length = min_length self . repetition_penalty = repetition_penalty self . trunc_length = trunc_length get_special_tokens () Retrun dict of special tokens to be renderable from template. Source code in npc_engine\\services\\text_generation\\bart.py 159 160 161 def get_special_tokens ( self ) -> Dict [ str , str ]: \"\"\"Retrun dict of special tokens to be renderable from template.\"\"\" return self . special_tokens run ( prompt , temperature = 1.0 , topk = None ) Run text generation from given prompt and parameters. Parameters: Name Type Description Default prompt str Fromatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1.0 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text Source code in npc_engine\\services\\text_generation\\bart.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def run ( self , prompt : str , temperature : float = 1.0 , topk : int = None ) -> str : \"\"\"Run text generation from given prompt and parameters. Args: prompt: Fromatted prompt. temperature: Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness topk: If not none selects top n of predictions to sample from during generation. Returns: Generated text \"\"\" tokens = self . tokenizer . encode ( prompt ) total = np . asarray ( tokens . ids , dtype = np . int64 ) . reshape ([ 1 , - 1 ]) total_enc = self . encoder_model . run ( None , { \"input_ids\" : total })[ 0 ] utterance = np . asarray ([ self . eos_token_id ], dtype = np . int64 ) . reshape ([ 1 , 1 ]) for i in range ( self . max_steps ): o = self . decoder_model . run ( None , { \"encoder_hidden_state\" : total_enc , \"decoder_input_ids\" : utterance }, ) logits = o [ 0 ][ 0 , - 1 , :] if i < self . min_length : logits [ self . eos_token_id ] = float ( \"-inf\" ) if topk is not None : ind = np . argpartition ( logits , - topk )[ - topk :] new_logits = np . zeros ( logits . shape ) new_logits [ ind ] = logits [ ind ] logits = new_logits probs = scp . softmax ( logits / temperature , axis = 0 ) token = np . random . choice ( np . arange ( probs . shape [ 0 ]), p = probs ) token = token . reshape ([ 1 , 1 ]) utterance = np . concatenate ([ utterance , token ], axis = 1 ) if token [ 0 , 0 ] == self . eos_token_id : break return self . tokenizer . decode ( utterance [ 0 , :] . tolist (), skip_special_tokens = True ) string_too_long ( prompt ) Check if prompt is too long for the model. Source code in npc_engine\\services\\text_generation\\bart.py 163 164 165 def string_too_long ( self , prompt ): \"\"\"Check if prompt is too long for the model.\"\"\" return len ( self . tokenizer . encode ( prompt )) > self . trunc_length hf_text_generation BART based chatbot implementation. HfChatbot Bases: TextGenerationAPI Chatbot that uses Huggingface transformer architectures. ONNX export of Huggingface transformer is required (see https://huggingface.co/docs/transformers/serialization). Features seq2seq-lm, causal-lm, seq2seq-lm-with-past, causal-lm-with-past are supported Source code in npc_engine\\services\\text_generation\\hf_text_generation.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 class HfChatbot ( TextGenerationAPI ): \"\"\"Chatbot that uses Huggingface transformer architectures. ONNX export of Huggingface transformer is required (see https://huggingface.co/docs/transformers/serialization). Features seq2seq-lm, causal-lm, seq2seq-lm-with-past, causal-lm-with-past are supported \"\"\" def __init__ ( self , model_path : str , max_length : int = 100 , min_length : int = 2 , repetition_penalty : float = 1 , trunc_length : int = 512 , * args , ** kwargs , ): \"\"\"Create the chatbot from config args and kwargs. Args: model_path: path to scan for model files (weights and configs) max_length: stop generation at this number of tokens min_length: model can't stop generating text before it's atleast this long in tokens repetition_penalty: probability coef for same tokens to appear multiple times trunc_length: length to truncate model prompt to \"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = rt . GraphOptimizationLevel . ORT_ENABLE_ALL self . model = rt . InferenceSession ( os . path . join ( model_path , \"model.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) self . max_steps = max_length self . min_length = min_length self . repetition_penalty = repetition_penalty special_tokens_map_path = os . path . join ( model_path , \"special_tokens_map.json\" ) with open ( special_tokens_map_path , \"r\" ) as f : self . special_tokens = json . load ( f ) self . eos_token_id = self . tokenizer . encode ( self . special_tokens [ \"eos_token\" ]) . ids [ 0 ] self . model_inputs = self . model . get_inputs () self . is_encdec = ( len ([ i . name for i in self . model_inputs if \"decoder\" in i . name ]) > 0 ) self . with_past = ( len ([ i . name for i in self . model_inputs if \"past_key_values\" in i . name ]) > 0 ) self . shape_dict = { \"batch\" : 1 , \"past_encoder_sequence\" : 0 , \"past_decoder_sequence\" : 0 , \"past_sequence + sequence\" : 0 , } self . dtypes = { i . name : DTYPE_MAP [ i . type ] for i in self . model_inputs } self . trunc_length = trunc_length def run ( self , prompt : str , temperature : float = 1.0 , topk : int = None ) -> str : \"\"\"Run text generation from given prompt and parameters. Args: prompt: Formatted prompt. temperature: Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness topk: If not none selects top n of predictions to sample from during generation. Returns: Generated text \"\"\" inputs = self . create_starter_inputs ( prompt ) utterance = [] for i in range ( self . max_steps ): o = self . model . run ( None , inputs , ) logit = o [ 0 ][ 0 , - 1 , :] if i < self . min_length : logit [ self . eos_token_id ] = float ( \"-inf\" ) token = self . decode_logit ( logit , temperature , topk ) utterance . append ( token ) result_dict = { outp . name : o [ i ] for i , outp in enumerate ( self . model . get_outputs ()) } inputs = self . update_inputs_with_results ( inputs , result_dict , token ) if token == self . eos_token_id : break return self . tokenizer . decode ( utterance , skip_special_tokens = True ) def create_starter_inputs ( self , prompt : str = \"\" ) -> Dict [ str , Any ]: \"\"\"Create starter inputs for the model. Args: prompt: Prompt to start generation from. Returns: Dict of inputs to the model \"\"\" tokens = self . tokenizer . encode ( prompt ) . ids inputs = {} if self . is_encdec : prompt_start = tokens [ - 1 :] inputs [ \"input_ids\" ] = np . asarray ( tokens [: - 1 ], dtype = self . dtypes [ \"input_ids\" ] ) . reshape ([ 1 , - 1 ]) inputs [ \"decoder_input_ids\" ] = np . asarray ( prompt_start , dtype = self . dtypes [ \"decoder_input_ids\" ] ) . reshape ([ 1 , - 1 ]) inputs [ \"attention_mask\" ] = np . ones ( [ 1 , 6 ], dtype = self . dtypes [ \"attention_mask\" ] ) inputs [ \"decoder_attention_mask\" ] = np . ones ( [ 1 , 3 ], dtype = self . dtypes [ \"decoder_attention_mask\" ] ) else : inputs [ \"input_ids\" ] = np . asarray ( tokens , dtype = self . dtypes [ \"input_ids\" ] ) . reshape ([ 1 , - 1 ]) inputs [ \"attention_mask\" ] = np . ones_like ( inputs [ \"input_ids\" ], dtype = self . dtypes [ \"attention_mask\" ] ) if self . with_past : for i in self . model_inputs : if \"past_key_values\" in i . name : shape_tuple = [ self . shape_dict . get ( dim , dim ) for dim in i . shape ] inputs [ i . name ] = np . empty ( shape_tuple , dtype = self . dtypes [ i . name ]) return inputs def decode_logit ( self , logit : np . ndarray , temperature : float , topk : int ) -> int : \"\"\"Decode logit to token. Args: logit: Logit to decode of shape (vocab_size,) Returns: Decoded token of shape \"\"\" if topk is not None : ind = np . argpartition ( logit , - topk )[ - topk :] new_logits = np . zeros ( logit . shape ) new_logits [ ind ] = logit [ ind ] logit = new_logits probs = scp . softmax ( logit / temperature , axis = 0 ) token = np . random . choice ( np . arange ( probs . shape [ 0 ]), p = probs ) token = token . ravel ()[ 0 ] return token def update_inputs_with_results ( self , inputs : Dict [ str , np . ndarray ], results : Dict [ str , np . ndarray ], decoded_token : int , ) -> Dict [ str , np . ndarray ]: \"\"\"Update inputs with results from model. Args: inputs: Inputs to the model results: Results from the model Returns: Updated inputs Finished generation \"\"\" ids_name = \"decoder_input_ids\" if self . is_encdec else \"input_ids\" att_mask_name = \"decoder_attention_mask\" if self . is_encdec else \"attention_mask\" if self . with_past : inputs [ ids_name ] = np . asarray ( [ decoded_token ], dtype = self . dtypes [ ids_name ] ) . reshape ([ 1 , - 1 ]) inputs [ att_mask_name ] = np . ones ( [ 1 , inputs [ att_mask_name ] . shape [ - 1 ] + 1 ], dtype = self . dtypes [ att_mask_name ], ) for inp in self . model_inputs : if \"past_key_values\" in inp . name : inputs [ inp . name ] = results [ inp . name . replace ( \"past_key_values\" , \"present\" ) ] if self . is_encdec : inputs . pop ( \"input_ids\" , None ) else : decoder_input_ids = inputs [ ids_name ] decoder_attention_mask = inputs [ att_mask_name ] decoder_input_ids = np . concatenate ( [ decoder_input_ids , np . asarray ([[ decoded_token ]], dtype = np . int32 )], axis = 1 , ) decoder_attention_mask = np . ones_like ( decoder_input_ids , dtype = self . dtypes [ att_mask_name ] ) inputs [ ids_name ] = decoder_input_ids inputs [ att_mask_name ] = decoder_attention_mask return inputs def get_special_tokens ( self ) -> Dict [ str , str ]: \"\"\"Return dict of special tokens to be renderable from template.\"\"\" return self . special_tokens def string_too_long ( self , prompt ): \"\"\"Check if prompt is too long for the model.\"\"\" return len ( self . tokenizer . encode ( prompt )) > self . trunc_length __init__ ( model_path , max_length = 100 , min_length = 2 , repetition_penalty = 1 , trunc_length = 512 , * args , ** kwargs ) Create the chatbot from config args and kwargs. Parameters: Name Type Description Default model_path str path to scan for model files (weights and configs) required max_length int stop generation at this number of tokens 100 min_length int model can't stop generating text before it's atleast this long in tokens 2 repetition_penalty float probability coef for same tokens to appear multiple times 1 trunc_length int length to truncate model prompt to 512 Source code in npc_engine\\services\\text_generation\\hf_text_generation.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , model_path : str , max_length : int = 100 , min_length : int = 2 , repetition_penalty : float = 1 , trunc_length : int = 512 , * args , ** kwargs , ): \"\"\"Create the chatbot from config args and kwargs. Args: model_path: path to scan for model files (weights and configs) max_length: stop generation at this number of tokens min_length: model can't stop generating text before it's atleast this long in tokens repetition_penalty: probability coef for same tokens to appear multiple times trunc_length: length to truncate model prompt to \"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = rt . GraphOptimizationLevel . ORT_ENABLE_ALL self . model = rt . InferenceSession ( os . path . join ( model_path , \"model.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) self . max_steps = max_length self . min_length = min_length self . repetition_penalty = repetition_penalty special_tokens_map_path = os . path . join ( model_path , \"special_tokens_map.json\" ) with open ( special_tokens_map_path , \"r\" ) as f : self . special_tokens = json . load ( f ) self . eos_token_id = self . tokenizer . encode ( self . special_tokens [ \"eos_token\" ]) . ids [ 0 ] self . model_inputs = self . model . get_inputs () self . is_encdec = ( len ([ i . name for i in self . model_inputs if \"decoder\" in i . name ]) > 0 ) self . with_past = ( len ([ i . name for i in self . model_inputs if \"past_key_values\" in i . name ]) > 0 ) self . shape_dict = { \"batch\" : 1 , \"past_encoder_sequence\" : 0 , \"past_decoder_sequence\" : 0 , \"past_sequence + sequence\" : 0 , } self . dtypes = { i . name : DTYPE_MAP [ i . type ] for i in self . model_inputs } self . trunc_length = trunc_length create_starter_inputs ( prompt = '' ) Create starter inputs for the model. Parameters: Name Type Description Default prompt str Prompt to start generation from. '' Returns: Type Description Dict [ str , Any ] Dict of inputs to the model Source code in npc_engine\\services\\text_generation\\hf_text_generation.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def create_starter_inputs ( self , prompt : str = \"\" ) -> Dict [ str , Any ]: \"\"\"Create starter inputs for the model. Args: prompt: Prompt to start generation from. Returns: Dict of inputs to the model \"\"\" tokens = self . tokenizer . encode ( prompt ) . ids inputs = {} if self . is_encdec : prompt_start = tokens [ - 1 :] inputs [ \"input_ids\" ] = np . asarray ( tokens [: - 1 ], dtype = self . dtypes [ \"input_ids\" ] ) . reshape ([ 1 , - 1 ]) inputs [ \"decoder_input_ids\" ] = np . asarray ( prompt_start , dtype = self . dtypes [ \"decoder_input_ids\" ] ) . reshape ([ 1 , - 1 ]) inputs [ \"attention_mask\" ] = np . ones ( [ 1 , 6 ], dtype = self . dtypes [ \"attention_mask\" ] ) inputs [ \"decoder_attention_mask\" ] = np . ones ( [ 1 , 3 ], dtype = self . dtypes [ \"decoder_attention_mask\" ] ) else : inputs [ \"input_ids\" ] = np . asarray ( tokens , dtype = self . dtypes [ \"input_ids\" ] ) . reshape ([ 1 , - 1 ]) inputs [ \"attention_mask\" ] = np . ones_like ( inputs [ \"input_ids\" ], dtype = self . dtypes [ \"attention_mask\" ] ) if self . with_past : for i in self . model_inputs : if \"past_key_values\" in i . name : shape_tuple = [ self . shape_dict . get ( dim , dim ) for dim in i . shape ] inputs [ i . name ] = np . empty ( shape_tuple , dtype = self . dtypes [ i . name ]) return inputs decode_logit ( logit , temperature , topk ) Decode logit to token. Parameters: Name Type Description Default logit np . ndarray Logit to decode of shape (vocab_size,) required Returns: Type Description int Decoded token of shape Source code in npc_engine\\services\\text_generation\\hf_text_generation.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def decode_logit ( self , logit : np . ndarray , temperature : float , topk : int ) -> int : \"\"\"Decode logit to token. Args: logit: Logit to decode of shape (vocab_size,) Returns: Decoded token of shape \"\"\" if topk is not None : ind = np . argpartition ( logit , - topk )[ - topk :] new_logits = np . zeros ( logit . shape ) new_logits [ ind ] = logit [ ind ] logit = new_logits probs = scp . softmax ( logit / temperature , axis = 0 ) token = np . random . choice ( np . arange ( probs . shape [ 0 ]), p = probs ) token = token . ravel ()[ 0 ] return token get_special_tokens () Return dict of special tokens to be renderable from template. Source code in npc_engine\\services\\text_generation\\hf_text_generation.py 219 220 221 def get_special_tokens ( self ) -> Dict [ str , str ]: \"\"\"Return dict of special tokens to be renderable from template.\"\"\" return self . special_tokens run ( prompt , temperature = 1.0 , topk = None ) Run text generation from given prompt and parameters. Parameters: Name Type Description Default prompt str Formatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1.0 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text Source code in npc_engine\\services\\text_generation\\hf_text_generation.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def run ( self , prompt : str , temperature : float = 1.0 , topk : int = None ) -> str : \"\"\"Run text generation from given prompt and parameters. Args: prompt: Formatted prompt. temperature: Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness topk: If not none selects top n of predictions to sample from during generation. Returns: Generated text \"\"\" inputs = self . create_starter_inputs ( prompt ) utterance = [] for i in range ( self . max_steps ): o = self . model . run ( None , inputs , ) logit = o [ 0 ][ 0 , - 1 , :] if i < self . min_length : logit [ self . eos_token_id ] = float ( \"-inf\" ) token = self . decode_logit ( logit , temperature , topk ) utterance . append ( token ) result_dict = { outp . name : o [ i ] for i , outp in enumerate ( self . model . get_outputs ()) } inputs = self . update_inputs_with_results ( inputs , result_dict , token ) if token == self . eos_token_id : break return self . tokenizer . decode ( utterance , skip_special_tokens = True ) string_too_long ( prompt ) Check if prompt is too long for the model. Source code in npc_engine\\services\\text_generation\\hf_text_generation.py 223 224 225 def string_too_long ( self , prompt ): \"\"\"Check if prompt is too long for the model.\"\"\" return len ( self . tokenizer . encode ( prompt )) > self . trunc_length update_inputs_with_results ( inputs , results , decoded_token ) Update inputs with results from model. Parameters: Name Type Description Default inputs Dict [ str , np . ndarray ] Inputs to the model required results Dict [ str , np . ndarray ] Results from the model required Returns: Type Description Dict [ str , np . ndarray ] Updated inputs Dict [ str , np . ndarray ] Finished generation Source code in npc_engine\\services\\text_generation\\hf_text_generation.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 def update_inputs_with_results ( self , inputs : Dict [ str , np . ndarray ], results : Dict [ str , np . ndarray ], decoded_token : int , ) -> Dict [ str , np . ndarray ]: \"\"\"Update inputs with results from model. Args: inputs: Inputs to the model results: Results from the model Returns: Updated inputs Finished generation \"\"\" ids_name = \"decoder_input_ids\" if self . is_encdec else \"input_ids\" att_mask_name = \"decoder_attention_mask\" if self . is_encdec else \"attention_mask\" if self . with_past : inputs [ ids_name ] = np . asarray ( [ decoded_token ], dtype = self . dtypes [ ids_name ] ) . reshape ([ 1 , - 1 ]) inputs [ att_mask_name ] = np . ones ( [ 1 , inputs [ att_mask_name ] . shape [ - 1 ] + 1 ], dtype = self . dtypes [ att_mask_name ], ) for inp in self . model_inputs : if \"past_key_values\" in inp . name : inputs [ inp . name ] = results [ inp . name . replace ( \"past_key_values\" , \"present\" ) ] if self . is_encdec : inputs . pop ( \"input_ids\" , None ) else : decoder_input_ids = inputs [ ids_name ] decoder_attention_mask = inputs [ att_mask_name ] decoder_input_ids = np . concatenate ( [ decoder_input_ids , np . asarray ([[ decoded_token ]], dtype = np . int32 )], axis = 1 , ) decoder_attention_mask = np . ones_like ( decoder_input_ids , dtype = self . dtypes [ att_mask_name ] ) inputs [ ids_name ] = decoder_input_ids inputs [ att_mask_name ] = decoder_attention_mask return inputs text_generation_base Module that implements text generation model API. TextGenerationAPI Bases: BaseService Abstract base class for text generation models. Source code in npc_engine\\services\\text_generation\\text_generation_base.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 class TextGenerationAPI ( BaseService ): \"\"\"Abstract base class for text generation models.\"\"\" API_METHODS : List [ str ] = [ \"generate_reply\" , \"get_prompt_template\" , \"get_special_tokens\" , \"get_context_template\" , ] def __init__ ( self , template_string : str = None , context_template : str = None , history_template : str = None , * args , ** kwargs , ): \"\"\"Initialize prompt formatting variables. Args: template_string: Template string to be rendered as prompt. \"\"\" super () . __init__ ( * args , ** kwargs ) if template_string is not None : logger . warning ( \"Using legacy template string. It might cause context to be cropped and models functioning incorrectly.\" ) self . template_string = template_string self . template = Template ( template_string ) self . legacy = True else : self . template_string = None self . legacy = False self . context_template_string = context_template self . history_template_string = history_template self . context_template = Template ( context_template ) self . history_template = Template ( history_template ) self . initialized = True @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"TextGenerationAPI\" def generate_reply ( self , context : Dict [ str , Any ], * args , ** kwargs ) -> str : \"\"\"Format the model prompt and generate response. Args: context: Prompt context. *args **kwargs Returns: Text response to a prompt. \"\"\" if not self . initialized : raise AssertionError ( \"Can not generate replies before Base Service class was initialized\" ) if self . legacy : prompt = self . template . render ( ** context , ** self . get_special_tokens ()) else : history = context . get ( \"history\" , []) history_prompt = self . history_template . render ( ** context , ** self . get_special_tokens () ) context_prompt = self . context_template . render ( ** context , ** self . get_special_tokens () ) prompt = context_prompt + \"\" . join ( history_prompt ) if isinstance ( history , list ): while self . string_too_long ( prompt ): history . pop ( 0 ) history_prompt = self . history_template . render ( ** context , ** self . get_special_tokens () ) prompt = context_prompt + history_prompt if len ( history ) == 0 : break else : history_prompt = self . history_template . render ( ** context , ** self . get_special_tokens () ) prompt = context_prompt + history_prompt return self . run ( prompt , * args , ** kwargs ) def get_prompt_template ( self ) -> str : \"\"\"Return prompt template string used to render model prompt. Returns: A template string. \"\"\" if self . legacy : return self . template_string else : return self . context_template_string + self . history_template_string def get_context_template ( self ) -> Dict [ str , Any ]: \"\"\"Return context template. Returns: Example context \"\"\" if self . legacy : return schema_to_json ( to_json_schema ( infer ( self . template_string ))) else : context_dict = schema_to_json ( to_json_schema ( infer ( self . context_template_string )) ) history_dict = schema_to_json ( to_json_schema ( infer ( self . history_template_string )) ) combined_dict = {} for key in chain ( context_dict , history_dict ): if key in history_dict and key in context_dict : if context_dict [ key ] != history_dict [ key ]: raise AssertionError ( f \"Context and history templates have different values for { key } \" ) if key in history_dict : combined_dict [ key ] = history_dict [ key ] else : combined_dict [ key ] = context_dict [ key ] return combined_dict @abstractmethod def run ( self , prompt : str , temperature : float = 1 , topk : int = None ) -> str : \"\"\"Abstract method for concrete implementation of generation. Args: prompt: Fromatted prompt. temperature: Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness topk: If not none selects top n of predictions to sample from during generation. Returns: Generated text \"\"\" return None @abstractmethod def string_too_long ( self , prompt : str ) -> bool : \"\"\"Check if prompt is too long. Args: prompt: Prompt to check. Returns: True if prompt is too long, False otherwise. \"\"\" return False @abstractmethod def get_special_tokens ( self ) -> Dict [ str , str ]: \"\"\"Return dictionary mapping for special tokens. To be implemented by child class. Can then be used in template string as fields Returns: Dictionary of special tokens \"\"\" return None __init__ ( template_string = None , context_template = None , history_template = None , * args , ** kwargs ) Initialize prompt formatting variables. Parameters: Name Type Description Default template_string str Template string to be rendered as prompt. None Source code in npc_engine\\services\\text_generation\\text_generation_base.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def __init__ ( self , template_string : str = None , context_template : str = None , history_template : str = None , * args , ** kwargs , ): \"\"\"Initialize prompt formatting variables. Args: template_string: Template string to be rendered as prompt. \"\"\" super () . __init__ ( * args , ** kwargs ) if template_string is not None : logger . warning ( \"Using legacy template string. It might cause context to be cropped and models functioning incorrectly.\" ) self . template_string = template_string self . template = Template ( template_string ) self . legacy = True else : self . template_string = None self . legacy = False self . context_template_string = context_template self . history_template_string = history_template self . context_template = Template ( context_template ) self . history_template = Template ( history_template ) self . initialized = True generate_reply ( context , * args , ** kwargs ) Format the model prompt and generate response. Parameters: Name Type Description Default context Dict [ str , Any ] Prompt context. required Returns: Type Description str Text response to a prompt. Source code in npc_engine\\services\\text_generation\\text_generation_base.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def generate_reply ( self , context : Dict [ str , Any ], * args , ** kwargs ) -> str : \"\"\"Format the model prompt and generate response. Args: context: Prompt context. *args **kwargs Returns: Text response to a prompt. \"\"\" if not self . initialized : raise AssertionError ( \"Can not generate replies before Base Service class was initialized\" ) if self . legacy : prompt = self . template . render ( ** context , ** self . get_special_tokens ()) else : history = context . get ( \"history\" , []) history_prompt = self . history_template . render ( ** context , ** self . get_special_tokens () ) context_prompt = self . context_template . render ( ** context , ** self . get_special_tokens () ) prompt = context_prompt + \"\" . join ( history_prompt ) if isinstance ( history , list ): while self . string_too_long ( prompt ): history . pop ( 0 ) history_prompt = self . history_template . render ( ** context , ** self . get_special_tokens () ) prompt = context_prompt + history_prompt if len ( history ) == 0 : break else : history_prompt = self . history_template . render ( ** context , ** self . get_special_tokens () ) prompt = context_prompt + history_prompt return self . run ( prompt , * args , ** kwargs ) get_api_name () classmethod Get the API name. Source code in npc_engine\\services\\text_generation\\text_generation_base.py 54 55 56 57 @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"TextGenerationAPI\" get_context_template () Return context template. Returns: Type Description Dict [ str , Any ] Example context Source code in npc_engine\\services\\text_generation\\text_generation_base.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def get_context_template ( self ) -> Dict [ str , Any ]: \"\"\"Return context template. Returns: Example context \"\"\" if self . legacy : return schema_to_json ( to_json_schema ( infer ( self . template_string ))) else : context_dict = schema_to_json ( to_json_schema ( infer ( self . context_template_string )) ) history_dict = schema_to_json ( to_json_schema ( infer ( self . history_template_string )) ) combined_dict = {} for key in chain ( context_dict , history_dict ): if key in history_dict and key in context_dict : if context_dict [ key ] != history_dict [ key ]: raise AssertionError ( f \"Context and history templates have different values for { key } \" ) if key in history_dict : combined_dict [ key ] = history_dict [ key ] else : combined_dict [ key ] = context_dict [ key ] return combined_dict get_prompt_template () Return prompt template string used to render model prompt. Returns: Type Description str A template string. Source code in npc_engine\\services\\text_generation\\text_generation_base.py 102 103 104 105 106 107 108 109 110 111 def get_prompt_template ( self ) -> str : \"\"\"Return prompt template string used to render model prompt. Returns: A template string. \"\"\" if self . legacy : return self . template_string else : return self . context_template_string + self . history_template_string get_special_tokens () abstractmethod Return dictionary mapping for special tokens. To be implemented by child class. Can then be used in template string as fields Returns: Type Description Dict [ str , str ] Dictionary of special tokens Source code in npc_engine\\services\\text_generation\\text_generation_base.py 168 169 170 171 172 173 174 175 176 177 @abstractmethod def get_special_tokens ( self ) -> Dict [ str , str ]: \"\"\"Return dictionary mapping for special tokens. To be implemented by child class. Can then be used in template string as fields Returns: Dictionary of special tokens \"\"\" return None run ( prompt , temperature = 1 , topk = None ) abstractmethod Abstract method for concrete implementation of generation. Parameters: Name Type Description Default prompt str Fromatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text Source code in npc_engine\\services\\text_generation\\text_generation_base.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 @abstractmethod def run ( self , prompt : str , temperature : float = 1 , topk : int = None ) -> str : \"\"\"Abstract method for concrete implementation of generation. Args: prompt: Fromatted prompt. temperature: Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness topk: If not none selects top n of predictions to sample from during generation. Returns: Generated text \"\"\" return None string_too_long ( prompt ) abstractmethod Check if prompt is too long. Parameters: Name Type Description Default prompt str Prompt to check. required Returns: Type Description bool True if prompt is too long, False otherwise. Source code in npc_engine\\services\\text_generation\\text_generation_base.py 156 157 158 159 160 161 162 163 164 165 166 @abstractmethod def string_too_long ( self , prompt : str ) -> bool : \"\"\"Check if prompt is too long. Args: prompt: Prompt to check. Returns: True if prompt is too long, False otherwise. \"\"\" return False tts Text to speech specific model implementations. This module implements specific models and wraps them under the common interface for loading and inference. Example from npc_engine.models.tts import TextToSpeechAPI model = TextToSpeechAPI.load(\"path/to/model_dir\") model.run(speaker_id=0, text=\"Hello, world!\") espnet_onnx espnet_onnx text to speech inference implementation. ESPNetTTS Bases: TextToSpeechAPI Service implementation for the espnet_onnx text to speech models. Source code in npc_engine\\services\\tts\\espnet_onnx.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class ESPNetTTS ( TextToSpeechAPI ): \"\"\"Service implementation for the espnet_onnx text to speech models.\"\"\" def __init__ ( self , model_path : str , speaker_num : int , * args , ** kwargs ): \"\"\"Create and load espnet_onnx Text2Speech model. Args: model_path: Path to the model directory. speaker_num: Number of speakers model supports. \"\"\" super () . __init__ ( * args , ** kwargs ) provider = self . get_providers () self . t2s = Text2Speech ( model_path , providers = provider ) logging . info ( \"ESPNetTTS using providers {} \" . format ( provider )) if not self . t2s . tts_model . use_sids : self . speaker_ids = range ( speaker_num ) else : self . speaker_ids = list () def get_speaker_ids ( self ) -> List [ str ]: \"\"\"Return available ids of different speakers.\"\"\" return self . speaker_ids def run ( self , speaker_id : str , text : str , n_chunks : int ) -> Iterator [ np . ndarray ]: \"\"\"Create a generator for iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. Returns: Generator that yields next chunk of speech in the form of f32 ndarray. \"\"\" try : speaker_id = int ( speaker_id ) except ValueError : raise ValueError ( \"Speaker id in espnet models must be an integer\" ) if speaker_id not in self . speaker_ids : raise ValueError ( \"Speaker id {} not supported\" . format ( speaker_id )) return iter ([ self . t2s ( text , sids = np . asarray ([ int ( speaker_id )]))[ \"wav\" ]]) __init__ ( model_path , speaker_num , * args , ** kwargs ) Create and load espnet_onnx Text2Speech model. Parameters: Name Type Description Default model_path str Path to the model directory. required speaker_num int Number of speakers model supports. required Source code in npc_engine\\services\\tts\\espnet_onnx.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def __init__ ( self , model_path : str , speaker_num : int , * args , ** kwargs ): \"\"\"Create and load espnet_onnx Text2Speech model. Args: model_path: Path to the model directory. speaker_num: Number of speakers model supports. \"\"\" super () . __init__ ( * args , ** kwargs ) provider = self . get_providers () self . t2s = Text2Speech ( model_path , providers = provider ) logging . info ( \"ESPNetTTS using providers {} \" . format ( provider )) if not self . t2s . tts_model . use_sids : self . speaker_ids = range ( speaker_num ) else : self . speaker_ids = list () get_speaker_ids () Return available ids of different speakers. Source code in npc_engine\\services\\tts\\espnet_onnx.py 28 29 30 def get_speaker_ids ( self ) -> List [ str ]: \"\"\"Return available ids of different speakers.\"\"\" return self . speaker_ids run ( speaker_id , text , n_chunks ) Create a generator for iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Returns: Type Description Iterator [ np . ndarray ] Generator that yields next chunk of speech in the form of f32 ndarray. Source code in npc_engine\\services\\tts\\espnet_onnx.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def run ( self , speaker_id : str , text : str , n_chunks : int ) -> Iterator [ np . ndarray ]: \"\"\"Create a generator for iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. Returns: Generator that yields next chunk of speech in the form of f32 ndarray. \"\"\" try : speaker_id = int ( speaker_id ) except ValueError : raise ValueError ( \"Speaker id in espnet models must be an integer\" ) if speaker_id not in self . speaker_ids : raise ValueError ( \"Speaker id {} not supported\" . format ( speaker_id )) return iter ([ self . t2s ( text , sids = np . asarray ([ int ( speaker_id )]))[ \"wav\" ]]) flowtron flowtron Flowtron (https://github.com/NVIDIA/flowtron) text to speech inference implementation. FlowtronTTS Bases: TextToSpeechAPI Implements Flowtron architecture inference. Paper: arXiv:2005.05957 Code: https://github.com/NVIDIA/flowtron Onnx export script can be found in this fork https://github.com/npc-engine/flowtron. This model class requires four ONNX models encoder.onnx , backward_flow.onnx , forward_flow.onnx and vocoder.onnx where first three are layers from Flowtron architecture ( flow corresponding to one direction pass of affine coupling layers) and vocoder.onnx is neural vocoder. For detailed specs refer to https://github.com/npc-engine/flowtron. Source code in npc_engine\\services\\tts\\flowtron\\flowtron.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 class FlowtronTTS ( TextToSpeechAPI ): \"\"\"Implements Flowtron architecture inference. Paper: [arXiv:2005.05957](https://arxiv.org/abs/2005.05957) Code: https://github.com/NVIDIA/flowtron Onnx export script can be found in this fork https://github.com/npc-engine/flowtron. This model class requires four ONNX models `encoder.onnx`, `backward_flow.onnx`, `forward_flow.onnx` and `vocoder.onnx` where first three are layers from Flowtron architecture (`flow` corresponding to one direction pass of affine coupling layers) and `vocoder.onnx` is neural vocoder. For detailed specs refer to https://github.com/npc-engine/flowtron. \"\"\" def __init__ ( self , model_path , max_frames = 400 , gate_threshold = 0.5 , sigma = 0.8 , smoothing_window = 3 , smoothing_weight = 0.5 , * args , ** kwargs ): \"\"\"Create and load Flowtron and vocoder models.\"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = onnxruntime . SessionOptions () sess_options . graph_optimization_level = ( onnxruntime . GraphOptimizationLevel . ORT_ENABLE_ALL ) provider = onnxruntime . get_available_providers ()[ 0 ] logging . info ( \"FlowtronTTS using provider {} \" . format ( provider )) self . max_frames = max_frames self . gate_threshold = gate_threshold self . sigma = sigma self . smoothing_window = smoothing_window self . smoothing_weight = smoothing_weight self . encoder = onnxruntime . InferenceSession ( path . join ( model_path , \"encoder.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . backward_flow = onnxruntime . InferenceSession ( path . join ( model_path , \"backward_flow.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . forward_flow = onnxruntime . InferenceSession ( path . join ( model_path , \"forward_flow.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . vocoder = onnxruntime . InferenceSession ( path . join ( model_path , \"vocoder.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . speaker_ids = [ str ( i ) for i in range ( 127 )] self . speaker_ids_map = { idx : i for i , idx in enumerate ( self . speaker_ids )} def get_speaker_ids ( self ) -> List [ str ]: \"\"\"Return available ids of different speakers.\"\"\" return self . speaker_ids def run ( self , speaker_id : str , text : str , n_chunks : int ) -> Iterator [ np . ndarray ]: \"\"\"Create a generator for iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. Returns: Generator that yields next chunk of speech in the form of f32 ndarray. \"\"\" text = self . _get_text ( text ) speaker_id = np . asarray ([[ self . speaker_ids_map [ speaker_id ]]], dtype = np . int64 ) enc_outps_ortvalue = onnxruntime . OrtValue . ortvalue_from_shape_and_type ( [ text . shape [ 1 ], 1 , 640 ], np . float32 , \"cpu\" , 0 ) io_binding = self . encoder . io_binding () io_binding . bind_ortvalue_output ( \"text_emb\" , enc_outps_ortvalue ) io_binding . bind_cpu_input ( \"speaker_vecs\" , speaker_id ) io_binding . bind_cpu_input ( \"text\" , text . reshape ([ 1 , - 1 ])) self . encoder . run_with_iobinding ( io_binding ) residual = np . random . normal ( 0 , self . sigma , size = [ self . max_frames , 1 , 80 ] ) . astype ( np . float32 ) residual = self . _run_backward_flow ( residual , enc_outps_ortvalue ) residual = self . _run_forward_flow ( residual , enc_outps_ortvalue , num_split = self . max_frames // n_chunks ) last_audio = None for residual in residual : residual = np . transpose ( residual , axes = ( 1 , 2 , 0 )) audio = self . vocoder . run ( None , { \"mels\" : residual })[ 0 ] # audio = np.where( # (audio > (audio.mean() - audio.std())) # | (audio < (audio.mean() + audio.std())), # audio, # audio.mean(), # ) tmp = audio if last_audio is None : audio = audio [:, 1000 :] if last_audio is not None : cumsum_vec = np . cumsum ( np . concatenate ([ last_audio , audio ], axis = 1 ), axis = 1 ) ma_vec = ( cumsum_vec [:, self . smoothing_window :] - cumsum_vec [:, : - self . smoothing_window ] ) / self . smoothing_window audio = ( 1 - self . smoothing_weight ) * audio [ :, self . smoothing_window : ] + self . smoothing_weight * ma_vec [:, last_audio . shape [ 1 ] :] last_audio = tmp audio = audio . reshape ( - 1 ) # audio = audio / np.abs(audio).max() yield audio def _get_text ( self , text : str ): text = _clean_text ( text , [ \"flowtron_cleaners\" ]) words = re . findall ( r \"\\S*\\{.*?\\}\\S*|\\S+\" , text ) text = \" \" . join ( words ) text_norm = np . asarray ( text_to_sequence ( text ), dtype = np . int64 ) . reshape ([ 1 , - 1 ]) return text_norm def _run_backward_flow ( self , residual , enc_outps_ortvalue ): residual_o , hidden_att , hidden_lstm = self . _init_states ( residual ) hidden_att_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 0 ], \"cpu\" , 0 ) hidden_att_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 1 ], \"cpu\" , 0 ) hidden_lstm_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 0 ], \"cpu\" , 0 ) hidden_lstm_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 1 ], \"cpu\" , 0 ) hidden_att_o_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 0 ], \"cpu\" , 0 ) hidden_att_o_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 1 ], \"cpu\" , 0 ) hidden_lstm_o_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 0 ], \"cpu\" , 0 ) hidden_lstm_o_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 1 ], \"cpu\" , 0 ) residual_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( residual_o , \"cpu\" , 0 ) residual_outp = [ residual_ortvalue ] for i in range ( residual . shape [ 0 ] - 1 , - 1 , - 1 ): io_binding = self . backward_flow . io_binding () io_binding . bind_cpu_input ( \"residual\" , residual [ i ]) io_binding . bind_ortvalue_input ( \"text\" , enc_outps_ortvalue ) io_binding . bind_ortvalue_input ( \"last_output\" , residual_outp [ 0 ]) io_binding . bind_ortvalue_input ( \"hidden_att\" , hidden_att_ortvalue ) io_binding . bind_ortvalue_input ( \"hidden_att_c\" , hidden_att_c_ortvalue ) io_binding . bind_ortvalue_input ( \"hidden_lstm\" , hidden_lstm_ortvalue ) io_binding . bind_ortvalue_input ( \"hidden_lstm_c\" , hidden_lstm_c_ortvalue ) io_binding . bind_output ( \"output\" , \"cpu\" ) io_binding . bind_output ( \"gate\" , \"cpu\" ) io_binding . bind_ortvalue_output ( \"hidden_att_o\" , hidden_att_o_ortvalue ) io_binding . bind_ortvalue_output ( \"hidden_att_o_c\" , hidden_att_o_c_ortvalue ) io_binding . bind_ortvalue_output ( \"hidden_lstm_o\" , hidden_lstm_o_ortvalue ) io_binding . bind_ortvalue_output ( \"hidden_lstm_o_c\" , hidden_lstm_o_c_ortvalue ) self . backward_flow . run_with_iobinding ( io_binding ) outp = io_binding . get_outputs () gates = outp [ 1 ] . numpy () residual_outp = [ outp [ 0 ]] + residual_outp if ( gates > self . gate_threshold ) . any (): break # Switch input and output to use latest output as input ( hidden_att_ortvalue , hidden_att_o_ortvalue ) = ( hidden_att_o_ortvalue , hidden_att_ortvalue , ) ( hidden_att_c_ortvalue , hidden_att_o_c_ortvalue ) = ( hidden_att_o_c_ortvalue , hidden_att_c_ortvalue , ) ( hidden_lstm_ortvalue , hidden_lstm_o_ortvalue ) = ( hidden_lstm_o_ortvalue , hidden_lstm_ortvalue , ) ( hidden_lstm_c_ortvalue , hidden_lstm_o_c_ortvalue ) = ( hidden_lstm_o_c_ortvalue , hidden_lstm_c_ortvalue , ) residual = np . concatenate ( [ residual_ort . numpy () for residual_ort in residual_outp ], axis = 0 ) return residual def _run_forward_flow ( self , residual , enc_outps_ortvalue , num_split ): residual_o , hidden_att , hidden_lstm = self . _init_states ( residual ) hidden_att_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 0 ], \"cpu\" , 0 ) hidden_att_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 1 ], \"cpu\" , 0 ) hidden_lstm_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 0 ], \"cpu\" , 0 ) hidden_lstm_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 1 ], \"cpu\" , 0 ) hidden_att_o_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 0 ], \"cpu\" , 0 ) hidden_att_o_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 1 ], \"cpu\" , 0 ) hidden_lstm_o_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 0 ], \"cpu\" , 0 ) hidden_lstm_o_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 1 ], \"cpu\" , 0 ) residual_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( residual_o , \"cpu\" , 0 ) residual_outp = [ residual_ortvalue ] last_output = residual_ortvalue for i in range ( residual . shape [ 0 ]): io_binding = self . forward_flow . io_binding () io_binding . bind_cpu_input ( \"residual\" , residual [ i ]) io_binding . bind_ortvalue_input ( \"text\" , enc_outps_ortvalue ) io_binding . bind_ortvalue_input ( \"last_output\" , last_output ) io_binding . bind_ortvalue_input ( \"hidden_att\" , hidden_att_ortvalue ) io_binding . bind_ortvalue_input ( \"hidden_att_c\" , hidden_att_c_ortvalue ) io_binding . bind_ortvalue_input ( \"hidden_lstm\" , hidden_lstm_ortvalue ) io_binding . bind_ortvalue_input ( \"hidden_lstm_c\" , hidden_lstm_c_ortvalue ) io_binding . bind_output ( \"output\" , \"cpu\" ) io_binding . bind_output ( \"gate\" , \"cpu\" ) io_binding . bind_ortvalue_output ( \"hidden_att_o\" , hidden_att_o_ortvalue ) io_binding . bind_ortvalue_output ( \"hidden_att_o_c\" , hidden_att_o_c_ortvalue ) io_binding . bind_ortvalue_output ( \"hidden_lstm_o\" , hidden_lstm_o_ortvalue ) io_binding . bind_ortvalue_output ( \"hidden_lstm_o_c\" , hidden_lstm_o_c_ortvalue ) self . forward_flow . run_with_iobinding ( io_binding ) outp = io_binding . get_outputs () gates = outp [ 1 ] . numpy () residual_outp . append ( outp [ 0 ]) last_output = outp [ 0 ] if ( gates > self . gate_threshold ) . any (): break # Switch input and output to use latest output as input ( hidden_att_ortvalue , hidden_att_o_ortvalue ) = ( hidden_att_o_ortvalue , hidden_att_ortvalue , ) ( hidden_att_c_ortvalue , hidden_att_o_c_ortvalue ) = ( hidden_att_o_c_ortvalue , hidden_att_c_ortvalue , ) ( hidden_lstm_ortvalue , hidden_lstm_o_ortvalue ) = ( hidden_lstm_o_ortvalue , hidden_lstm_ortvalue , ) ( hidden_lstm_c_ortvalue , hidden_lstm_o_c_ortvalue ) = ( hidden_lstm_o_c_ortvalue , hidden_lstm_c_ortvalue , ) if len ( residual_outp ) % num_split == 0 and i != 0 : residual_o = np . concatenate ( [ residual_ort . numpy () for residual_ort in residual_outp ], axis = 0 ) yield residual_o residual_outp = [] if len ( residual_outp ) > 0 : residual_o = np . concatenate ( [ residual_ort . numpy () for residual_ort in residual_outp ], axis = 0 ) yield residual_o def _init_states ( self , residual ): last_outputs = np . zeros ( [ 1 , residual . shape [ 1 ], residual . shape [ 2 ]], dtype = np . float32 ) hidden_att = [ np . zeros ([ 1 , 1 , 1024 ], dtype = np . float32 ), np . zeros ([ 1 , 1 , 1024 ], dtype = np . float32 ), ] hidden_lstm = [ np . zeros ([ 2 , 1 , 1024 ], dtype = np . float32 ), np . zeros ([ 2 , 1 , 1024 ], dtype = np . float32 ), ] return last_outputs , hidden_att , hidden_lstm __init__ ( model_path , max_frames = 400 , gate_threshold = 0.5 , sigma = 0.8 , smoothing_window = 3 , smoothing_weight = 0.5 , * args , ** kwargs ) Create and load Flowtron and vocoder models. Source code in npc_engine\\services\\tts\\flowtron\\flowtron.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def __init__ ( self , model_path , max_frames = 400 , gate_threshold = 0.5 , sigma = 0.8 , smoothing_window = 3 , smoothing_weight = 0.5 , * args , ** kwargs ): \"\"\"Create and load Flowtron and vocoder models.\"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = onnxruntime . SessionOptions () sess_options . graph_optimization_level = ( onnxruntime . GraphOptimizationLevel . ORT_ENABLE_ALL ) provider = onnxruntime . get_available_providers ()[ 0 ] logging . info ( \"FlowtronTTS using provider {} \" . format ( provider )) self . max_frames = max_frames self . gate_threshold = gate_threshold self . sigma = sigma self . smoothing_window = smoothing_window self . smoothing_weight = smoothing_weight self . encoder = onnxruntime . InferenceSession ( path . join ( model_path , \"encoder.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . backward_flow = onnxruntime . InferenceSession ( path . join ( model_path , \"backward_flow.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . forward_flow = onnxruntime . InferenceSession ( path . join ( model_path , \"forward_flow.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . vocoder = onnxruntime . InferenceSession ( path . join ( model_path , \"vocoder.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . speaker_ids = [ str ( i ) for i in range ( 127 )] self . speaker_ids_map = { idx : i for i , idx in enumerate ( self . speaker_ids )} get_speaker_ids () Return available ids of different speakers. Source code in npc_engine\\services\\tts\\flowtron\\flowtron.py 81 82 83 def get_speaker_ids ( self ) -> List [ str ]: \"\"\"Return available ids of different speakers.\"\"\" return self . speaker_ids run ( speaker_id , text , n_chunks ) Create a generator for iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Returns: Type Description Iterator [ np . ndarray ] Generator that yields next chunk of speech in the form of f32 ndarray. Source code in npc_engine\\services\\tts\\flowtron\\flowtron.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def run ( self , speaker_id : str , text : str , n_chunks : int ) -> Iterator [ np . ndarray ]: \"\"\"Create a generator for iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. Returns: Generator that yields next chunk of speech in the form of f32 ndarray. \"\"\" text = self . _get_text ( text ) speaker_id = np . asarray ([[ self . speaker_ids_map [ speaker_id ]]], dtype = np . int64 ) enc_outps_ortvalue = onnxruntime . OrtValue . ortvalue_from_shape_and_type ( [ text . shape [ 1 ], 1 , 640 ], np . float32 , \"cpu\" , 0 ) io_binding = self . encoder . io_binding () io_binding . bind_ortvalue_output ( \"text_emb\" , enc_outps_ortvalue ) io_binding . bind_cpu_input ( \"speaker_vecs\" , speaker_id ) io_binding . bind_cpu_input ( \"text\" , text . reshape ([ 1 , - 1 ])) self . encoder . run_with_iobinding ( io_binding ) residual = np . random . normal ( 0 , self . sigma , size = [ self . max_frames , 1 , 80 ] ) . astype ( np . float32 ) residual = self . _run_backward_flow ( residual , enc_outps_ortvalue ) residual = self . _run_forward_flow ( residual , enc_outps_ortvalue , num_split = self . max_frames // n_chunks ) last_audio = None for residual in residual : residual = np . transpose ( residual , axes = ( 1 , 2 , 0 )) audio = self . vocoder . run ( None , { \"mels\" : residual })[ 0 ] # audio = np.where( # (audio > (audio.mean() - audio.std())) # | (audio < (audio.mean() + audio.std())), # audio, # audio.mean(), # ) tmp = audio if last_audio is None : audio = audio [:, 1000 :] if last_audio is not None : cumsum_vec = np . cumsum ( np . concatenate ([ last_audio , audio ], axis = 1 ), axis = 1 ) ma_vec = ( cumsum_vec [:, self . smoothing_window :] - cumsum_vec [:, : - self . smoothing_window ] ) / self . smoothing_window audio = ( 1 - self . smoothing_weight ) * audio [ :, self . smoothing_window : ] + self . smoothing_weight * ma_vec [:, last_audio . shape [ 1 ] :] last_audio = tmp audio = audio . reshape ( - 1 ) # audio = audio / np.abs(audio).max() yield audio text from https://github.com/keithito/tacotron cleaners adapted from https://github.com/keithito/tacotron. Cleaners are transformations that run over the input text at both training and eval time. Cleaners can be selected by passing a comma-delimited list of cleaner names as the \"cleaners\" hyperparameter. Some cleaners are English-specific. You'll typically want to use: 1. \"english_cleaners\" for English text 2. \"transliteration_cleaners\" for non-English text that can be transliterated to ASCII using the Unidecode library (https://pypi.python.org/pypi/Unidecode) 3. \"basic_cleaners\" if you do not want to transliterate (in this case, you should also update the symbols in symbols.py to match your data). flowtron_cleaners ( text ) Clean text with a set of cleaners. Source code in npc_engine\\services\\tts\\flowtron\\text\\cleaners.py 98 99 100 101 102 103 104 105 def flowtron_cleaners ( text ): \"\"\"Clean text with a set of cleaners.\"\"\" text = collapse_whitespace ( text ) text = remove_hyphens ( text ) text = expand_datestime ( text ) text = expand_numbers ( text ) text = expand_safe_abbreviations ( text ) return text numbers from https://github.com/keithito/tacotron symbols from https://github.com/keithito/tacotron tts_base Module that implements text to speech model API. TextToSpeechAPI Bases: BaseService Abstract base class for text-to-speech models. Source code in npc_engine\\services\\tts\\tts_base.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 class TextToSpeechAPI ( BaseService ): \"\"\"Abstract base class for text-to-speech models.\"\"\" #: Methods that are going to be exposed as services. API_METHODS : List [ str ] = [ \"tts_start\" , \"tts_get_results\" , \"get_speaker_ids\" ] def __init__ ( self , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" self . generator = None super () . __init__ ( * args , ** kwargs ) self . initialized = True @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"TextToSpeechAPI\" def tts_start ( self , speaker_id : str , text : str , n_chunks : int ) -> None : \"\"\"Initiate iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. \"\"\" sentences = re . split ( r \"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\" , text ) self . generator = self . _chain_run ( speaker_id , sentences , n_chunks ) def _chain_run ( self , speaker_id , sentences , n_chunks ) -> Iterable [ np . ndarray ]: \"\"\"Chain the run method to be used in the generator.\"\"\" for sentence in sentences : sentence = sentence . strip () if sentence != \"\" : yield from self . run ( speaker_id , sentence , n_chunks ) def tts_get_results ( self ) -> Iterable [ np . ndarray ]: \"\"\"Retrieve the next chunk of generated speech. Returns: Next chunk of speech in the form of f32 ndarray. \"\"\" if self . generator is not None : return next ( self . generator ) . tolist () else : raise ValueError ( \"Speech generation was not started. Use tts_start to start it\" ) @abstractmethod def run ( self , speaker_id : str , text : str , n_chunks : int ) -> Iterable [ np . ndarray ]: \"\"\"Create a generator for iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. Returns: Generator that yields next chunk of speech in the form of f32 ndarray. \"\"\" return None @abstractmethod def get_speaker_ids ( self ) -> List [ str ]: \"\"\"Get list of available speaker ids. Returns: The return value. True for success, False otherwise. \"\"\" return None __init__ ( * args , ** kwargs ) Empty initialization method for API to be similar to other model base classes. Source code in npc_engine\\services\\tts\\tts_base.py 16 17 18 19 20 def __init__ ( self , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" self . generator = None super () . __init__ ( * args , ** kwargs ) self . initialized = True _chain_run ( speaker_id , sentences , n_chunks ) Chain the run method to be used in the generator. Source code in npc_engine\\services\\tts\\tts_base.py 39 40 41 42 43 44 def _chain_run ( self , speaker_id , sentences , n_chunks ) -> Iterable [ np . ndarray ]: \"\"\"Chain the run method to be used in the generator.\"\"\" for sentence in sentences : sentence = sentence . strip () if sentence != \"\" : yield from self . run ( speaker_id , sentence , n_chunks ) get_api_name () classmethod Get the API name. Source code in npc_engine\\services\\tts\\tts_base.py 22 23 24 25 @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"TextToSpeechAPI\" get_speaker_ids () abstractmethod Get list of available speaker ids. Returns: Type Description List [ str ] The return value. True for success, False otherwise. Source code in npc_engine\\services\\tts\\tts_base.py 73 74 75 76 77 78 79 80 @abstractmethod def get_speaker_ids ( self ) -> List [ str ]: \"\"\"Get list of available speaker ids. Returns: The return value. True for success, False otherwise. \"\"\" return None run ( speaker_id , text , n_chunks ) abstractmethod Create a generator for iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Returns: Type Description Iterable [ np . ndarray ] Generator that yields next chunk of speech in the form of f32 ndarray. Source code in npc_engine\\services\\tts\\tts_base.py 59 60 61 62 63 64 65 66 67 68 69 70 71 @abstractmethod def run ( self , speaker_id : str , text : str , n_chunks : int ) -> Iterable [ np . ndarray ]: \"\"\"Create a generator for iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. Returns: Generator that yields next chunk of speech in the form of f32 ndarray. \"\"\" return None tts_get_results () Retrieve the next chunk of generated speech. Returns: Type Description Iterable [ np . ndarray ] Next chunk of speech in the form of f32 ndarray. Source code in npc_engine\\services\\tts\\tts_base.py 46 47 48 49 50 51 52 53 54 55 56 57 def tts_get_results ( self ) -> Iterable [ np . ndarray ]: \"\"\"Retrieve the next chunk of generated speech. Returns: Next chunk of speech in the form of f32 ndarray. \"\"\" if self . generator is not None : return next ( self . generator ) . tolist () else : raise ValueError ( \"Speech generation was not started. Use tts_start to start it\" ) tts_start ( speaker_id , text , n_chunks ) Initiate iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Source code in npc_engine\\services\\tts\\tts_base.py 27 28 29 30 31 32 33 34 35 36 37 def tts_start ( self , speaker_id : str , text : str , n_chunks : int ) -> None : \"\"\"Initiate iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. \"\"\" sentences = re . split ( r \"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\" , text ) self . generator = self . _chain_run ( speaker_id , sentences , n_chunks ) utils Utility methods for model handling. config Config related functions. get_model_type_name ( models_path , model_id ) Get model type name. Parameters: Name Type Description Default models_path str Path to the models folder. required model_id str Model id (dirname). required Returns: Type Description str Model type name. Source code in npc_engine\\services\\utils\\config.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def get_model_type_name ( models_path : str , model_id : str ) -> str : \"\"\"Get model type name. Args: models_path: Path to the models folder. model_id: Model id (dirname). Returns: Model type name. \"\"\" model_path = os . path . join ( models_path , model_id ) config_path = os . path . join ( model_path , \"config.yml\" ) with open ( config_path , \"r\" ) as f : config = yaml . safe_load ( f ) return get_type_from_dict ( config ) get_type_from_dict ( config_dict ) Get model type from config dict. Parameters: Name Type Description Default config_dict dict Config dict. required Returns: Type Description str Model type. Source code in npc_engine\\services\\utils\\config.py 8 9 10 11 12 13 14 15 16 17 def get_type_from_dict ( config_dict : dict ) -> str : \"\"\"Get model type from config dict. Args: config_dict: Config dict. Returns: Model type. \"\"\" return config_dict . get ( \"model_type\" , config_dict . get ( \"type\" , \"\" )) validate_hub_model ( models_path , model_id ) Validate huggingface hub model by id. Parameters: Name Type Description Default models_path str Path to the models folder. required model_id str Huggingface hub model id. required Source code in npc_engine\\services\\utils\\config.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def validate_hub_model ( models_path : str , model_id : str ) -> bool : \"\"\"Validate huggingface hub model by id. Args: models_path: Path to the models folder. model_id: Huggingface hub model id. \"\"\" tmp_model_path = os . path . join ( models_path , model_id . replace ( \"/\" , \"_\" )) model_correct = True try : try : hf_hub_download ( repo_id = model_id , filename = \"config.yml\" , cache_dir = tmp_model_path , force_filename = \"config.yml\" , ) except HTTPError : return False config_path = os . path . join ( models_path , model_id . replace ( \"/\" , \"_\" ), \"config.yml\" ) with open ( config_path ) as f : config_dict = yaml . load ( f , Loader = yaml . Loader ) if \"model_type\" not in config_dict : model_correct = False if os . path . exists ( config_path ): os . remove ( config_path ) except ValueError : model_correct = False if os . path . exists ( tmp_model_path ): os . rmdir ( tmp_model_path ) return model_correct validate_local_model ( models_path , model_id ) Validate local model by id. Parameters: Name Type Description Default models_path str Path to the models folder. required model_id str Model id. required Source code in npc_engine\\services\\utils\\config.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def validate_local_model ( models_path : str , model_id : str ) -> bool : \"\"\"Validate local model by id. Args: models_path: Path to the models folder. model_id: Model id. \"\"\" model_correct = True model_path = os . path . join ( models_path , model_id ) if not os . path . exists ( model_path ): model_correct = False else : try : _ = get_model_type_name ( models_path , model_id ) except FileNotFoundError : model_correct = False return model_correct validate_model ( models_path , model_id ) Validate model by id. Parameters: Name Type Description Default models_path str Path to the models folder. required model_id str Model id. required Source code in npc_engine\\services\\utils\\config.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def validate_model ( models_path : str , model_id : str ) -> bool : \"\"\"Validate model by id. Args: models_path: Path to the models folder. model_id: Model id. \"\"\" model_correct = True model_path = os . path . join ( models_path , model_id ) if not os . path . exists ( model_path ): model_correct = validate_hub_model ( models_path , model_id ) else : model_correct = validate_local_model ( models_path , model_id ) return model_correct lru_cache LRU cache. NumpyLRUCache Dict based LRU cache for numpy arrays. Source code in npc_engine\\services\\utils\\lru_cache.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class NumpyLRUCache : \"\"\"Dict based LRU cache for numpy arrays.\"\"\" def __init__ ( self , size ): \"\"\"Crate cache.\"\"\" self . size = size self . lru_cache = collections . OrderedDict () self . common_dim = None def _get ( self , key : Any , default = None ) -> np . ndarray : try : value = self . lru_cache . pop ( key ) self . lru_cache [ key ] = value return value except KeyError : return default def _put ( self , key : Any , value : np . ndarray ): try : self . lru_cache . pop ( key ) except KeyError : if len ( self . lru_cache ) >= self . size : self . lru_cache . popitem ( last = False ) self . lru_cache [ key ] = value def cache_compute ( self , keys : List [ Any ], function : Callable ) -> Tuple [ np . ndarray , List [ Any ]]: \"\"\"Get batch from cache and compute missing. Args: keys: List of keys Returns: np.ndarray or None: Found entries concatenated over 0 axis. list(_) or None: Keys that were not found. \"\"\" if len ( self . lru_cache ) == 0 : result = function ( keys ) self . put_batch ( keys , result ) return result else : result = np . zeros (( len ( keys ), * self . common_dim )) items = [ self . _get ( key ) for key in keys ] not_found = [ key for item , key in zip ( items , keys ) if item is None ] if len ( not_found ) > 0 : computed = function ( not_found ) computed_idx = 0 for idx , item in enumerate ( items ): if item is None : result_slc = tuple ([ idx ] + [ slice ( None )] * len ( self . common_dim )) computed_slc = tuple ( [ computed_idx ] + [ slice ( None )] * len ( self . common_dim ) ) result [ result_slc ] = computed [ computed_slc ] computed_idx += 1 else : result_slc = tuple ([ idx ] + [ slice ( None )] * len ( self . common_dim )) result [ result_slc ] = item return result def put_batch ( self , keys : List [ Any ], values : np . ndarray ): \"\"\"Put batch to cache. Args: keys: List of keys values: Ndarray of shape (len(keys), *common_dim) \"\"\" self . _validate_shape ( values ) for key , item in zip ( keys , values ): self . _put ( key , item ) def _validate_shape ( self , value ): if self . common_dim is None : self . common_dim = value . shape [ 1 :] else : if self . common_dim != value . shape [ 1 :]: raise ValueError ( f \"\"\"Cached arrays must have the same shape. Shape expected: { self . common_dim } Shape found: { value . shape [ 1 :] } \"\"\" ) __init__ ( size ) Crate cache. Source code in npc_engine\\services\\utils\\lru_cache.py 10 11 12 13 14 def __init__ ( self , size ): \"\"\"Crate cache.\"\"\" self . size = size self . lru_cache = collections . OrderedDict () self . common_dim = None cache_compute ( keys , function ) Get batch from cache and compute missing. Parameters: Name Type Description Default keys List [ Any ] List of keys required Returns: Type Description np . ndarray np.ndarray or None: Found entries concatenated over 0 axis. List [ Any ] list(_) or None: Keys that were not found. Source code in npc_engine\\services\\utils\\lru_cache.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def cache_compute ( self , keys : List [ Any ], function : Callable ) -> Tuple [ np . ndarray , List [ Any ]]: \"\"\"Get batch from cache and compute missing. Args: keys: List of keys Returns: np.ndarray or None: Found entries concatenated over 0 axis. list(_) or None: Keys that were not found. \"\"\" if len ( self . lru_cache ) == 0 : result = function ( keys ) self . put_batch ( keys , result ) return result else : result = np . zeros (( len ( keys ), * self . common_dim )) items = [ self . _get ( key ) for key in keys ] not_found = [ key for item , key in zip ( items , keys ) if item is None ] if len ( not_found ) > 0 : computed = function ( not_found ) computed_idx = 0 for idx , item in enumerate ( items ): if item is None : result_slc = tuple ([ idx ] + [ slice ( None )] * len ( self . common_dim )) computed_slc = tuple ( [ computed_idx ] + [ slice ( None )] * len ( self . common_dim ) ) result [ result_slc ] = computed [ computed_slc ] computed_idx += 1 else : result_slc = tuple ([ idx ] + [ slice ( None )] * len ( self . common_dim )) result [ result_slc ] = item return result put_batch ( keys , values ) Put batch to cache. Parameters: Name Type Description Default keys List [ Any ] List of keys required values np . ndarray Ndarray of shape (len(keys), *common_dim) required Source code in npc_engine\\services\\utils\\lru_cache.py 68 69 70 71 72 73 74 75 76 77 def put_batch ( self , keys : List [ Any ], values : np . ndarray ): \"\"\"Put batch to cache. Args: keys: List of keys values: Ndarray of shape (len(keys), *common_dim) \"\"\" self . _validate_shape ( values ) for key , item in zip ( keys , values ): self . _put ( key , item ) mock Utility script to mock models for testing. build_output_shape_tensor_ ( graph , output , dynamic_shape_map , shape_name = 'dynamic_shape' , override = {}) Build output shape tensor for dynamic shape models. Parameters: Name Type Description Default graph Graph to add the output shape tensor to. required output Model output. required dynamic_shape_map Map of input names to their dynamic shape indices. required shape_name Name of the output shape tensor. 'dynamic_shape' Source code in npc_engine\\services\\utils\\mock.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def build_output_shape_tensor_ ( graph , output , dynamic_shape_map , shape_name = \"dynamic_shape\" , override = {} ): \"\"\"Build output shape tensor for dynamic shape models. Args: graph: Graph to add the output shape tensor to. output: Model output. dynamic_shape_map: Map of input names to their dynamic shape indices. shape_name: Name of the output shape tensor. \"\"\" dimensions_retrieved = [] for i , dim in enumerate ( output . type . tensor_type . shape . dim ): if \" + \" in dim . dim_param : dim1 , dim2 = dim . dim_param . split ( \" + \" ) create_dim_variable_ ( graph , shape_name , dim1 , override . get ( dim1 , dim . dim_value ), i , dynamic_shape_map , postfix = \"1\" , ) create_dim_variable_ ( graph , shape_name , dim2 , override . get ( dim2 , dim . dim_value ), i , dynamic_shape_map , postfix = \"2\" , ) so . add_node ( graph , so . node ( \"Add\" , inputs = [ f \" { shape_name } _ { i } _1\" , f \" { shape_name } _ { i } _2\" ], outputs = [ f \" { shape_name } _ { i } \" ], ), ) else : create_dim_variable_ ( graph , shape_name , dim . dim_param , override . get ( dim . dim_param , dim . dim_value ), i , dynamic_shape_map , ) dimensions_retrieved . append ( f \" { shape_name } _ { i } \" ) node = so . node ( \"Concat\" , inputs = dimensions_retrieved , outputs = [ f \" { shape_name } \" ], axis = 0 , ) so . add_node ( graph , node ) create_dim_variable_ ( graph , shape_name , dim_param , dim_value , dim_id , dynamic_shape_map , postfix = None ) Create a dimension variable for a dynamic shape model. Parameters: Name Type Description Default graph Graph to add the dimension variable to. required shape_name Name of the output shape tensor. required dim_param Dimension parameter name. required dim_value Dimension value. required dim_id Index of the dimension variable. required dynamic_shape_map Map of dynamic axes names to their inputs indices. required Source code in npc_engine\\services\\utils\\mock.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def create_dim_variable_ ( graph , shape_name , dim_param , dim_value , dim_id , dynamic_shape_map , postfix = None ): \"\"\"Create a dimension variable for a dynamic shape model. Args: graph: Graph to add the dimension variable to. shape_name: Name of the output shape tensor. dim_param: Dimension parameter name. dim_value: Dimension value. dim_id: Index of the dimension variable. dynamic_shape_map: Map of dynamic axes names to their inputs indices. \"\"\" if dim_param != \"\" and dim_param in dynamic_shape_map : node1 = so . node ( \"Shape\" , inputs = [ dynamic_shape_map [ dim_param ][ 0 ]], outputs = [ f \" { shape_name } _ { dim_id } \" if postfix is None else f \" { shape_name } _ { dim_id } _ { postfix } \" ], start = dynamic_shape_map [ dim_param ][ 1 ], end = dynamic_shape_map [ dim_param ][ 1 ] + 1 , ) so . add_node ( graph , node1 ) else : so . add_constant ( graph , f \" { shape_name } _ { dim_id } \" if postfix is None else f \" { shape_name } _ { dim_id } _ { postfix } \" , np . array ( [ dim_value if dim_value != 0 else 1 ], dtype = np . int64 , ), data_type = \"INT64\" , ) create_stub_onnx_model ( onnx_model_path , output_path , dynamic_shape_values ) Create stub onnx model for tests with correct input and output shapes and names. Parameters: Name Type Description Default onnx_model_path str Path to the onnx model. required output_path str Path to the output mock model. required dynamic_shape_values dict Map to specify dynamic dimension values. required Source code in npc_engine\\services\\utils\\mock.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def create_stub_onnx_model ( onnx_model_path : str , output_path : str , dynamic_shape_values : dict ): \"\"\"Create stub onnx model for tests with correct input and output shapes and names. Args: onnx_model_path: Path to the onnx model. output_path: Path to the output mock model. dynamic_shape_values: Map to specify dynamic dimension values. \"\"\" onnx_model = so . graph_from_file ( onnx_model_path ) inputs = onnx_model . input outputs = onnx_model . output mock_graph = so . empty_graph () inverse_data_dict = { value : key for key , value in glob . DATA_TYPES . items ()} dynamic_shape_map = {} for input_ in inputs : so . add_input ( mock_graph , name = input_ . name , dimensions = [ dim . dim_param if dim . dim_param != \"\" else dim . dim_value for dim in input_ . type . tensor_type . shape . dim ], data_type = inverse_data_dict [ input_ . type . tensor_type . elem_type ], ) for i , dim in enumerate ( input_ . type . tensor_type . shape . dim ): if dim . dim_param != \"\" : dynamic_shape_map [ dim . dim_param ] = ( input_ . name , i ) for output in outputs : so . add_output ( mock_graph , name = output . name , dimensions = [ dim . dim_param if dim . dim_param != \"\" else dim . dim_value for dim in output . type . tensor_type . shape . dim ], data_type = inverse_data_dict [ output . type . tensor_type . elem_type ], ) input_names = [ inp . name for inp in inputs ] if output . name not in input_names : build_output_shape_tensor_ ( mock_graph , output , dynamic_shape_map , f \"dynamic_shape_ { output . name } \" , override = dynamic_shape_values , ) node = so . node ( \"ConstantOfShape\" , inputs = [ f \"dynamic_shape_ { output . name } \" ], outputs = [ output . name ], value = xhelp . make_tensor ( name = f \"dynamic_shape_ { output . name } _value\" , data_type = output . type . tensor_type . elem_type , dims = [ 1 ], vals = [ 0 ], ), name = f \"ConstantOfShape_ { output . name } \" , ) so . add_node ( mock_graph , node ) so . graph_to_file ( mock_graph , output_path , onnx_opset_version = 15 ) main ( onnx_model_path , output_path , dynamic_dim ) Create stub onnx model for tests with correct input and output shapes and names. Source code in npc_engine\\services\\utils\\mock.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @click . command () @click . option ( \"-d\" , \"--dynamic-dim\" , type = str , multiple = True , help = \"Specify dynamic dimension in format `<dim name>:<dim value>`.\" , ) @click . option ( \"-m\" , \"--onnx-model-path\" , required = True , type = str ) @click . option ( \"-o\" , \"--output-path\" , required = True , type = str ) def main ( onnx_model_path : str , output_path : str , dynamic_dim : List [ str ]): \"\"\"Create stub onnx model for tests with correct input and output shapes and names.\"\"\" dynamic_dims_map = {} for dim in dynamic_dim : dim_name , dim_value = dim . split ( \":\" ) dynamic_dims_map [ dim_name ] = int ( dim_value ) create_stub_onnx_model ( onnx_model_path , output_path , dynamic_dims_map ) version This module contains project version information.","title":"Reference"},{"location":"inference_engine/reference/#npc_engine.cli","text":"This is the entry point for the command-line interface that starts npc-engine server.","title":"cli"},{"location":"inference_engine/reference/#npc_engine.cli.cli","text":"NPC engine JSON RPC server CLI. Source code in npc_engine\\cli.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 @click . group () @click . option ( \"--verbose/--silent\" , \"-v\" , default = False , help = \"Enable verbose output.\" ) def cli ( verbose : bool ): \"\"\"NPC engine JSON RPC server CLI.\"\"\" logger . remove () if verbose : logger . add ( sys . stdout , format = \" {time} {level} {message} \" , level = \"DEBUG\" , enqueue = True ) logger . add ( os . path . join ( \"Logs\" , \"npc-engine.log\" ), format = \" {time} {level} {message} \" , level = \"DEBUG\" , enqueue = True , rotation = \"500 MB\" , ) click . echo ( click . style ( \"Verbose logging is enabled. (LEVEL=INFO)\" , fg = \"yellow\" , ) ) else : logger . add ( sys . stdout , format = \" {time} {level} {message} \" , level = \"WARNING\" , enqueue = True ) logger . add ( os . path . join ( \"Logs\" , \"npc-engine.log\" ), format = \" {time} {level} {message} \" , level = \"WARNING\" , enqueue = True , rotation = \"500 MB\" , )","title":"cli()"},{"location":"inference_engine/reference/#npc_engine.cli.describe","text":"Show service detailed information. model_id argument follows service resolution rules of the npc-engine. Source code in npc_engine\\cli.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 @cli . command () @click . option ( \"--models-path\" , default = os . environ . get ( \"NPC_ENGINE_MODELS_PATH\" , \"./models\" ), help = \"The path to the folder with service configs.\" , ) @click . argument ( \"model_id\" ) def describe ( models_path : str , model_id : str ): \"\"\"Show service detailed information. model_id argument follows service resolution rules of the npc-engine. \"\"\" model_manager = MetadataManager ( models_path , \"not_used\" ) metadata = model_manager . get_metadata ( model_id ) click . echo ( metadata [ \"id\" ]) click . echo ( get_type_from_dict ( metadata )) click . echo ( \"Service description:\" ) click . echo ( metadata [ \"service_description\" ]) click . echo ( \"Model description:\" ) click . echo ( metadata [ \"readme\" ])","title":"describe()"},{"location":"inference_engine/reference/#npc_engine.cli.download_default_models","text":"Download default models into the folder. Source code in npc_engine\\cli.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @cli . command () @click . option ( \"--models-path\" , default = os . environ . get ( \"NPC_ENGINE_MODELS_PATH\" , \"./models\" ), help = \"The path to the folder with service configs.\" , ) def download_default_models ( models_path : str ): # pragma: no cover \"\"\"Download default models into the folder.\"\"\" model_names = [ \"npc-engine/exported-paraphrase-MiniLM-L6-v2\" , \"npc-engine/exported-bart-light-gail-chatbot\" , \"npc-engine/exported-nemo-quartznet-ctc-stt\" , \"npc-engine/exported-flowtron-waveglow-librispeech-tts\" , ] revs = [ \"main\" , \"crop-fix\" , \"main\" , \"main\" , ] for model , rev in zip ( model_names , revs ): logger . info ( \"Downloading model {} \" , model ) logger . info ( \"Downloading {} \" , model ) tmp_folder = snapshot_download ( repo_id = model , revision = rev , cache_dir = models_path ) os . rename ( tmp_folder , os . path . join ( models_path , model . split ( \"/\" )[ - 1 ]))","title":"download_default_models()"},{"location":"inference_engine/reference/#npc_engine.cli.download_model","text":"Download a model from Huggingface Hub. Source code in npc_engine\\cli.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 @cli . command () @click . option ( \"--models-path\" , default = os . environ . get ( \"NPC_ENGINE_MODELS_PATH\" , \"./models\" , ), help = \"The path to the folder with service configs.\" , ) @click . option ( \"--revision\" , default = \"main\" , help = \"The commit/branch to use.\" ) @click . argument ( \"model_id\" ) def download_model ( models_path : str , revision , model_id : str ): # pragma: no cover \"\"\"Download a model from Huggingface Hub.\"\"\" model_correct = validate_hub_model ( models_path , model_id ) if model_correct : logger . info ( \"Downloading model {} \" , model_id ) tmp_folder = snapshot_download ( repo_id = model_id , revision = revision , cache_dir = models_path ) os . rename ( tmp_folder , os . path . join ( models_path , model_id . split ( \"/\" )[ - 1 ])) else : click . echo ( click . style ( f \" { model_id } is not a valid npc-engine model. You first need to import it.\" , fg = \"yellow\" , ) )","title":"download_model()"},{"location":"inference_engine/reference/#npc_engine.cli.list_models","text":"List the models in the folder. Source code in npc_engine\\cli.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 @cli . command () @click . option ( \"--models-path\" , default = os . environ . get ( \"NPC_ENGINE_MODELS_PATH\" , \"./models\" ), help = \"The path to the folder with service configs.\" , ) def list_models ( models_path : str ): \"\"\"List the models in the folder.\"\"\" metadata_manager = MetadataManager ( models_path , \"not_used\" ) metadata_list = metadata_manager . get_services_metadata () for metadata in metadata_list : click . echo ( metadata [ \"id\" ]) click . echo ( metadata [ \"service\" ]) click . echo ( \"Service description:\" ) click . echo ( metadata [ \"service_short_description\" ]) click . echo ( \"Model description:\" ) click . echo ( metadata [ \"readme\" ] . split ( \" \\n\\n \" )[ 0 ]) click . echo ( \"--------------------\" )","title":"list_models()"},{"location":"inference_engine/reference/#npc_engine.cli.run","text":"Load the models and start JSONRPC server. Source code in npc_engine\\cli.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def run ( port : str , start_all : bool , models_path : str , http : bool ): \"\"\"Load the models and start JSONRPC server.\"\"\" from npc_engine.server.control_service import ControlService context = zmq . asyncio . Context ( io_threads = 5 ) metadata_manager = MetadataManager ( models_path , port ) metadata_manager . port = port control_service = ControlService ( context , metadata_manager ) if not http : from npc_engine.server.server import ZMQServer server = ZMQServer ( context , control_service , metadata_manager , start_all ) else : from npc_engine.server.server import HTTPServer server = HTTPServer ( context , control_service , metadata_manager , start_all ) server . run ()","title":"run()"},{"location":"inference_engine/reference/#npc_engine.cli.run_command","text":"Start the server. Source code in npc_engine\\cli.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 @cli . command ( \"run\" ) @click . option ( \"--port\" , default = \"5555\" , help = \"The port to listen on.\" ) @click . option ( \"--start-all/--dont-start\" , default = True , help = \"Whether to start all services or not.\" , ) @click . option ( \"--models-path\" , default = os . environ . get ( \"NPC_ENGINE_MODELS_PATH\" , \"./models\" ), help = \"The path to the folder with service configs\" , ) @click . option ( \"--http/--zmq\" , default = False , help = \"Whether to use HTTP or ZMQ.\" ) def run_command ( port : str , start_all : bool , models_path : str , http : bool ): \"\"\"Start the server.\"\"\" run ( port , start_all , models_path , http )","title":"run_command()"},{"location":"inference_engine/reference/#npc_engine.cli.set_models_path","text":"Set the default models path. Parameters: Name Type Description Default models_path str The path to the models. required Source code in npc_engine\\cli.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @cli . command () @click . option ( \"--models-path\" , default = os . environ . get ( \"NPC_ENGINE_MODELS_PATH\" , \"./models\" ), help = \"The path to the folder with service configs.\" , ) def set_models_path ( models_path : str ): \"\"\"Set the default models path. Args: models_path (str): The path to the models. \"\"\" os . environ [ \"NPC_ENGINE_MODELS_PATH\" ] = models_path","title":"set_models_path()"},{"location":"inference_engine/reference/#npc_engine.cli.version","text":"Get the npc engine version. Source code in npc_engine\\cli.py 216 217 218 219 @cli . command () def version (): \"\"\"Get the npc engine version.\"\"\" click . echo ( click . style ( f \" { __version__ } \" , bold = True ))","title":"version()"},{"location":"inference_engine/reference/#npc_engine.server","text":"Module with RPC related functionality implementation.","title":"server"},{"location":"inference_engine/reference/#npc_engine.server.control_service","text":"Module that implements control service.","title":"control_service"},{"location":"inference_engine/reference/#npc_engine.server.control_service.ControlService","text":"Service that manages other services and routes requests. Source code in npc_engine\\server\\control_service.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 class ControlService : \"\"\"Service that manages other services and routes requests.\"\"\" def __init__ ( self , zmq_context : zmq . asyncio . Context , metadata_manager : MetadataManager , ) -> None : \"\"\"Initialize control service. Args: zmq_context: asyncio zmq context server_port: server port that will be passed to services for inter-communication \"\"\" self . server_port = metadata_manager . port self . metadata = metadata_manager self . control_dispatcher = Dispatcher () self . control_dispatcher . update ( { \"get_services_metadata\" : self . metadata . get_services_metadata , \"get_service_metadata\" : self . metadata . get_metadata , \"get_service_status\" : self . get_service_status , \"start_service\" : self . start_service , \"stop_service\" : self . stop_service , \"restart_service\" : self . restart_service , \"check_dependency\" : self . check_dependency , } ) self . zmq_context = zmq_context self . services = { service_id : { \"process\" : None , \"socket\" : None , \"state\" : ServiceState . STOPPED , } for service_id in self . metadata . services . keys () } def __del__ ( self ): \"\"\"Stop all services.\"\"\" if hasattr ( self , \"services\" ): for service_id , service in self . services . items (): if service [ \"state\" ] == ServiceState . RUNNING : try : self . stop_service ( service_id ) except Exception : pass async def handle_request ( self , address : str , request : str ) -> str : \"\"\"Parse request string and route request to correct service. Args: address (str): address of the service (either model name or class name) request (str): jsonRPC string Returns: str: jsonRPC response \"\"\" request_dict = json . loads ( request ) service_id = self . metadata . resolve_service ( address , request_dict [ \"method\" ]) self . check_service ( service_id ) logger . info ( f \"Request from { address } \\n Request: { request } \" ) if service_id == \"control\" : return JSONRPCResponseManager . handle ( request , self . control_dispatcher ) . json else : if self . services [ service_id ][ \"state\" ] != ServiceState . RUNNING : raise ValueError ( f \"Service { service_id } is not running\" ) else : socket = self . services [ service_id ][ \"socket\" ] await socket . send_string ( request ) response = await socket . recv_string () return response def check_service ( self , service_id ): \"\"\"Check if the service process is running.\"\"\" if ( service_id != \"control\" and ( self . services [ service_id ][ \"state\" ] == ServiceState . RUNNING or self . services [ service_id ][ \"state\" ] == ServiceState . STARTING or self . services [ service_id ][ \"state\" ] == ServiceState . AWAITING ) and not self . services [ service_id ][ \"process\" ] . is_alive () ): self . services [ service_id ][ \"state\" ] = ServiceState . ERROR raise ValueError ( f \"Error in service { service_id } . Process is not alive.\" ) def get_service_status ( self , service_id ): \"\"\"Get the status of the service.\"\"\" service_id = self . metadata . resolve_service ( service_id , None ) self . check_service ( service_id ) return self . services [ service_id ][ \"state\" ] def start_service ( self , service_id ): \"\"\"Start the service.\"\"\" service_id = self . metadata . resolve_service ( service_id , None ) self . check_service ( service_id ) if self . services [ service_id ][ \"state\" ] == ServiceState . RUNNING : raise ValueError ( f \"Service { service_id } is already running\" ) process = Process ( target = service_process , args = ( self . metadata , service_id , logger ), daemon = True , ) process . start () self . services [ service_id ][ \"process\" ] = process self . services [ service_id ][ \"state\" ] = ServiceState . STARTING self . services [ service_id ][ \"socket\" ] = self . zmq_context . socket ( zmq . REQ ) self . services [ service_id ][ \"socket\" ] . setsockopt ( zmq . LINGER , 0 ) self . services [ service_id ][ \"socket\" ] . setsockopt ( zmq . RCVTIMEO , 10000 ) self . services [ service_id ][ \"socket\" ] . connect ( self . metadata . services [ service_id ] . uri ) try : asyncio . create_task ( self . confirm_state_coroutine ( service_id )) except RuntimeError : logger . warning ( \"Create task to confirm service state failed.\" + \" Probably asyncio loop is not running.\" + \" Trying to execute it via asyncio.run()\" ) asyncio . run ( self . confirm_state_coroutine ( service_id )) async def confirm_state_coroutine ( self , service_id ): \"\"\"Confirm the state of the service.\"\"\" request = json . dumps ({ \"jsonrpc\" : \"2.0\" , \"method\" : \"status\" , \"id\" : 1 }) socket = self . services [ service_id ][ \"socket\" ] try : await socket . send_string ( request ) except zmq . Again : self . services [ service_id ][ \"state\" ] = ServiceState . STARTING logger . warning ( f \" { service_id } is not responding.\" ) await asyncio . sleep ( 1 ) await self . confirm_state_coroutine ( service_id ) return response = None while response is None : try : response = await socket . recv_string () except zmq . Again : self . services [ service_id ][ \"state\" ] = ServiceState . STARTING logger . warning ( f \" { service_id } is not responding.\" ) await asyncio . sleep ( 1 ) continue resp_dict = json . loads ( response ) if resp_dict [ \"result\" ] == ServiceState . RUNNING : self . services [ service_id ][ \"state\" ] = ServiceState . RUNNING elif resp_dict [ \"result\" ] == ServiceState . STARTING : logger . info ( f \"Service { service_id } responds but still starting\" ) await asyncio . sleep ( 1 ) await self . confirm_state_coroutine ( service_id ) else : logger . warning ( f \"Service { service_id } failed to start and returned incorrect state.\" ) self . services [ service_id ][ \"state\" ] = ServiceState . ERROR def stop_service ( self , service_id ): \"\"\"Stop the service.\"\"\" service_id = self . metadata . resolve_service ( service_id , None ) self . check_service ( service_id ) if self . services [ service_id ][ \"state\" ] != ServiceState . RUNNING : raise ValueError ( f \"Service { service_id } is not running\" ) self . services [ service_id ][ \"socket\" ] . close () self . services [ service_id ][ \"socket\" ] = None self . services [ service_id ][ \"process\" ] . terminate () self . services [ service_id ][ \"process\" ] = None self . services [ service_id ][ \"state\" ] = ServiceState . STOPPED def restart_service ( self , service_id ): \"\"\"Restart the service.\"\"\" self . stop_service ( service_id ) self . start_service ( service_id ) def check_dependency ( self , service_id , dependency ): \"\"\"Check if the service has the dependency.\"\"\" service_id = self . metadata . resolve_service ( service_id , None ) self . metadata . services [ service_id ] . dependencies . append ( dependency ) self . metadata . check_dependency_cycles ()","title":"ControlService"},{"location":"inference_engine/reference/#npc_engine.server.control_service.ControlService.__del__","text":"Stop all services. Source code in npc_engine\\server\\control_service.py 85 86 87 88 89 90 91 92 93 def __del__ ( self ): \"\"\"Stop all services.\"\"\" if hasattr ( self , \"services\" ): for service_id , service in self . services . items (): if service [ \"state\" ] == ServiceState . RUNNING : try : self . stop_service ( service_id ) except Exception : pass","title":"__del__()"},{"location":"inference_engine/reference/#npc_engine.server.control_service.ControlService.__init__","text":"Initialize control service. Parameters: Name Type Description Default zmq_context zmq . asyncio . Context asyncio zmq context required server_port server port that will be passed to services for inter-communication required Source code in npc_engine\\server\\control_service.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def __init__ ( self , zmq_context : zmq . asyncio . Context , metadata_manager : MetadataManager , ) -> None : \"\"\"Initialize control service. Args: zmq_context: asyncio zmq context server_port: server port that will be passed to services for inter-communication \"\"\" self . server_port = metadata_manager . port self . metadata = metadata_manager self . control_dispatcher = Dispatcher () self . control_dispatcher . update ( { \"get_services_metadata\" : self . metadata . get_services_metadata , \"get_service_metadata\" : self . metadata . get_metadata , \"get_service_status\" : self . get_service_status , \"start_service\" : self . start_service , \"stop_service\" : self . stop_service , \"restart_service\" : self . restart_service , \"check_dependency\" : self . check_dependency , } ) self . zmq_context = zmq_context self . services = { service_id : { \"process\" : None , \"socket\" : None , \"state\" : ServiceState . STOPPED , } for service_id in self . metadata . services . keys () }","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.server.control_service.ControlService.check_dependency","text":"Check if the service has the dependency. Source code in npc_engine\\server\\control_service.py 223 224 225 226 227 def check_dependency ( self , service_id , dependency ): \"\"\"Check if the service has the dependency.\"\"\" service_id = self . metadata . resolve_service ( service_id , None ) self . metadata . services [ service_id ] . dependencies . append ( dependency ) self . metadata . check_dependency_cycles ()","title":"check_dependency()"},{"location":"inference_engine/reference/#npc_engine.server.control_service.ControlService.check_service","text":"Check if the service process is running. Source code in npc_engine\\server\\control_service.py 120 121 122 123 124 125 126 127 128 129 130 131 132 def check_service ( self , service_id ): \"\"\"Check if the service process is running.\"\"\" if ( service_id != \"control\" and ( self . services [ service_id ][ \"state\" ] == ServiceState . RUNNING or self . services [ service_id ][ \"state\" ] == ServiceState . STARTING or self . services [ service_id ][ \"state\" ] == ServiceState . AWAITING ) and not self . services [ service_id ][ \"process\" ] . is_alive () ): self . services [ service_id ][ \"state\" ] = ServiceState . ERROR raise ValueError ( f \"Error in service { service_id } . Process is not alive.\" )","title":"check_service()"},{"location":"inference_engine/reference/#npc_engine.server.control_service.ControlService.confirm_state_coroutine","text":"Confirm the state of the service. Source code in npc_engine\\server\\control_service.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 async def confirm_state_coroutine ( self , service_id ): \"\"\"Confirm the state of the service.\"\"\" request = json . dumps ({ \"jsonrpc\" : \"2.0\" , \"method\" : \"status\" , \"id\" : 1 }) socket = self . services [ service_id ][ \"socket\" ] try : await socket . send_string ( request ) except zmq . Again : self . services [ service_id ][ \"state\" ] = ServiceState . STARTING logger . warning ( f \" { service_id } is not responding.\" ) await asyncio . sleep ( 1 ) await self . confirm_state_coroutine ( service_id ) return response = None while response is None : try : response = await socket . recv_string () except zmq . Again : self . services [ service_id ][ \"state\" ] = ServiceState . STARTING logger . warning ( f \" { service_id } is not responding.\" ) await asyncio . sleep ( 1 ) continue resp_dict = json . loads ( response ) if resp_dict [ \"result\" ] == ServiceState . RUNNING : self . services [ service_id ][ \"state\" ] = ServiceState . RUNNING elif resp_dict [ \"result\" ] == ServiceState . STARTING : logger . info ( f \"Service { service_id } responds but still starting\" ) await asyncio . sleep ( 1 ) await self . confirm_state_coroutine ( service_id ) else : logger . warning ( f \"Service { service_id } failed to start and returned incorrect state.\" ) self . services [ service_id ][ \"state\" ] = ServiceState . ERROR","title":"confirm_state_coroutine()"},{"location":"inference_engine/reference/#npc_engine.server.control_service.ControlService.get_service_status","text":"Get the status of the service. Source code in npc_engine\\server\\control_service.py 134 135 136 137 138 def get_service_status ( self , service_id ): \"\"\"Get the status of the service.\"\"\" service_id = self . metadata . resolve_service ( service_id , None ) self . check_service ( service_id ) return self . services [ service_id ][ \"state\" ]","title":"get_service_status()"},{"location":"inference_engine/reference/#npc_engine.server.control_service.ControlService.handle_request","text":"Parse request string and route request to correct service. Parameters: Name Type Description Default address str address of the service (either model name or class name) required request str jsonRPC string required Returns: Name Type Description str str jsonRPC response Source code in npc_engine\\server\\control_service.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 async def handle_request ( self , address : str , request : str ) -> str : \"\"\"Parse request string and route request to correct service. Args: address (str): address of the service (either model name or class name) request (str): jsonRPC string Returns: str: jsonRPC response \"\"\" request_dict = json . loads ( request ) service_id = self . metadata . resolve_service ( address , request_dict [ \"method\" ]) self . check_service ( service_id ) logger . info ( f \"Request from { address } \\n Request: { request } \" ) if service_id == \"control\" : return JSONRPCResponseManager . handle ( request , self . control_dispatcher ) . json else : if self . services [ service_id ][ \"state\" ] != ServiceState . RUNNING : raise ValueError ( f \"Service { service_id } is not running\" ) else : socket = self . services [ service_id ][ \"socket\" ] await socket . send_string ( request ) response = await socket . recv_string () return response","title":"handle_request()"},{"location":"inference_engine/reference/#npc_engine.server.control_service.ControlService.restart_service","text":"Restart the service. Source code in npc_engine\\server\\control_service.py 218 219 220 221 def restart_service ( self , service_id ): \"\"\"Restart the service.\"\"\" self . stop_service ( service_id ) self . start_service ( service_id )","title":"restart_service()"},{"location":"inference_engine/reference/#npc_engine.server.control_service.ControlService.start_service","text":"Start the service. Source code in npc_engine\\server\\control_service.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def start_service ( self , service_id ): \"\"\"Start the service.\"\"\" service_id = self . metadata . resolve_service ( service_id , None ) self . check_service ( service_id ) if self . services [ service_id ][ \"state\" ] == ServiceState . RUNNING : raise ValueError ( f \"Service { service_id } is already running\" ) process = Process ( target = service_process , args = ( self . metadata , service_id , logger ), daemon = True , ) process . start () self . services [ service_id ][ \"process\" ] = process self . services [ service_id ][ \"state\" ] = ServiceState . STARTING self . services [ service_id ][ \"socket\" ] = self . zmq_context . socket ( zmq . REQ ) self . services [ service_id ][ \"socket\" ] . setsockopt ( zmq . LINGER , 0 ) self . services [ service_id ][ \"socket\" ] . setsockopt ( zmq . RCVTIMEO , 10000 ) self . services [ service_id ][ \"socket\" ] . connect ( self . metadata . services [ service_id ] . uri ) try : asyncio . create_task ( self . confirm_state_coroutine ( service_id )) except RuntimeError : logger . warning ( \"Create task to confirm service state failed.\" + \" Probably asyncio loop is not running.\" + \" Trying to execute it via asyncio.run()\" ) asyncio . run ( self . confirm_state_coroutine ( service_id ))","title":"start_service()"},{"location":"inference_engine/reference/#npc_engine.server.control_service.ControlService.stop_service","text":"Stop the service. Source code in npc_engine\\server\\control_service.py 206 207 208 209 210 211 212 213 214 215 216 def stop_service ( self , service_id ): \"\"\"Stop the service.\"\"\" service_id = self . metadata . resolve_service ( service_id , None ) self . check_service ( service_id ) if self . services [ service_id ][ \"state\" ] != ServiceState . RUNNING : raise ValueError ( f \"Service { service_id } is not running\" ) self . services [ service_id ][ \"socket\" ] . close () self . services [ service_id ][ \"socket\" ] = None self . services [ service_id ][ \"process\" ] . terminate () self . services [ service_id ][ \"process\" ] = None self . services [ service_id ][ \"state\" ] = ServiceState . STOPPED","title":"stop_service()"},{"location":"inference_engine/reference/#npc_engine.server.control_service.ServiceState","text":"Enum for the state of the service. Source code in npc_engine\\server\\control_service.py 14 15 16 17 18 19 20 21 22 class ServiceState : \"\"\"Enum for the state of the service.\"\"\" STARTING = \"starting\" RUNNING = \"running\" STOPPED = \"stopped\" AWAITING = \"awaiting\" TIMEOUT = \"timeout\" ERROR = \"error\"","title":"ServiceState"},{"location":"inference_engine/reference/#npc_engine.server.control_service.service_process","text":"Service subprocess function. Starts the service and runs it's loop. Source code in npc_engine\\server\\control_service.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def service_process ( metadata : MetadataManager , service_id : str , logger ) -> None : \"\"\"Service subprocess function. Starts the service and runs it's loop. \"\"\" set_logger ( logger ) context = zmq . Context () service = services . BaseService . create ( context , metadata . services [ service_id ] . path , metadata . services [ service_id ] . uri , service_id , ) service . loop ()","title":"service_process()"},{"location":"inference_engine/reference/#npc_engine.server.control_service.set_logger","text":"Set the logger for the service. Source code in npc_engine\\server\\control_service.py 25 26 27 28 def set_logger ( logger_ ): \"\"\"Set the logger for the service.\"\"\" global logger logger = logger_","title":"set_logger()"},{"location":"inference_engine/reference/#npc_engine.server.metadata_manager","text":"Module that implements lifetime and discoverability of the services.","title":"metadata_manager"},{"location":"inference_engine/reference/#npc_engine.server.metadata_manager.MetadataManager","text":"Class that manages service name resolution and metadata. Source code in npc_engine\\server\\metadata_manager.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 class MetadataManager : \"\"\"Class that manages service name resolution and metadata.\"\"\" def __init__ ( self , path : str , port : str ): \"\"\"Create model manager and load models from the given path.\"\"\" self . services = self . _scan_path ( path ) self . models_path = path self . port = port def resolve_service ( self , id_or_type , method = None ): \"\"\"Resolve service id or type to service id.\"\"\" service_id = None if id_or_type == \"control\" : service_id = \"control\" else : if id_or_type in self . services : service_id = id_or_type if service_id is None : for service_key , service in self . services . items (): if service . type == id_or_type or service . api_name == id_or_type : service_id = service_key break if service_id is None and method is not None : service_id = self . resolve_by_method ( method ) elif service_id is None : raise ValueError ( f \"Service { id_or_type } not found\" ) return service_id def get_services_metadata ( self ): \"\"\"List the models in the folder.\"\"\" return [ self . get_metadata ( service ) for service in self . services ] def resolve_by_method ( self , method_name ): \"\"\"Resolve service id by method name.\"\"\" for service_id , service in self . services . items (): if method_name in service . api_methods : return service_id raise ValueError ( f \"Service with method { method_name } not found\" ) def check_dependency_cycles ( self ): \"\"\"Check if there are any dependency cycles.\"\"\" # build graph graph = {} for service_id , service in self . services . items (): graph [ service_id ] = [ self . resolve_service ( dep ) for dep in service . dependencies ] # Tarjan's algorithm visited = {} llvs = {} stack = [] sccs = [] self . idx = 0 for node in graph : if node not in visited : self . __scc ( graph , node , llvs , visited , stack , sccs ) cycles = [ scc for scc in sccs if len ( scc ) > 1 ] if len ( cycles ) > 0 : to_str = \" \\n \" . join ([ \" -> \" . join ( cycle + [ cycle [ 0 ]]) for cycle in cycles ]) raise ValueError ( f \"There are dependency cycles: { to_str } \" ) del self . idx def __scc ( self , graph , node , llvs , visited , stack , sccs ): visited [ node ] = self . idx llvs [ node ] = self . idx self . idx += 1 stack . append ( node ) for v in graph [ node ]: if v not in visited : self . __scc ( graph , v , llvs , visited , stack , sccs ) llvs [ node ] = min ( llvs [ node ], llvs [ v ]) elif v in stack : llvs [ node ] = min ( llvs [ node ], visited . get ( v , - 1 )) if llvs [ node ] == visited [ node ]: sccs . append ([ stack . pop ()]) while sccs [ - 1 ][ - 1 ] != node : sccs [ - 1 ] . append ( stack . pop ()) def _scan_path ( self , path : str ) -> Dict [ str , ServiceDescriptor ]: \"\"\"Scan services defined in the given path.\"\"\" norm_path = ntpath . normpath ( path ) . replace ( \" \\\\ \" , os . path . sep ) paths = [ f . path for f in os . scandir ( norm_path ) if f . is_dir () and os . path . exists ( os . path . join ( f , \"config.yml\" )) ] svcs = {} for path in paths : with open ( os . path . join ( path , \"config.yml\" )) as f : config_dict = yaml . safe_load ( f ) uri = build_ipc_uri ( os . path . basename ( path )) cls = getattr ( services , config_dict . get ( \"model_type\" , config_dict . get ( \"type\" , None )), ) svcs [ os . path . basename ( path )] = ServiceDescriptor ( id = os . path . basename ( path ), type = config_dict . get ( \"model_type\" , config_dict . get ( \"type\" , None )), path = path , uri = uri , api_name = cls . get_api_name (), api_methods = cls . API_METHODS , dependencies = [], ) return svcs def get_metadata ( self , service_id : str ) -> Dict [ str , str ]: \"\"\"Print the model from the path.\"\"\" service_id = self . resolve_service ( service_id ) config_path = os . path . join ( self . services [ service_id ] . path , \"config.yml\" ) readme_path = os . path . join ( self . services [ service_id ] . path , \"README.md\" ) with open ( config_path ) as f : config_dict = yaml . load ( f , Loader = yaml . Loader ) try : with open ( readme_path ) as f : readme = f . read () . split ( \"---\" )[ - 1 ] except FileNotFoundError : readme = \"\" cls = getattr ( services , config_dict . get ( \"model_type\" , config_dict . get ( \"type\" , None )), ) return { \"id\" : self . services [ service_id ] . id , \"service\" : self . services [ service_id ] . type , \"api_name\" : self . services [ service_id ] . api_name , \"path\" : self . services [ service_id ] . path , \"service_short_description\" : cls . models [ self . services [ service_id ] . type ] . __doc__ . split ( \" \\n\\n \" )[ 0 ], \"service_description\" : cls . models [ self . services [ service_id ] . type ] . __doc__ , \"readme\" : readme , }","title":"MetadataManager"},{"location":"inference_engine/reference/#npc_engine.server.metadata_manager.MetadataManager.__init__","text":"Create model manager and load models from the given path. Source code in npc_engine\\server\\metadata_manager.py 22 23 24 25 26 def __init__ ( self , path : str , port : str ): \"\"\"Create model manager and load models from the given path.\"\"\" self . services = self . _scan_path ( path ) self . models_path = path self . port = port","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.server.metadata_manager.MetadataManager._scan_path","text":"Scan services defined in the given path. Source code in npc_engine\\server\\metadata_manager.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def _scan_path ( self , path : str ) -> Dict [ str , ServiceDescriptor ]: \"\"\"Scan services defined in the given path.\"\"\" norm_path = ntpath . normpath ( path ) . replace ( \" \\\\ \" , os . path . sep ) paths = [ f . path for f in os . scandir ( norm_path ) if f . is_dir () and os . path . exists ( os . path . join ( f , \"config.yml\" )) ] svcs = {} for path in paths : with open ( os . path . join ( path , \"config.yml\" )) as f : config_dict = yaml . safe_load ( f ) uri = build_ipc_uri ( os . path . basename ( path )) cls = getattr ( services , config_dict . get ( \"model_type\" , config_dict . get ( \"type\" , None )), ) svcs [ os . path . basename ( path )] = ServiceDescriptor ( id = os . path . basename ( path ), type = config_dict . get ( \"model_type\" , config_dict . get ( \"type\" , None )), path = path , uri = uri , api_name = cls . get_api_name (), api_methods = cls . API_METHODS , dependencies = [], ) return svcs","title":"_scan_path()"},{"location":"inference_engine/reference/#npc_engine.server.metadata_manager.MetadataManager.check_dependency_cycles","text":"Check if there are any dependency cycles. Source code in npc_engine\\server\\metadata_manager.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def check_dependency_cycles ( self ): \"\"\"Check if there are any dependency cycles.\"\"\" # build graph graph = {} for service_id , service in self . services . items (): graph [ service_id ] = [ self . resolve_service ( dep ) for dep in service . dependencies ] # Tarjan's algorithm visited = {} llvs = {} stack = [] sccs = [] self . idx = 0 for node in graph : if node not in visited : self . __scc ( graph , node , llvs , visited , stack , sccs ) cycles = [ scc for scc in sccs if len ( scc ) > 1 ] if len ( cycles ) > 0 : to_str = \" \\n \" . join ([ \" -> \" . join ( cycle + [ cycle [ 0 ]]) for cycle in cycles ]) raise ValueError ( f \"There are dependency cycles: { to_str } \" ) del self . idx","title":"check_dependency_cycles()"},{"location":"inference_engine/reference/#npc_engine.server.metadata_manager.MetadataManager.get_metadata","text":"Print the model from the path. Source code in npc_engine\\server\\metadata_manager.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def get_metadata ( self , service_id : str ) -> Dict [ str , str ]: \"\"\"Print the model from the path.\"\"\" service_id = self . resolve_service ( service_id ) config_path = os . path . join ( self . services [ service_id ] . path , \"config.yml\" ) readme_path = os . path . join ( self . services [ service_id ] . path , \"README.md\" ) with open ( config_path ) as f : config_dict = yaml . load ( f , Loader = yaml . Loader ) try : with open ( readme_path ) as f : readme = f . read () . split ( \"---\" )[ - 1 ] except FileNotFoundError : readme = \"\" cls = getattr ( services , config_dict . get ( \"model_type\" , config_dict . get ( \"type\" , None )), ) return { \"id\" : self . services [ service_id ] . id , \"service\" : self . services [ service_id ] . type , \"api_name\" : self . services [ service_id ] . api_name , \"path\" : self . services [ service_id ] . path , \"service_short_description\" : cls . models [ self . services [ service_id ] . type ] . __doc__ . split ( \" \\n\\n \" )[ 0 ], \"service_description\" : cls . models [ self . services [ service_id ] . type ] . __doc__ , \"readme\" : readme , }","title":"get_metadata()"},{"location":"inference_engine/reference/#npc_engine.server.metadata_manager.MetadataManager.get_services_metadata","text":"List the models in the folder. Source code in npc_engine\\server\\metadata_manager.py 48 49 50 def get_services_metadata ( self ): \"\"\"List the models in the folder.\"\"\" return [ self . get_metadata ( service ) for service in self . services ]","title":"get_services_metadata()"},{"location":"inference_engine/reference/#npc_engine.server.metadata_manager.MetadataManager.resolve_by_method","text":"Resolve service id by method name. Source code in npc_engine\\server\\metadata_manager.py 52 53 54 55 56 57 def resolve_by_method ( self , method_name ): \"\"\"Resolve service id by method name.\"\"\" for service_id , service in self . services . items (): if method_name in service . api_methods : return service_id raise ValueError ( f \"Service with method { method_name } not found\" )","title":"resolve_by_method()"},{"location":"inference_engine/reference/#npc_engine.server.metadata_manager.MetadataManager.resolve_service","text":"Resolve service id or type to service id. Source code in npc_engine\\server\\metadata_manager.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def resolve_service ( self , id_or_type , method = None ): \"\"\"Resolve service id or type to service id.\"\"\" service_id = None if id_or_type == \"control\" : service_id = \"control\" else : if id_or_type in self . services : service_id = id_or_type if service_id is None : for service_key , service in self . services . items (): if service . type == id_or_type or service . api_name == id_or_type : service_id = service_key break if service_id is None and method is not None : service_id = self . resolve_by_method ( method ) elif service_id is None : raise ValueError ( f \"Service { id_or_type } not found\" ) return service_id","title":"resolve_service()"},{"location":"inference_engine/reference/#npc_engine.server.server","text":"Module that implements ZMQ server communication over JSON-RPC 2.0 (https://www.jsonrpc.org/specification).","title":"server"},{"location":"inference_engine/reference/#npc_engine.server.server.BaseServer","text":"Bases: ABC Base JSON RPC server. Source code in npc_engine\\server\\server.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 class BaseServer ( ABC ): \"\"\"Base JSON RPC server.\"\"\" def __init__ ( self , zmq_context : zmq . asyncio . Context , service_manager : ControlService , metadata : MetadataManager , start_services : bool = True , ): \"\"\"Initialize the server. Args: zmq_context: The ZMQ context. service_manager: Control service that manages services. metadata: Metadata manager. start_services: Start services on initialization. \"\"\" self . context = zmq_context if sys . platform == \"win32\" : asyncio . set_event_loop_policy ( asyncio . WindowsSelectorEventLoopPolicy ()) self . socket_ipc = self . context . socket ( zmq . ROUTER ) self . socket_ipc . setsockopt ( zmq . LINGER , 0 ) ipc_uri = build_ipc_uri ( \"self\" ) ipc_path = ipc_uri . replace ( \"ipc://\" , \"\" ) os . makedirs ( os . path . dirname ( ipc_path ), exist_ok = True ) self . socket_ipc . bind ( build_ipc_uri ( \"self\" )) self . metadata = metadata self . service_manager = service_manager self . start_services_flag = start_services @abstractmethod def run ( self ): \"\"\"Run the server.\"\"\" pass async def msg_loop ( self , socket : zmq . asyncio . Socket ): \"\"\"Asynchoriniously handle a request and reply.\"\"\" while True : address = await socket . recv () _ = await socket . recv () message = await socket . recv_string () logger . info ( f \"Received request to { address } : { message } \" ) asyncio . create_task ( self . handle_reply ( socket , address , message )) async def interrupt_loop ( self ): \"\"\"Handle interrupts loop.\"\"\" while True : await asyncio . sleep ( 1 ) async def start_services ( self ): \"\"\"Start all services.\"\"\" logger . info ( \"Starting services\" ) for service in self . service_manager . services : self . service_manager . start_service ( service ) async def handle_reply ( self , socket , address : str , message : str ): \"\"\"Handle message and reply.\"\"\" logging . info ( \"Handling reply\" ) start = time . time () try : address_str = address . decode ( \"utf-8\" ) except UnicodeDecodeError : address_str = address . hex () try : response = await self . service_manager . handle_request ( address_str , message ) except Exception as e : response = { \"code\" : - 32000 , \"message\" : f \"Internal error: { type ( e ) } { e } \" , \"data\" : tb . extract_tb ( e . __traceback__ ) . format () if hasattr ( e , \"__traceback__\" ) else None , } response = json . dumps ( response ) end = time . time () logger . info ( \"Handle message time: %d \" % ( end - start )) logger . info ( \"Message reply: %s \" % ( response )) # Send reply back to client await socket . send ( address , zmq . SNDMORE ) await socket . send_string ( \"\" , zmq . SNDMORE ) await socket . send_string ( response )","title":"BaseServer"},{"location":"inference_engine/reference/#npc_engine.server.server.BaseServer.__init__","text":"Initialize the server. Parameters: Name Type Description Default zmq_context zmq . asyncio . Context The ZMQ context. required service_manager ControlService Control service that manages services. required metadata MetadataManager Metadata manager. required start_services bool Start services on initialization. True Source code in npc_engine\\server\\server.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , zmq_context : zmq . asyncio . Context , service_manager : ControlService , metadata : MetadataManager , start_services : bool = True , ): \"\"\"Initialize the server. Args: zmq_context: The ZMQ context. service_manager: Control service that manages services. metadata: Metadata manager. start_services: Start services on initialization. \"\"\" self . context = zmq_context if sys . platform == \"win32\" : asyncio . set_event_loop_policy ( asyncio . WindowsSelectorEventLoopPolicy ()) self . socket_ipc = self . context . socket ( zmq . ROUTER ) self . socket_ipc . setsockopt ( zmq . LINGER , 0 ) ipc_uri = build_ipc_uri ( \"self\" ) ipc_path = ipc_uri . replace ( \"ipc://\" , \"\" ) os . makedirs ( os . path . dirname ( ipc_path ), exist_ok = True ) self . socket_ipc . bind ( build_ipc_uri ( \"self\" )) self . metadata = metadata self . service_manager = service_manager self . start_services_flag = start_services","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.server.server.BaseServer.handle_reply","text":"Handle message and reply. Source code in npc_engine\\server\\server.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 async def handle_reply ( self , socket , address : str , message : str ): \"\"\"Handle message and reply.\"\"\" logging . info ( \"Handling reply\" ) start = time . time () try : address_str = address . decode ( \"utf-8\" ) except UnicodeDecodeError : address_str = address . hex () try : response = await self . service_manager . handle_request ( address_str , message ) except Exception as e : response = { \"code\" : - 32000 , \"message\" : f \"Internal error: { type ( e ) } { e } \" , \"data\" : tb . extract_tb ( e . __traceback__ ) . format () if hasattr ( e , \"__traceback__\" ) else None , } response = json . dumps ( response ) end = time . time () logger . info ( \"Handle message time: %d \" % ( end - start )) logger . info ( \"Message reply: %s \" % ( response )) # Send reply back to client await socket . send ( address , zmq . SNDMORE ) await socket . send_string ( \"\" , zmq . SNDMORE ) await socket . send_string ( response )","title":"handle_reply()"},{"location":"inference_engine/reference/#npc_engine.server.server.BaseServer.interrupt_loop","text":"Handle interrupts loop. Source code in npc_engine\\server\\server.py 66 67 68 69 async def interrupt_loop ( self ): \"\"\"Handle interrupts loop.\"\"\" while True : await asyncio . sleep ( 1 )","title":"interrupt_loop()"},{"location":"inference_engine/reference/#npc_engine.server.server.BaseServer.msg_loop","text":"Asynchoriniously handle a request and reply. Source code in npc_engine\\server\\server.py 57 58 59 60 61 62 63 64 async def msg_loop ( self , socket : zmq . asyncio . Socket ): \"\"\"Asynchoriniously handle a request and reply.\"\"\" while True : address = await socket . recv () _ = await socket . recv () message = await socket . recv_string () logger . info ( f \"Received request to { address } : { message } \" ) asyncio . create_task ( self . handle_reply ( socket , address , message ))","title":"msg_loop()"},{"location":"inference_engine/reference/#npc_engine.server.server.BaseServer.run","text":"Run the server. Source code in npc_engine\\server\\server.py 52 53 54 55 @abstractmethod def run ( self ): \"\"\"Run the server.\"\"\" pass","title":"run()"},{"location":"inference_engine/reference/#npc_engine.server.server.BaseServer.start_services","text":"Start all services. Source code in npc_engine\\server\\server.py 71 72 73 74 75 async def start_services ( self ): \"\"\"Start all services.\"\"\" logger . info ( \"Starting services\" ) for service in self . service_manager . services : self . service_manager . start_service ( service )","title":"start_services()"},{"location":"inference_engine/reference/#npc_engine.server.server.HTTPServer","text":"Bases: BaseServer HTTP server to handle requests. Source code in npc_engine\\server\\server.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 class HTTPServer ( BaseServer ): \"\"\"HTTP server to handle requests.\"\"\" def __init__ ( self , zmq_context : zmq . asyncio . Context , service_manager : ControlService , metadata : MetadataManager , start_services : bool = True , ): \"\"\"Create a server on the port.\"\"\" super () . __init__ ( zmq_context , service_manager , metadata , start_services ) self . app = web . Application () self . app . router . add_get ( \"/\" , self . handle_request ) self . app . router . add_get ( \"/ {name} \" , self . handle_request ) self . app . router . add_post ( \"/\" , self . handle_request ) self . app . router . add_post ( \"/ {name} \" , self . handle_request ) async def add_msg_loop_ipc ( self , app ): \"\"\"Add message loop for IPC.\"\"\" app [ \"msg_loop_ipc\" ] = asyncio . create_task ( self . msg_loop ( self . socket_ipc )) async def add_interrupt_loop ( self , app ): \"\"\"Add interrupt loop.\"\"\" app [ \"interrupt_loop\" ] = asyncio . create_task ( self . interrupt_loop ()) def run ( self ): \"\"\"Run an npc-engine json rpc server and start listening.\"\"\" if self . start_services_flag : self . app . on_startup . append ( lambda _ : asyncio . create_task ( self . start_services ()) ) self . app . on_startup . append ( self . add_msg_loop_ipc ) self . app . on_startup . append ( self . add_interrupt_loop ) logger . info ( \"Starting server\" ) web . run_app ( self . app , host = \"localhost\" , port = int ( self . metadata . port )) async def handle_request ( self , request ): \"\"\"Handle request.\"\"\" try : address = request . match_info . get ( \"name\" , \"xxxxxxxxxxxx\" ) message = await request . text () logger . info ( f \"Received request to { address } : { message } \" ) response = await self . service_manager . handle_request ( address , message ) except Exception as e : response = { \"code\" : - 32000 , \"message\" : f \"Internal error: { type ( e ) } { e } \" , \"data\" : tb . extract_tb ( e . __traceback__ ) . format () if hasattr ( e , \"__traceback__\" ) else None , } response = json . dumps ( response ) return web . json_response ( text = response )","title":"HTTPServer"},{"location":"inference_engine/reference/#npc_engine.server.server.HTTPServer.__init__","text":"Create a server on the port. Source code in npc_engine\\server\\server.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def __init__ ( self , zmq_context : zmq . asyncio . Context , service_manager : ControlService , metadata : MetadataManager , start_services : bool = True , ): \"\"\"Create a server on the port.\"\"\" super () . __init__ ( zmq_context , service_manager , metadata , start_services ) self . app = web . Application () self . app . router . add_get ( \"/\" , self . handle_request ) self . app . router . add_get ( \"/ {name} \" , self . handle_request ) self . app . router . add_post ( \"/\" , self . handle_request ) self . app . router . add_post ( \"/ {name} \" , self . handle_request )","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.server.server.HTTPServer.add_interrupt_loop","text":"Add interrupt loop. Source code in npc_engine\\server\\server.py 173 174 175 async def add_interrupt_loop ( self , app ): \"\"\"Add interrupt loop.\"\"\" app [ \"interrupt_loop\" ] = asyncio . create_task ( self . interrupt_loop ())","title":"add_interrupt_loop()"},{"location":"inference_engine/reference/#npc_engine.server.server.HTTPServer.add_msg_loop_ipc","text":"Add message loop for IPC. Source code in npc_engine\\server\\server.py 169 170 171 async def add_msg_loop_ipc ( self , app ): \"\"\"Add message loop for IPC.\"\"\" app [ \"msg_loop_ipc\" ] = asyncio . create_task ( self . msg_loop ( self . socket_ipc ))","title":"add_msg_loop_ipc()"},{"location":"inference_engine/reference/#npc_engine.server.server.HTTPServer.handle_request","text":"Handle request. Source code in npc_engine\\server\\server.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 async def handle_request ( self , request ): \"\"\"Handle request.\"\"\" try : address = request . match_info . get ( \"name\" , \"xxxxxxxxxxxx\" ) message = await request . text () logger . info ( f \"Received request to { address } : { message } \" ) response = await self . service_manager . handle_request ( address , message ) except Exception as e : response = { \"code\" : - 32000 , \"message\" : f \"Internal error: { type ( e ) } { e } \" , \"data\" : tb . extract_tb ( e . __traceback__ ) . format () if hasattr ( e , \"__traceback__\" ) else None , } response = json . dumps ( response ) return web . json_response ( text = response )","title":"handle_request()"},{"location":"inference_engine/reference/#npc_engine.server.server.HTTPServer.run","text":"Run an npc-engine json rpc server and start listening. Source code in npc_engine\\server\\server.py 177 178 179 180 181 182 183 184 185 186 def run ( self ): \"\"\"Run an npc-engine json rpc server and start listening.\"\"\" if self . start_services_flag : self . app . on_startup . append ( lambda _ : asyncio . create_task ( self . start_services ()) ) self . app . on_startup . append ( self . add_msg_loop_ipc ) self . app . on_startup . append ( self . add_interrupt_loop ) logger . info ( \"Starting server\" ) web . run_app ( self . app , host = \"localhost\" , port = int ( self . metadata . port ))","title":"run()"},{"location":"inference_engine/reference/#npc_engine.server.server.ZMQServer","text":"Bases: BaseServer Json rpc server over zmq. Source code in npc_engine\\server\\server.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 class ZMQServer ( BaseServer ): \"\"\"Json rpc server over zmq.\"\"\" def __init__ ( self , zmq_context : zmq . asyncio . Context , service_manager : ControlService , metadata : MetadataManager , start_services : bool = True , ): \"\"\"Create a server on the port.\"\"\" super () . __init__ ( zmq_context , service_manager , metadata , start_services ) self . socket = self . context . socket ( zmq . ROUTER ) self . socket . setsockopt ( zmq . LINGER , 0 ) self . socket . bind ( f \"tcp://*: { metadata . port } \" ) def run ( self ): \"\"\"Run an npc-engine json rpc server and start listening.\"\"\" logger . info ( \"Starting server\" ) asyncio . get_event_loop () . run_until_complete ( self . loop ()) async def loop ( self ): \"\"\"Run the server loop.\"\"\" try : if self . start_services_flag : await asyncio . create_task ( self . start_services ()) logger . info ( \"Starting message loop\" ) await asyncio . gather ( self . msg_loop ( self . socket ), self . msg_loop ( self . socket_ipc ), self . interrupt_loop (), ) except Exception as e : logger . error ( f \"Error in message loop: { e } \" ) logger . error ( tb . format_exc ()) raise e finally : logger . info ( \"Closing message loop\" ) self . socket . close () self . socket_ipc . close () self . context . destroy ()","title":"ZMQServer"},{"location":"inference_engine/reference/#npc_engine.server.server.ZMQServer.__init__","text":"Create a server on the port. Source code in npc_engine\\server\\server.py 110 111 112 113 114 115 116 117 118 119 120 121 def __init__ ( self , zmq_context : zmq . asyncio . Context , service_manager : ControlService , metadata : MetadataManager , start_services : bool = True , ): \"\"\"Create a server on the port.\"\"\" super () . __init__ ( zmq_context , service_manager , metadata , start_services ) self . socket = self . context . socket ( zmq . ROUTER ) self . socket . setsockopt ( zmq . LINGER , 0 ) self . socket . bind ( f \"tcp://*: { metadata . port } \" )","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.server.server.ZMQServer.loop","text":"Run the server loop. Source code in npc_engine\\server\\server.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 async def loop ( self ): \"\"\"Run the server loop.\"\"\" try : if self . start_services_flag : await asyncio . create_task ( self . start_services ()) logger . info ( \"Starting message loop\" ) await asyncio . gather ( self . msg_loop ( self . socket ), self . msg_loop ( self . socket_ipc ), self . interrupt_loop (), ) except Exception as e : logger . error ( f \"Error in message loop: { e } \" ) logger . error ( tb . format_exc ()) raise e finally : logger . info ( \"Closing message loop\" ) self . socket . close () self . socket_ipc . close () self . context . destroy ()","title":"loop()"},{"location":"inference_engine/reference/#npc_engine.server.server.ZMQServer.run","text":"Run an npc-engine json rpc server and start listening. Source code in npc_engine\\server\\server.py 123 124 125 126 def run ( self ): \"\"\"Run an npc-engine json rpc server and start listening.\"\"\" logger . info ( \"Starting server\" ) asyncio . get_event_loop () . run_until_complete ( self . loop ())","title":"run()"},{"location":"inference_engine/reference/#npc_engine.server.utils","text":"Utility functions for RPC communication.","title":"utils"},{"location":"inference_engine/reference/#npc_engine.server.utils.build_ipc_uri","text":"Build ipc uri for the given service. Source code in npc_engine\\server\\utils.py 44 45 46 def build_ipc_uri ( service_id : str ) -> str : \"\"\"Build ipc uri for the given service.\"\"\" return f \"ipc:// { os . path . join ( user_cache_dir ( 'npc-engine' ), service_id ) } \"","title":"build_ipc_uri()"},{"location":"inference_engine/reference/#npc_engine.server.utils.schema_to_json","text":"Iterate the schema and return simplified dictionary. Source code in npc_engine\\server\\utils.py 9 10 11 12 13 14 15 16 17 18 19 20 def schema_to_json ( s : Dict [ str , Any ], fill_value : Callable [[ str ], Any ] = lambda _ : \"\" ) -> Dict [ str , Any ]: \"\"\"Iterate the schema and return simplified dictionary.\"\"\" if \"type\" not in s and \"anyOf\" in s : return fill_value ( s [ \"title\" ]) elif \"type\" in s and s [ \"type\" ] == \"object\" : return { k : schema_to_json ( v ) for k , v in s [ \"properties\" ] . items ()} elif \"type\" in s and s [ \"type\" ] == \"array\" : return [ schema_to_json ( s [ \"items\" ])] else : raise ValueError ( f \"Unknown schema type: { s } \" )","title":"schema_to_json()"},{"location":"inference_engine/reference/#npc_engine.server.utils.start_test_server","text":"Start the test server. Parameters: Name Type Description Default port str The port to start the server on. required models_path str The path to the models. required Source code in npc_engine\\server\\utils.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def start_test_server ( port : str , models_path : str ): # pragma: no cover \"\"\"Start the test server. Args: port: The port to start the server on. models_path: The path to the models. \"\"\" subprocess . Popen ( [ \"npc-engine\" , \"--verbose\" , \"run\" , \"--port\" , port , \"--models-path\" , models_path , ], creationflags = subprocess . CREATE_NEW_CONSOLE , )","title":"start_test_server()"},{"location":"inference_engine/reference/#npc_engine.service_clients","text":"Module implementing the clients for services.","title":"service_clients"},{"location":"inference_engine/reference/#npc_engine.service_clients.control_client","text":"Control interface client implementation.","title":"control_client"},{"location":"inference_engine/reference/#npc_engine.service_clients.control_client.ControlClient","text":"Bases: ServiceClient Json rpc client for control requests. Source code in npc_engine\\service_clients\\control_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class ControlClient ( ServiceClient ): \"\"\"Json rpc client for control requests.\"\"\" def __init__ ( self , zmq_context : zmq . Context ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , \"control\" ) def start_service ( self , service_id ): \"\"\"Send a start service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"start_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) def stop_service ( self , service_id ): \"\"\"Send a stop service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"stop_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) def get_service_status ( self , service_id ) -> str : \"\"\"Send a get service status request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_service_status\" , \"id\" : 0 , \"params\" : [ service_id ], } return self . send_request ( request ) def restart_service ( self , service_id ): \"\"\"Send a restart service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"restart_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request ) def get_services_metadata ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Send a get services metadata request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_services_metadata\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) def get_service_metadata ( self , service_id : str ) -> Dict [ str , Any ]: \"\"\"Send a get services metadata request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_service_metadata\" , \"id\" : 0 , \"params\" : [ service_id ], } return self . send_request ( request ) def check_dependency ( self , service_id , dependency_id ) -> bool : \"\"\"Send a check dependency request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"check_dependency\" , \"id\" : 0 , \"params\" : [ service_id , dependency_id ], } return self . send_request ( request ) @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"control\"","title":"ControlClient"},{"location":"inference_engine/reference/#npc_engine.service_clients.control_client.ControlClient.__init__","text":"Connect to the server on the port. Source code in npc_engine\\service_clients\\control_client.py 10 11 12 def __init__ ( self , zmq_context : zmq . Context ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , \"control\" )","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.service_clients.control_client.ControlClient.check_dependency","text":"Send a check dependency request to the server. Source code in npc_engine\\service_clients\\control_client.py 74 75 76 77 78 79 80 81 82 def check_dependency ( self , service_id , dependency_id ) -> bool : \"\"\"Send a check dependency request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"check_dependency\" , \"id\" : 0 , \"params\" : [ service_id , dependency_id ], } return self . send_request ( request )","title":"check_dependency()"},{"location":"inference_engine/reference/#npc_engine.service_clients.control_client.ControlClient.get_api_name","text":"Return the name of the API. Source code in npc_engine\\service_clients\\control_client.py 84 85 86 87 @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"control\"","title":"get_api_name()"},{"location":"inference_engine/reference/#npc_engine.service_clients.control_client.ControlClient.get_service_metadata","text":"Send a get services metadata request to the server. Source code in npc_engine\\service_clients\\control_client.py 64 65 66 67 68 69 70 71 72 def get_service_metadata ( self , service_id : str ) -> Dict [ str , Any ]: \"\"\"Send a get services metadata request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_service_metadata\" , \"id\" : 0 , \"params\" : [ service_id ], } return self . send_request ( request )","title":"get_service_metadata()"},{"location":"inference_engine/reference/#npc_engine.service_clients.control_client.ControlClient.get_service_status","text":"Send a get service status request to the server. Source code in npc_engine\\service_clients\\control_client.py 34 35 36 37 38 39 40 41 42 def get_service_status ( self , service_id ) -> str : \"\"\"Send a get service status request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_service_status\" , \"id\" : 0 , \"params\" : [ service_id ], } return self . send_request ( request )","title":"get_service_status()"},{"location":"inference_engine/reference/#npc_engine.service_clients.control_client.ControlClient.get_services_metadata","text":"Send a get services metadata request to the server. Source code in npc_engine\\service_clients\\control_client.py 54 55 56 57 58 59 60 61 62 def get_services_metadata ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Send a get services metadata request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_services_metadata\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request )","title":"get_services_metadata()"},{"location":"inference_engine/reference/#npc_engine.service_clients.control_client.ControlClient.restart_service","text":"Send a restart service request to the server. Source code in npc_engine\\service_clients\\control_client.py 44 45 46 47 48 49 50 51 52 def restart_service ( self , service_id ): \"\"\"Send a restart service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"restart_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request )","title":"restart_service()"},{"location":"inference_engine/reference/#npc_engine.service_clients.control_client.ControlClient.start_service","text":"Send a start service request to the server. Source code in npc_engine\\service_clients\\control_client.py 14 15 16 17 18 19 20 21 22 def start_service ( self , service_id ): \"\"\"Send a start service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"start_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request )","title":"start_service()"},{"location":"inference_engine/reference/#npc_engine.service_clients.control_client.ControlClient.stop_service","text":"Send a stop service request to the server. Source code in npc_engine\\service_clients\\control_client.py 24 25 26 27 28 29 30 31 32 def stop_service ( self , service_id ): \"\"\"Send a stop service request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"stop_service\" , \"id\" : 0 , \"params\" : [ service_id ], } self . send_request ( request )","title":"stop_service()"},{"location":"inference_engine/reference/#npc_engine.service_clients.sequence_classifier_client","text":"Huggingface sequence classifier interface client implementation.","title":"sequence_classifier_client"},{"location":"inference_engine/reference/#npc_engine.service_clients.sequence_classifier_client.SequenceClassifierClient","text":"Bases: ServiceClient Json rpc client for sequence classifier service. Source code in npc_engine\\service_clients\\sequence_classifier_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class SequenceClassifierClient ( ServiceClient ): \"\"\"Json rpc client for sequence classifier service.\"\"\" def __init__ ( self , zmq_context : zmq . Context , service_id : str = \"SequenceClassifierAPI\" , ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , service_id ) def classify ( self , texts : List [ Union [ str , Tuple [ str , str ]]]) -> str : \"\"\"Send a classify request to the SequenceClassifierAPI. Args: texts: Batch of texts to classify. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"classify\" , \"id\" : 0 , \"params\" : [ texts ], } reply = self . send_request ( request ) return reply @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"SequenceClassifierAPI\"","title":"SequenceClassifierClient"},{"location":"inference_engine/reference/#npc_engine.service_clients.sequence_classifier_client.SequenceClassifierClient.__init__","text":"Connect to the server on the port. Source code in npc_engine\\service_clients\\sequence_classifier_client.py 10 11 12 13 14 15 16 def __init__ ( self , zmq_context : zmq . Context , service_id : str = \"SequenceClassifierAPI\" , ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , service_id )","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.service_clients.sequence_classifier_client.SequenceClassifierClient.classify","text":"Send a classify request to the SequenceClassifierAPI. Parameters: Name Type Description Default texts List [ Union [ str , Tuple [ str , str ]]] Batch of texts to classify. required Source code in npc_engine\\service_clients\\sequence_classifier_client.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def classify ( self , texts : List [ Union [ str , Tuple [ str , str ]]]) -> str : \"\"\"Send a classify request to the SequenceClassifierAPI. Args: texts: Batch of texts to classify. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"classify\" , \"id\" : 0 , \"params\" : [ texts ], } reply = self . send_request ( request ) return reply","title":"classify()"},{"location":"inference_engine/reference/#npc_engine.service_clients.sequence_classifier_client.SequenceClassifierClient.get_api_name","text":"Return the name of the API. Source code in npc_engine\\service_clients\\sequence_classifier_client.py 33 34 35 36 @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"SequenceClassifierAPI\"","title":"get_api_name()"},{"location":"inference_engine/reference/#npc_engine.service_clients.service_client","text":"Module that implements ZMQ base client communication over JSON-RPC 2.0 (https://www.jsonrpc.org/specification).","title":"service_client"},{"location":"inference_engine/reference/#npc_engine.service_clients.service_client.ServiceClient","text":"Bases: ABC Base json rpc client. Source code in npc_engine\\service_clients\\service_client.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class ServiceClient ( ABC ): \"\"\"Base json rpc client.\"\"\" clients = {} def __init_subclass__ ( cls , ** kwargs ): \"\"\"Init subclass where service classes get registered to be discovered.\"\"\" super () . __init_subclass__ ( ** kwargs ) cls . clients [ cls . get_api_name ()] = cls def __init__ ( self , zmq_context : zmq . Context , service_id : str = None ): \"\"\"Connect to the server on the port.\"\"\" self . context = zmq_context self . socket = self . context . socket ( zmq . REQ ) self . socket . setsockopt ( zmq . LINGER , 0 ) self . socket . setsockopt ( zmq . IDENTITY , service_id . encode ( \"utf-8\" ) if service_id else self . get_api_name () . encode ( \"utf-8\" ), ) self . socket . connect ( build_ipc_uri ( \"self\" )) logger . info ( \"Connected to server\" ) def send_request ( self , request : Dict [ str , Any ]) -> Any : \"\"\"Send request to the server and return the response. Args: request: The request to send to the server. Returns: The result from the server. \"\"\" logger . trace ( f \"Sending request: { request } \" ) self . socket . send_json ( request ) response = self . socket . recv_json () logger . trace ( f \"Received response: { response } \" ) if \"result\" in response : return response [ \"result\" ] elif \"code\" in response : raise RuntimeError ( f \"code: { response [ 'code' ] } . { response [ 'message' ] } \" ) @abstractclassmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" pass @classmethod def get_api_client ( cls , api_name : str ) -> \"ServiceClient\" : \"\"\"Return the client for the api.\"\"\" try : return cls . clients [ api_name ] except KeyError : raise RuntimeError ( f \"Client for the API { api_name } not found. Available clients: { list ( cls . clients . keys ()) } \" )","title":"ServiceClient"},{"location":"inference_engine/reference/#npc_engine.service_clients.service_client.ServiceClient.__init__","text":"Connect to the server on the port. Source code in npc_engine\\service_clients\\service_client.py 22 23 24 25 26 27 28 29 30 31 32 33 34 def __init__ ( self , zmq_context : zmq . Context , service_id : str = None ): \"\"\"Connect to the server on the port.\"\"\" self . context = zmq_context self . socket = self . context . socket ( zmq . REQ ) self . socket . setsockopt ( zmq . LINGER , 0 ) self . socket . setsockopt ( zmq . IDENTITY , service_id . encode ( \"utf-8\" ) if service_id else self . get_api_name () . encode ( \"utf-8\" ), ) self . socket . connect ( build_ipc_uri ( \"self\" )) logger . info ( \"Connected to server\" )","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.service_clients.service_client.ServiceClient.__init_subclass__","text":"Init subclass where service classes get registered to be discovered. Source code in npc_engine\\service_clients\\service_client.py 17 18 19 20 def __init_subclass__ ( cls , ** kwargs ): \"\"\"Init subclass where service classes get registered to be discovered.\"\"\" super () . __init_subclass__ ( ** kwargs ) cls . clients [ cls . get_api_name ()] = cls","title":"__init_subclass__()"},{"location":"inference_engine/reference/#npc_engine.service_clients.service_client.ServiceClient.get_api_client","text":"Return the client for the api. Source code in npc_engine\\service_clients\\service_client.py 59 60 61 62 63 64 65 66 67 @classmethod def get_api_client ( cls , api_name : str ) -> \"ServiceClient\" : \"\"\"Return the client for the api.\"\"\" try : return cls . clients [ api_name ] except KeyError : raise RuntimeError ( f \"Client for the API { api_name } not found. Available clients: { list ( cls . clients . keys ()) } \" )","title":"get_api_client()"},{"location":"inference_engine/reference/#npc_engine.service_clients.service_client.ServiceClient.get_api_name","text":"Return the name of the API. Source code in npc_engine\\service_clients\\service_client.py 54 55 56 57 @abstractclassmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" pass","title":"get_api_name()"},{"location":"inference_engine/reference/#npc_engine.service_clients.service_client.ServiceClient.send_request","text":"Send request to the server and return the response. Parameters: Name Type Description Default request Dict [ str , Any ] The request to send to the server. required Returns: Type Description Any The result from the server. Source code in npc_engine\\service_clients\\service_client.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def send_request ( self , request : Dict [ str , Any ]) -> Any : \"\"\"Send request to the server and return the response. Args: request: The request to send to the server. Returns: The result from the server. \"\"\" logger . trace ( f \"Sending request: { request } \" ) self . socket . send_json ( request ) response = self . socket . recv_json () logger . trace ( f \"Received response: { response } \" ) if \"result\" in response : return response [ \"result\" ] elif \"code\" in response : raise RuntimeError ( f \"code: { response [ 'code' ] } . { response [ 'message' ] } \" )","title":"send_request()"},{"location":"inference_engine/reference/#npc_engine.service_clients.similarity_client","text":"Huggingface semantic similarity interface client implementation.","title":"similarity_client"},{"location":"inference_engine/reference/#npc_engine.service_clients.similarity_client.SimilarityClient","text":"Bases: ServiceClient Json rpc client for semantic similarity service. Source code in npc_engine\\service_clients\\similarity_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class SimilarityClient ( ServiceClient ): \"\"\"Json rpc client for semantic similarity service.\"\"\" def __init__ ( self , zmq_context : zmq . Context , service_id : str = \"SimilarityAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , service_id ) def compare ( self , query : str , context : List [ str ]) -> float : \"\"\"Send a comparison request to the server. Args: query: A string to compute similarity with contexts. context: A list of strings to compute similiarity with query. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"compare\" , \"id\" : 0 , \"params\" : [ query , context ], } reply = self . send_request ( request ) return reply @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"SimilarityAPI\"","title":"SimilarityClient"},{"location":"inference_engine/reference/#npc_engine.service_clients.similarity_client.SimilarityClient.__init__","text":"Connect to the server on the port. Source code in npc_engine\\service_clients\\similarity_client.py 10 11 12 def __init__ ( self , zmq_context : zmq . Context , service_id : str = \"SimilarityAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , service_id )","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.service_clients.similarity_client.SimilarityClient.compare","text":"Send a comparison request to the server. Parameters: Name Type Description Default query str A string to compute similarity with contexts. required context List [ str ] A list of strings to compute similiarity with query. required Source code in npc_engine\\service_clients\\similarity_client.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def compare ( self , query : str , context : List [ str ]) -> float : \"\"\"Send a comparison request to the server. Args: query: A string to compute similarity with contexts. context: A list of strings to compute similiarity with query. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"compare\" , \"id\" : 0 , \"params\" : [ query , context ], } reply = self . send_request ( request ) return reply","title":"compare()"},{"location":"inference_engine/reference/#npc_engine.service_clients.similarity_client.SimilarityClient.get_api_name","text":"Return the name of the API. Source code in npc_engine\\service_clients\\similarity_client.py 30 31 32 33 @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"SimilarityAPI\"","title":"get_api_name()"},{"location":"inference_engine/reference/#npc_engine.service_clients.text_generation_client","text":"Huggingface chatbot interface client implementation.","title":"text_generation_client"},{"location":"inference_engine/reference/#npc_engine.service_clients.text_generation_client.TextGenerationClient","text":"Bases: ServiceClient Json rpc client for chatbot service. Source code in npc_engine\\service_clients\\text_generation_client.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 class TextGenerationClient ( ServiceClient ): \"\"\"Json rpc client for chatbot service.\"\"\" def __init__ ( self , zmq_context : zmq . Context , service_id : str = \"TextGenerationAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , service_id ) def generate_reply ( self , context : Dict [ str , Any ]) -> str : \"\"\"Send a chatbot request to the server. Args: context: A dictionary containing the chatbot request. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"generate_reply\" , \"id\" : 0 , \"params\" : [ context ], } reply = self . send_request ( request ) return reply def get_prompt_template ( self ) -> str : \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_prompt_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) def get_context_template ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_context_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) def get_special_tokens ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_special_tokens\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request ) @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"TextGenerationAPI\"","title":"TextGenerationClient"},{"location":"inference_engine/reference/#npc_engine.service_clients.text_generation_client.TextGenerationClient.__init__","text":"Connect to the server on the port. Source code in npc_engine\\service_clients\\text_generation_client.py 10 11 12 def __init__ ( self , zmq_context : zmq . Context , service_id : str = \"TextGenerationAPI\" ): \"\"\"Connect to the server on the port.\"\"\" super () . __init__ ( zmq_context , service_id )","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.service_clients.text_generation_client.TextGenerationClient.generate_reply","text":"Send a chatbot request to the server. Parameters: Name Type Description Default context Dict [ str , Any ] A dictionary containing the chatbot request. required Source code in npc_engine\\service_clients\\text_generation_client.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def generate_reply ( self , context : Dict [ str , Any ]) -> str : \"\"\"Send a chatbot request to the server. Args: context: A dictionary containing the chatbot request. \"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"generate_reply\" , \"id\" : 0 , \"params\" : [ context ], } reply = self . send_request ( request ) return reply","title":"generate_reply()"},{"location":"inference_engine/reference/#npc_engine.service_clients.text_generation_client.TextGenerationClient.get_api_name","text":"Return the name of the API. Source code in npc_engine\\service_clients\\text_generation_client.py 59 60 61 62 @classmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" return \"TextGenerationAPI\"","title":"get_api_name()"},{"location":"inference_engine/reference/#npc_engine.service_clients.text_generation_client.TextGenerationClient.get_context_template","text":"Send a chatbot request to the server. Source code in npc_engine\\service_clients\\text_generation_client.py 39 40 41 42 43 44 45 46 47 def get_context_template ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_context_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request )","title":"get_context_template()"},{"location":"inference_engine/reference/#npc_engine.service_clients.text_generation_client.TextGenerationClient.get_prompt_template","text":"Send a chatbot request to the server. Source code in npc_engine\\service_clients\\text_generation_client.py 29 30 31 32 33 34 35 36 37 def get_prompt_template ( self ) -> str : \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_prompt_template\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request )","title":"get_prompt_template()"},{"location":"inference_engine/reference/#npc_engine.service_clients.text_generation_client.TextGenerationClient.get_special_tokens","text":"Send a chatbot request to the server. Source code in npc_engine\\service_clients\\text_generation_client.py 49 50 51 52 53 54 55 56 57 def get_special_tokens ( self ) -> Dict [ str , Any ]: \"\"\"Send a chatbot request to the server.\"\"\" request = { \"jsonrpc\" : \"2.0\" , \"method\" : \"get_special_tokens\" , \"id\" : 0 , \"params\" : [], } return self . send_request ( request )","title":"get_special_tokens()"},{"location":"inference_engine/reference/#npc_engine.services","text":"Module that contains everything related to deep learning models. For your model API to be discovered it must be imported here","title":"services"},{"location":"inference_engine/reference/#npc_engine.services.base_service","text":"Module with Model base class.","title":"base_service"},{"location":"inference_engine/reference/#npc_engine.services.base_service.BaseService","text":"Bases: FactoryMixin , ABC Abstract base class for managed services. Source code in npc_engine\\services\\base_service.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 class BaseService ( FactoryMixin , ABC ): \"\"\"Abstract base class for managed services.\"\"\" def __init__ ( self , service_id : str , context : zmq . Context , uri : str , providers : List [ str ] = None , * args , ** kwargs , ): \"\"\"Initialize the service. Args: context (zmq.Context): ZMQ context uri (str): URI to serve requests to dependency_clients (list(ServiceClient)): List of dependency clients \"\"\" super ( BaseService , self ) . __init__ () self . zmq_context = context self . socket = context . socket ( zmq . REP ) self . socket . setsockopt ( zmq . LINGER , 0 ) if uri . startswith ( \"ipc://\" ): os . makedirs ( Path ( uri . replace ( \"ipc://\" , \"\" )) . parent , exist_ok = True ) os . chmod ( Path ( uri . replace ( \"ipc://\" , \"\" )) . parent , 777 ) self . socket . bind ( uri ) self . service_id = service_id self . control_client = None self . _set_and_validate_providers ( providers ) @classmethod @abstractmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" pass def create_client ( self , name : str ): \"\"\"Get a dependency client by name to use it in service logic. Args: name (str): Name of the dependency Returns: ServiceClient: Client for the dependency \"\"\" if self . control_client is None : self . control_client = ControlClient ( self . zmq_context ) if name == \"control\" : return self . control_client self . control_client . check_dependency ( self . service_id , name ) api_name = self . control_client . get_service_metadata ( name )[ \"api_name\" ] return ControlClient . get_api_client ( api_name )( self . zmq_context , name ) def loop ( self ): \"\"\"Run service main loop that accepts json rpc.\"\"\" try : dispatcher = Dispatcher () dispatcher . update ( self . build_api_dict ()) dispatcher . update ({ \"status\" : self . status }) while True : request = self . socket . recv_string () response = JSONRPCResponseManager . handle ( request , dispatcher ) self . socket . send_string ( response . json ) except Exception as e : logger . exception ( e ) raise e finally : self . socket . close () self . zmq_context . destroy () def status ( self ): \"\"\"Return status of the service. Returns: ServiceState \"\"\" from npc_engine.server.control_service import ServiceState return ServiceState . RUNNING def build_api_dict ( self ) -> Dict [ str , Callable ]: \"\"\"Build api dict. Returns: dict(str,str): Mapping \"method_name\" -> callable that will be exposed to API \"\"\" api_dict = {} for method in type ( self ) . API_METHODS : logger . info ( f \"Registering method { method } for service { type ( self ) . __name__ } \" ) api_dict [ method ] = getattr ( self , method ) return api_dict def get_providers ( self ) -> List [ str ]: \"\"\"Return onnxruntime providers to use.\"\"\" return self . providers def _set_and_validate_providers ( self , providers : List [ str ]): if providers is not None : self . providers = providers for provider in self . providers : if provider == \"gpu\" : provider = [ prov for prov in rt . get_available_providers () if \"DML\" in prov or \"CUDA\" in prov or \"Tensorrt\" in prov ][ 0 ] if provider == \"cpu\" : provider = [ prov for prov in rt . get_available_providers () if \"CPU\" in prov ][ 0 ] if provider not in rt . get_available_providers (): raise RuntimeError ( f \"Provider { provider } is not available\" ) else : self . providers = [ \"DmlExecutionProvider\" ]","title":"BaseService"},{"location":"inference_engine/reference/#npc_engine.services.base_service.BaseService.__init__","text":"Initialize the service. Parameters: Name Type Description Default context zmq . Context ZMQ context required uri str URI to serve requests to required dependency_clients list(ServiceClient List of dependency clients required Source code in npc_engine\\services\\base_service.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , service_id : str , context : zmq . Context , uri : str , providers : List [ str ] = None , * args , ** kwargs , ): \"\"\"Initialize the service. Args: context (zmq.Context): ZMQ context uri (str): URI to serve requests to dependency_clients (list(ServiceClient)): List of dependency clients \"\"\" super ( BaseService , self ) . __init__ () self . zmq_context = context self . socket = context . socket ( zmq . REP ) self . socket . setsockopt ( zmq . LINGER , 0 ) if uri . startswith ( \"ipc://\" ): os . makedirs ( Path ( uri . replace ( \"ipc://\" , \"\" )) . parent , exist_ok = True ) os . chmod ( Path ( uri . replace ( \"ipc://\" , \"\" )) . parent , 777 ) self . socket . bind ( uri ) self . service_id = service_id self . control_client = None self . _set_and_validate_providers ( providers )","title":"__init__()"},{"location":"inference_engine/reference/#npc_engine.services.base_service.BaseService.build_api_dict","text":"Build api dict. Returns: Name Type Description dict str , str Mapping \"method_name\" -> callable that will be exposed to API Source code in npc_engine\\services\\base_service.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def build_api_dict ( self ) -> Dict [ str , Callable ]: \"\"\"Build api dict. Returns: dict(str,str): Mapping \"method_name\" -> callable that will be exposed to API \"\"\" api_dict = {} for method in type ( self ) . API_METHODS : logger . info ( f \"Registering method { method } for service { type ( self ) . __name__ } \" ) api_dict [ method ] = getattr ( self , method ) return api_dict","title":"build_api_dict()"},{"location":"inference_engine/reference/#npc_engine.services.base_service.BaseService.create_client","text":"Get a dependency client by name to use it in service logic. Parameters: Name Type Description Default name str Name of the dependency required Returns: Name Type Description ServiceClient Client for the dependency Source code in npc_engine\\services\\base_service.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def create_client ( self , name : str ): \"\"\"Get a dependency client by name to use it in service logic. Args: name (str): Name of the dependency Returns: ServiceClient: Client for the dependency \"\"\" if self . control_client is None : self . control_client = ControlClient ( self . zmq_context ) if name == \"control\" : return self . control_client self . control_client . check_dependency ( self . service_id , name ) api_name = self . control_client . get_service_metadata ( name )[ \"api_name\" ] return ControlClient . get_api_client ( api_name )( self . zmq_context , name )","title":"create_client()"},{"location":"inference_engine/reference/#npc_engine.services.base_service.BaseService.get_api_name","text":"Return the name of the API. Source code in npc_engine\\services\\base_service.py 45 46 47 48 49 @classmethod @abstractmethod def get_api_name ( cls ) -> str : \"\"\"Return the name of the API.\"\"\" pass","title":"get_api_name()"},{"location":"inference_engine/reference/#npc_engine.services.base_service.BaseService.get_providers","text":"Return onnxruntime providers to use. Source code in npc_engine\\services\\base_service.py 110 111 112 def get_providers ( self ) -> List [ str ]: \"\"\"Return onnxruntime providers to use.\"\"\" return self . providers","title":"get_providers()"},{"location":"inference_engine/reference/#npc_engine.services.base_service.BaseService.loop","text":"Run service main loop that accepts json rpc. Source code in npc_engine\\services\\base_service.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def loop ( self ): \"\"\"Run service main loop that accepts json rpc.\"\"\" try : dispatcher = Dispatcher () dispatcher . update ( self . build_api_dict ()) dispatcher . update ({ \"status\" : self . status }) while True : request = self . socket . recv_string () response = JSONRPCResponseManager . handle ( request , dispatcher ) self . socket . send_string ( response . json ) except Exception as e : logger . exception ( e ) raise e finally : self . socket . close () self . zmq_context . destroy ()","title":"loop()"},{"location":"inference_engine/reference/#npc_engine.services.base_service.BaseService.status","text":"Return status of the service. Returns: Type Description ServiceState Source code in npc_engine\\services\\base_service.py 85 86 87 88 89 90 91 92 93 def status ( self ): \"\"\"Return status of the service. Returns: ServiceState \"\"\" from npc_engine.server.control_service import ServiceState return ServiceState . RUNNING","title":"status()"},{"location":"inference_engine/reference/#npc_engine.services.factory_mixin","text":"Factory mixin for service.","title":"factory_mixin"},{"location":"inference_engine/reference/#npc_engine.services.factory_mixin.FactoryMixin","text":"Mixin base class for services that can be created via create method. Source code in npc_engine\\services\\factory_mixin.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class FactoryMixin : \"\"\"Mixin base class for services that can be created via create method.\"\"\" models = {} def __init_subclass__ ( cls , ** kwargs ): \"\"\"Init subclass where service classes get registered to be discovered.\"\"\" super () . __init_subclass__ ( ** kwargs ) cls . models [ cls . __name__ ] = cls @classmethod def create ( cls , context : zmq . Context , path : str , uri : str , service_id : str ): \"\"\"Create a service from the path. Args: context (zmq.Context): ZMQ context path (str): Path to the service uri (str): URI to serve requests to Returns: Service: Service instance \"\"\" config_path = os . path . join ( path , \"config.yml\" ) with open ( config_path ) as f : config_dict = yaml . load ( f , Loader = yaml . Loader ) config_dict [ \"model_path\" ] = path model_cls = cls . models [ get_type_from_dict ( config_dict )] return model_cls ( ** config_dict , context = context , uri = uri , service_id = service_id )","title":"FactoryMixin"},{"location":"inference_engine/reference/#npc_engine.services.factory_mixin.FactoryMixin.__init_subclass__","text":"Init subclass where service classes get registered to be discovered. Source code in npc_engine\\services\\factory_mixin.py 13 14 15 16 def __init_subclass__ ( cls , ** kwargs ): \"\"\"Init subclass where service classes get registered to be discovered.\"\"\" super () . __init_subclass__ ( ** kwargs ) cls . models [ cls . __name__ ] = cls","title":"__init_subclass__()"},{"location":"inference_engine/reference/#npc_engine.services.factory_mixin.FactoryMixin.create","text":"Create a service from the path. Parameters: Name Type Description Default context zmq . Context ZMQ context required path str Path to the service required uri str URI to serve requests to required Returns: Name Type Description Service Service instance Source code in npc_engine\\services\\factory_mixin.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 @classmethod def create ( cls , context : zmq . Context , path : str , uri : str , service_id : str ): \"\"\"Create a service from the path. Args: context (zmq.Context): ZMQ context path (str): Path to the service uri (str): URI to serve requests to Returns: Service: Service instance \"\"\" config_path = os . path . join ( path , \"config.yml\" ) with open ( config_path ) as f : config_dict = yaml . load ( f , Loader = yaml . Loader ) config_dict [ \"model_path\" ] = path model_cls = cls . models [ get_type_from_dict ( config_dict )] return model_cls ( ** config_dict , context = context , uri = uri , service_id = service_id )","title":"create()"},{"location":"inference_engine/reference/#npc_engine.services.persona_dialogue","text":"Module that implements persona dialogue API.","title":"persona_dialogue"},{"location":"inference_engine/reference/#npc_engine.services.persona_dialogue.persona_dialogue","text":"Persona Dialogue implementation using the provided services.","title":"persona_dialogue"},{"location":"inference_engine/reference/#npc_engine.services.persona_dialogue.persona_dialogue.PersonaDialogue","text":"Bases: PersonaDialogueAPI Persona dialogue API implementation using the provided services. Scripted utterances are matched via semantic similarity. Utterances are generated using the provided text generation service. Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 class PersonaDialogue ( PersonaDialogueAPI ): \"\"\"Persona dialogue API implementation using the provided services. Scripted utterances are matched via semantic similarity. Utterances are generated using the provided text generation service. \"\"\" def __init__ ( self , text_generation_svc : str = \"TextGenerationAPI\" , similarity_svc : str = \"SimilarityAPI\" , * args , ** kwargs , ): \"\"\"Initialize the persona dialogue API. Args: text_generation_service: Name of the text generation service. similarity_api: Name of the similarity API. similarity_threshold: Threshold for semantic similarity. \"\"\" super () . __init__ ( * args , ** kwargs ) self . text_generation_service = self . create_client ( text_generation_svc ) self . similarity_api = self . create_client ( similarity_svc ) self . dialogues = {} self . dialogue_id_counter = 0 self . context_template = self . text_generation_service . get_context_template () self . _validate_context_template ( self . context_template ) def start_dialogue ( self , name1 : str = None , persona1 : str = None , name2 : str = None , persona2 : str = None , location_name : str = None , location_description : str = None , dialogue_id : str = None , * args , ** kwargs , ) -> str : \"\"\"Start a dialogue between two characters. Args: dialogue_id: ID of the dialogue. If None it will be named automatically. name1: Name of the first character. persona1: Persona of the first character. name2: Name of the second character. persona2: Persona of the second character. location_name: Name of the place where dialogue happens. location_description: Description of the place where dialogue happens. Returns: Dialogue id. \"\"\" if dialogue_id in self . dialogues : raise ValueError ( \"Dialogue already exists.\" ) if dialogue_id is None : dialogue_id = f \"dialogue_ { self . dialogue_id_counter } \" self . dialogue_id_counter += 1 while dialogue_id in self . dialogues : dialogue_id = f \"dialogue_ { self . dialogue_id_counter } \" self . dialogue_id_counter += 1 self . dialogues [ dialogue_id ] = { \"characters\" : [ { \"name\" : name1 , \"persona\" : persona1 }, { \"name\" : name2 , \"persona\" : persona2 }, ], \"location\" : { \"name\" : location_name , \"description\" : location_description }, \"history\" : [], \"other\" : kwargs . get ( \"other\" , {}), } return dialogue_id def end_dialogue ( self , dialogue_id : str ): \"\"\"End a dialogue. Args: dialogue_id: ID of the dialogue. \"\"\" del self . dialogues [ dialogue_id ] def generate_utterance ( self , dialogue_id : str , speaker_id : str ) -> str : \"\"\"Generate an utterance for the given dialogue. Args: dialogue_id: ID of the dialogue. speaker: 0 for the first character, 1 for the second character. \"\"\" context = self . _build_context ( dialogue_id , speaker_id ) return self . text_generation_service . generate_utterance ( context ) def check_scripted_utterances ( self , utterance : str , scripted_utterances : List [ str ], threshold : float ) -> int : \"\"\"Check if the given utterance is one of the scripted utterances. Args: utterance: Natural language utterance. scripted_utterances: Natural language utterances. threshold: [0,1] threshold for the similarity between the utterance and the scripted utterances. Returns: id of the utterance, None if the utterance is not one of the scripted utterances. \"\"\" scores = self . similarity_api . check_similarity ( utterance , scripted_utterances , threshold ) scores = [ score if score > threshold else 0 for score in scores ] if max ( scores ) > threshold : return scores . index ( max ( scores )) return None def update_dialogue ( self , dialogue_id : str , speaker_id : str , utterance : str ): \"\"\"Update dialogue state. Args: dialogue_id: ID of the dialogue. speaker_id: 0 for the first character, 1 for the second character. utterance: Natural language utterance. \"\"\" self . dialogues [ dialogue_id ][ \"history\" ] . append ( { \"speaker\" : speaker_id , \"line\" : utterance } ) def get_history ( self , dialogue_id : str ) -> List [ Dict [ str , Any ]]: \"\"\"Get the history of a dialogue. Args: dialogue_id: ID of the dialogue. \"\"\" return self . dialogues [ dialogue_id ][ \"history\" ] def _build_context ( self , dialogue_id : str , speaker_id : str ) -> Dict [ str , Any ]: \"\"\"Build the context for the given dialogue. Args: dialogue_id: ID of the dialogue. \"\"\" speaker = [ c [ \"name\" ] for c in self . dialogues [ dialogue_id ][ \"characters\" ]] . index ( speaker_id ) other_speaker = speaker ^ 1 context = copy ( self . context_template ) context [ \"history\" ] = self . dialogues [ dialogue_id ][ \"history\" ] context [ \"location\" ] = self . dialogues [ dialogue_id ][ \"location\" ][ \"description\" ] context [ \"location_name\" ] = self . dialogues [ dialogue_id ][ \"location\" ][ \"name\" ] context [ \"name\" ] = self . dialogues [ dialogue_id ][ \"characters\" ][ speaker ][ \"name\" ] context [ \"persona\" ] = self . dialogues [ dialogue_id ][ \"characters\" ][ speaker ][ \"persona\" ] context [ \"other_name\" ] = self . dialogues [ dialogue_id ][ \"characters\" ][ other_speaker ][ \"name\" ] context [ \"other_persona\" ] = self . dialogues [ dialogue_id ][ \"characters\" ][ other_speaker ][ \"persona\" ] return context def _validate_context_template ( self , context_template : Dict [ str , Any ]): \"\"\"Validate the context template. Args: context_template: Context template. \"\"\" if \"persona\" not in context_template : raise ValueError ( \"Context template must contain a persona.\" ) if \"name\" not in context_template : raise ValueError ( \"Context template must contain a name.\" ) if \"location\" not in context_template : raise ValueError ( \"Context template must contain a location.\" ) if \"location_name\" not in context_template : raise ValueError ( \"Context template must contain a location name.\" ) if \"other_name\" not in context_template : raise ValueError ( \"Context template must contain a other_name field.\" ) if \"other_persona\" not in context_template : raise ValueError ( \"Context template must contain a other_persona field.\" ) if \"history\" not in context_template : raise ValueError ( \"Context template must contain a history field.\" ) __init__ ( text_generation_svc = 'TextGenerationAPI' , similarity_svc = 'SimilarityAPI' , * args , ** kwargs ) Initialize the persona dialogue API. Parameters: Name Type Description Default text_generation_service Name of the text generation service. required similarity_api Name of the similarity API. required similarity_threshold Threshold for semantic similarity. required Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def __init__ ( self , text_generation_svc : str = \"TextGenerationAPI\" , similarity_svc : str = \"SimilarityAPI\" , * args , ** kwargs , ): \"\"\"Initialize the persona dialogue API. Args: text_generation_service: Name of the text generation service. similarity_api: Name of the similarity API. similarity_threshold: Threshold for semantic similarity. \"\"\" super () . __init__ ( * args , ** kwargs ) self . text_generation_service = self . create_client ( text_generation_svc ) self . similarity_api = self . create_client ( similarity_svc ) self . dialogues = {} self . dialogue_id_counter = 0 self . context_template = self . text_generation_service . get_context_template () self . _validate_context_template ( self . context_template ) _build_context ( dialogue_id , speaker_id ) Build the context for the given dialogue. Parameters: Name Type Description Default dialogue_id str ID of the dialogue. required Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def _build_context ( self , dialogue_id : str , speaker_id : str ) -> Dict [ str , Any ]: \"\"\"Build the context for the given dialogue. Args: dialogue_id: ID of the dialogue. \"\"\" speaker = [ c [ \"name\" ] for c in self . dialogues [ dialogue_id ][ \"characters\" ]] . index ( speaker_id ) other_speaker = speaker ^ 1 context = copy ( self . context_template ) context [ \"history\" ] = self . dialogues [ dialogue_id ][ \"history\" ] context [ \"location\" ] = self . dialogues [ dialogue_id ][ \"location\" ][ \"description\" ] context [ \"location_name\" ] = self . dialogues [ dialogue_id ][ \"location\" ][ \"name\" ] context [ \"name\" ] = self . dialogues [ dialogue_id ][ \"characters\" ][ speaker ][ \"name\" ] context [ \"persona\" ] = self . dialogues [ dialogue_id ][ \"characters\" ][ speaker ][ \"persona\" ] context [ \"other_name\" ] = self . dialogues [ dialogue_id ][ \"characters\" ][ other_speaker ][ \"name\" ] context [ \"other_persona\" ] = self . dialogues [ dialogue_id ][ \"characters\" ][ other_speaker ][ \"persona\" ] return context _validate_context_template ( context_template ) Validate the context template. Parameters: Name Type Description Default context_template Dict [ str , Any ] Context template. required Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def _validate_context_template ( self , context_template : Dict [ str , Any ]): \"\"\"Validate the context template. Args: context_template: Context template. \"\"\" if \"persona\" not in context_template : raise ValueError ( \"Context template must contain a persona.\" ) if \"name\" not in context_template : raise ValueError ( \"Context template must contain a name.\" ) if \"location\" not in context_template : raise ValueError ( \"Context template must contain a location.\" ) if \"location_name\" not in context_template : raise ValueError ( \"Context template must contain a location name.\" ) if \"other_name\" not in context_template : raise ValueError ( \"Context template must contain a other_name field.\" ) if \"other_persona\" not in context_template : raise ValueError ( \"Context template must contain a other_persona field.\" ) if \"history\" not in context_template : raise ValueError ( \"Context template must contain a history field.\" ) check_scripted_utterances ( utterance , scripted_utterances , threshold ) Check if the given utterance is one of the scripted utterances. Parameters: Name Type Description Default utterance str Natural language utterance. required scripted_utterances List [ str ] Natural language utterances. required threshold float [0,1] threshold for the similarity between the utterance and the scripted utterances. required Returns: Type Description int id of the utterance, None if the utterance is not one of the scripted utterances. Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def check_scripted_utterances ( self , utterance : str , scripted_utterances : List [ str ], threshold : float ) -> int : \"\"\"Check if the given utterance is one of the scripted utterances. Args: utterance: Natural language utterance. scripted_utterances: Natural language utterances. threshold: [0,1] threshold for the similarity between the utterance and the scripted utterances. Returns: id of the utterance, None if the utterance is not one of the scripted utterances. \"\"\" scores = self . similarity_api . check_similarity ( utterance , scripted_utterances , threshold ) scores = [ score if score > threshold else 0 for score in scores ] if max ( scores ) > threshold : return scores . index ( max ( scores )) return None end_dialogue ( dialogue_id ) End a dialogue. Parameters: Name Type Description Default dialogue_id str ID of the dialogue. required Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue.py 81 82 83 84 85 86 87 def end_dialogue ( self , dialogue_id : str ): \"\"\"End a dialogue. Args: dialogue_id: ID of the dialogue. \"\"\" del self . dialogues [ dialogue_id ] generate_utterance ( dialogue_id , speaker_id ) Generate an utterance for the given dialogue. Parameters: Name Type Description Default dialogue_id str ID of the dialogue. required speaker 0 for the first character, 1 for the second character. required Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue.py 89 90 91 92 93 94 95 96 97 def generate_utterance ( self , dialogue_id : str , speaker_id : str ) -> str : \"\"\"Generate an utterance for the given dialogue. Args: dialogue_id: ID of the dialogue. speaker: 0 for the first character, 1 for the second character. \"\"\" context = self . _build_context ( dialogue_id , speaker_id ) return self . text_generation_service . generate_utterance ( context ) get_history ( dialogue_id ) Get the history of a dialogue. Parameters: Name Type Description Default dialogue_id str ID of the dialogue. required Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue.py 132 133 134 135 136 137 138 def get_history ( self , dialogue_id : str ) -> List [ Dict [ str , Any ]]: \"\"\"Get the history of a dialogue. Args: dialogue_id: ID of the dialogue. \"\"\" return self . dialogues [ dialogue_id ][ \"history\" ] start_dialogue ( name1 = None , persona1 = None , name2 = None , persona2 = None , location_name = None , location_description = None , dialogue_id = None , * args , ** kwargs ) Start a dialogue between two characters. Parameters: Name Type Description Default dialogue_id str ID of the dialogue. If None it will be named automatically. None name1 str Name of the first character. None persona1 str Persona of the first character. None name2 str Name of the second character. None persona2 str Persona of the second character. None location_name str Name of the place where dialogue happens. None location_description str Description of the place where dialogue happens. None Returns: Type Description str Dialogue id. Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def start_dialogue ( self , name1 : str = None , persona1 : str = None , name2 : str = None , persona2 : str = None , location_name : str = None , location_description : str = None , dialogue_id : str = None , * args , ** kwargs , ) -> str : \"\"\"Start a dialogue between two characters. Args: dialogue_id: ID of the dialogue. If None it will be named automatically. name1: Name of the first character. persona1: Persona of the first character. name2: Name of the second character. persona2: Persona of the second character. location_name: Name of the place where dialogue happens. location_description: Description of the place where dialogue happens. Returns: Dialogue id. \"\"\" if dialogue_id in self . dialogues : raise ValueError ( \"Dialogue already exists.\" ) if dialogue_id is None : dialogue_id = f \"dialogue_ { self . dialogue_id_counter } \" self . dialogue_id_counter += 1 while dialogue_id in self . dialogues : dialogue_id = f \"dialogue_ { self . dialogue_id_counter } \" self . dialogue_id_counter += 1 self . dialogues [ dialogue_id ] = { \"characters\" : [ { \"name\" : name1 , \"persona\" : persona1 }, { \"name\" : name2 , \"persona\" : persona2 }, ], \"location\" : { \"name\" : location_name , \"description\" : location_description }, \"history\" : [], \"other\" : kwargs . get ( \"other\" , {}), } return dialogue_id update_dialogue ( dialogue_id , speaker_id , utterance ) Update dialogue state. Parameters: Name Type Description Default dialogue_id str ID of the dialogue. required speaker_id str 0 for the first character, 1 for the second character. required utterance str Natural language utterance. required Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue.py 120 121 122 123 124 125 126 127 128 129 130 def update_dialogue ( self , dialogue_id : str , speaker_id : str , utterance : str ): \"\"\"Update dialogue state. Args: dialogue_id: ID of the dialogue. speaker_id: 0 for the first character, 1 for the second character. utterance: Natural language utterance. \"\"\" self . dialogues [ dialogue_id ][ \"history\" ] . append ( { \"speaker\" : speaker_id , \"line\" : utterance } )","title":"PersonaDialogue"},{"location":"inference_engine/reference/#npc_engine.services.persona_dialogue.persona_dialogue_base","text":"Module that implements persona dialogue API.","title":"persona_dialogue_base"},{"location":"inference_engine/reference/#npc_engine.services.persona_dialogue.persona_dialogue_base.PersonaDialogueAPI","text":"Bases: BaseService Abstract base class for persona dialogue models. Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue_base.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 class PersonaDialogueAPI ( BaseService ): \"\"\"Abstract base class for persona dialogue models.\"\"\" API_METHODS : List [ str ] = [ \"start_dialogue\" , \"step_dialogue\" , \"get_history\" ] def __init__ ( self , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"PersonaDialogueAPI\" @abstractmethod def start_dialogue ( self , name1 : str = None , persona1 : str = None , name2 : str = None , persona2 : str = None , location_name : str = None , location_description : str = None , items_of_interest : List [ str ] = None , dialogue_id : str = None , other : Dict [ str , Any ] = None , ) -> str : \"\"\"Start a dialogue between two characters. All arguments are supposed to be natural language descriptions. Args: name1: Name of the first character. persona1: Persona of the first character. name2: Name of the second character. persona2: Persona of the second character. location_name: Name of the place where dialogue happens. location_description: Description of the place where dialogue happens. items_of_interest: List of items of interest that could be mentioned in the dialogue. dialogue_id: ID of the dialogue. If None it will be named automatically. other: Other information that could be used to start the dialogue. Returns: Dialogue id. \"\"\" pass @abstractmethod def end_dialogue ( self , dialogue_id : str ): \"\"\"End a dialogue between two characters. Args: dialogue_id: ID of the dialogue. \"\"\" pass def step_dialogue ( self , dialogue_id : str , speaker_id : str , utterance : str = None , scripted_utterances : List [ str ] = None , scripted_threshold : float = 0.5 , update_history : bool = True , ) -> Tuple [ str , bool ]: \"\"\"Step a dialogue between two characters. Args: dialogue_id: ID of the dialogue. speaker: 0 for the first character, 1 for the second character. utterance: Natural language utterance. If None it will be generated. scripted_utterances: List of natural language utterances that will be matched against utterance. scripted_threshold: Threshold for matching scripted utterances. update_history: If True, the dialogue history will be updated. Returns: str: Next utterance. bool: scripted utterance triggered \"\"\" if utterance is None : utterance = self . generate_utterance ( dialogue_id , speaker_id ) scripted = False if scripted_utterances is not None : idx = self . check_scripted_utterances ( utterance , scripted_utterances , scripted_threshold ) if idx is not None : utterance = scripted_utterances [ idx ] scripted = True if update_history : self . update_dialogue ( dialogue_id , speaker_id , utterance ) return utterance , scripted @abstractmethod def generate_utterance ( self , dialogue_id : str , speaker_id : str ) -> str : \"\"\"Generate an utterance for the given speaker. Args: dialogue_id: ID of the dialogue. speaker: 0 for the first character, 1 for the second character. \"\"\" pass @abstractmethod def check_scripted_utterances ( self , utterance : str , scripted_utterances : List [ str ], threshold : float ) -> int : \"\"\"Check if the given utterance is one of the scripted utterances. Args: utterance: Natural language utterance. scripted_utterances: Natural language utterances. threshold: [0,1] threshold for the similarity between the utterance and the scripted utterances. Returns: id of the utterance, None if the utterance is not one of the scripted utterances. \"\"\" pass @abstractmethod def update_dialogue ( self , dialogue_id : str , speaker_id : str , utterance : str ): \"\"\"Update dialogue state. Args: dialogue_id: ID of the dialogue. speaker_id: 0 for the first character, 1 for the second character. utterance: Natural language utterance. \"\"\" pass @abstractmethod def get_history ( self , dialogue_id : str ) -> List [ Dict [ str , Any ]]: \"\"\"Get the history of a dialogue. Args: dialogue_id: ID of the dialogue. \"\"\" pass __init__ ( * args , ** kwargs ) Empty initialization method for API to be similar to other model base classes. Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue_base.py 13 14 15 16 def __init__ ( self , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True check_scripted_utterances ( utterance , scripted_utterances , threshold ) abstractmethod Check if the given utterance is one of the scripted utterances. Parameters: Name Type Description Default utterance str Natural language utterance. required scripted_utterances List [ str ] Natural language utterances. required threshold float [0,1] threshold for the similarity between the utterance and the scripted utterances. required Returns: Type Description int id of the utterance, None if the utterance is not one of the scripted utterances. Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue_base.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 @abstractmethod def check_scripted_utterances ( self , utterance : str , scripted_utterances : List [ str ], threshold : float ) -> int : \"\"\"Check if the given utterance is one of the scripted utterances. Args: utterance: Natural language utterance. scripted_utterances: Natural language utterances. threshold: [0,1] threshold for the similarity between the utterance and the scripted utterances. Returns: id of the utterance, None if the utterance is not one of the scripted utterances. \"\"\" pass end_dialogue ( dialogue_id ) abstractmethod End a dialogue between two characters. Parameters: Name Type Description Default dialogue_id str ID of the dialogue. required Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue_base.py 55 56 57 58 59 60 61 62 @abstractmethod def end_dialogue ( self , dialogue_id : str ): \"\"\"End a dialogue between two characters. Args: dialogue_id: ID of the dialogue. \"\"\" pass generate_utterance ( dialogue_id , speaker_id ) abstractmethod Generate an utterance for the given speaker. Parameters: Name Type Description Default dialogue_id str ID of the dialogue. required speaker 0 for the first character, 1 for the second character. required Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue_base.py 101 102 103 104 105 106 107 108 109 @abstractmethod def generate_utterance ( self , dialogue_id : str , speaker_id : str ) -> str : \"\"\"Generate an utterance for the given speaker. Args: dialogue_id: ID of the dialogue. speaker: 0 for the first character, 1 for the second character. \"\"\" pass get_api_name () classmethod Get the API name. Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue_base.py 18 19 20 21 @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"PersonaDialogueAPI\" get_history ( dialogue_id ) abstractmethod Get the history of a dialogue. Parameters: Name Type Description Default dialogue_id str ID of the dialogue. required Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue_base.py 138 139 140 141 142 143 144 145 @abstractmethod def get_history ( self , dialogue_id : str ) -> List [ Dict [ str , Any ]]: \"\"\"Get the history of a dialogue. Args: dialogue_id: ID of the dialogue. \"\"\" pass start_dialogue ( name1 = None , persona1 = None , name2 = None , persona2 = None , location_name = None , location_description = None , items_of_interest = None , dialogue_id = None , other = None ) abstractmethod Start a dialogue between two characters. All arguments are supposed to be natural language descriptions. Parameters: Name Type Description Default name1 str Name of the first character. None persona1 str Persona of the first character. None name2 str Name of the second character. None persona2 str Persona of the second character. None location_name str Name of the place where dialogue happens. None location_description str Description of the place where dialogue happens. None items_of_interest List [ str ] List of items of interest that could be mentioned in the dialogue. None dialogue_id str ID of the dialogue. If None it will be named automatically. None other Dict [ str , Any ] Other information that could be used to start the dialogue. None Returns: Type Description str Dialogue id. Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue_base.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 @abstractmethod def start_dialogue ( self , name1 : str = None , persona1 : str = None , name2 : str = None , persona2 : str = None , location_name : str = None , location_description : str = None , items_of_interest : List [ str ] = None , dialogue_id : str = None , other : Dict [ str , Any ] = None , ) -> str : \"\"\"Start a dialogue between two characters. All arguments are supposed to be natural language descriptions. Args: name1: Name of the first character. persona1: Persona of the first character. name2: Name of the second character. persona2: Persona of the second character. location_name: Name of the place where dialogue happens. location_description: Description of the place where dialogue happens. items_of_interest: List of items of interest that could be mentioned in the dialogue. dialogue_id: ID of the dialogue. If None it will be named automatically. other: Other information that could be used to start the dialogue. Returns: Dialogue id. \"\"\" pass step_dialogue ( dialogue_id , speaker_id , utterance = None , scripted_utterances = None , scripted_threshold = 0.5 , update_history = True ) Step a dialogue between two characters. Parameters: Name Type Description Default dialogue_id str ID of the dialogue. required speaker 0 for the first character, 1 for the second character. required utterance str Natural language utterance. If None it will be generated. None scripted_utterances List [ str ] List of natural language utterances that will be matched against utterance. None scripted_threshold float Threshold for matching scripted utterances. 0.5 update_history bool If True, the dialogue history will be updated. True Returns: Name Type Description str str Next utterance. bool bool scripted utterance triggered Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue_base.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def step_dialogue ( self , dialogue_id : str , speaker_id : str , utterance : str = None , scripted_utterances : List [ str ] = None , scripted_threshold : float = 0.5 , update_history : bool = True , ) -> Tuple [ str , bool ]: \"\"\"Step a dialogue between two characters. Args: dialogue_id: ID of the dialogue. speaker: 0 for the first character, 1 for the second character. utterance: Natural language utterance. If None it will be generated. scripted_utterances: List of natural language utterances that will be matched against utterance. scripted_threshold: Threshold for matching scripted utterances. update_history: If True, the dialogue history will be updated. Returns: str: Next utterance. bool: scripted utterance triggered \"\"\" if utterance is None : utterance = self . generate_utterance ( dialogue_id , speaker_id ) scripted = False if scripted_utterances is not None : idx = self . check_scripted_utterances ( utterance , scripted_utterances , scripted_threshold ) if idx is not None : utterance = scripted_utterances [ idx ] scripted = True if update_history : self . update_dialogue ( dialogue_id , speaker_id , utterance ) return utterance , scripted update_dialogue ( dialogue_id , speaker_id , utterance ) abstractmethod Update dialogue state. Parameters: Name Type Description Default dialogue_id str ID of the dialogue. required speaker_id str 0 for the first character, 1 for the second character. required utterance str Natural language utterance. required Source code in npc_engine\\services\\persona_dialogue\\persona_dialogue_base.py 127 128 129 130 131 132 133 134 135 136 @abstractmethod def update_dialogue ( self , dialogue_id : str , speaker_id : str , utterance : str ): \"\"\"Update dialogue state. Args: dialogue_id: ID of the dialogue. speaker_id: 0 for the first character, 1 for the second character. utterance: Natural language utterance. \"\"\" pass","title":"PersonaDialogueAPI"},{"location":"inference_engine/reference/#npc_engine.services.sequence_classifier","text":"Sequence classification API module.","title":"sequence_classifier"},{"location":"inference_engine/reference/#npc_engine.services.sequence_classifier.hf_classifier","text":"Module that implements Huggingface transformers classification.","title":"hf_classifier"},{"location":"inference_engine/reference/#npc_engine.services.sequence_classifier.hf_classifier.HfClassifier","text":"Bases: SequenceClassifierAPI Huggingface transformers sequence classification. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with sequence-classification feature. Also requires saved tokenizer with huggingface tokenizers. Source code in npc_engine\\services\\sequence_classifier\\hf_classifier.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class HfClassifier ( SequenceClassifierAPI ): \"\"\"Huggingface transformers sequence classification. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with sequence-classification feature. Also requires saved tokenizer with huggingface tokenizers. \"\"\" def __init__ ( self , model_path : str , * args , ** kwargs ): \"\"\"Create and load biencoder model for semantic similarity. Args: model_path: A path where model config and weights are. \"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_ALL self . model = rt . InferenceSession ( os . path . join ( model_path , \"model.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) input_names = [ inp . name for inp in self . model . get_inputs ()] self . token_type_support = \"token_type_ids\" in input_names special_tokens_map_path = os . path . join ( model_path , \"special_tokens_map.json\" ) with open ( special_tokens_map_path , \"r\" ) as f : self . special_tokens = json . load ( f ) self . pad_token_id = self . tokenizer . encode ( self . special_tokens [ \"pad_token\" ]) . ids [ 0 ] self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) self . tokenizer . enable_padding ( direction = \"right\" , pad_id = self . pad_token_id , pad_type_id = 0 , pad_token = self . tokenizer . decode ( [ self . pad_token_id ], skip_special_tokens = False ), length = None , pad_to_multiple_of = None , ) self . tests = {} def compute_scores_batch ( self , texts : List [ Union [ str , Tuple [ str , str ]]] ) -> np . ndarray : \"\"\"Compute scores. Args: texts: Sentence to embed Returns: scores: Scores for each text \"\"\" tokenized = self . tokenizer . encode_batch ( texts ) ids = np . stack ([ np . asarray ( encoding . ids ) for encoding in tokenized ]) . astype ( np . int64 ) attention_mask = np . stack ( [ np . asarray ( encoding . attention_mask ) for encoding in tokenized ] ) . astype ( np . int64 ) if not self . token_type_support : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask } else : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : np . stack ( [ np . asarray ( encoding . type_ids ) for encoding in tokenized ] ) . astype ( np . int64 ), } outp = self . model . run ( None , input_dict ) return outp [ 0 ] __init__ ( model_path , * args , ** kwargs ) Create and load biencoder model for semantic similarity. Parameters: Name Type Description Default model_path str A path where model config and weights are. required Source code in npc_engine\\services\\sequence_classifier\\hf_classifier.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , model_path : str , * args , ** kwargs ): \"\"\"Create and load biencoder model for semantic similarity. Args: model_path: A path where model config and weights are. \"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_ALL self . model = rt . InferenceSession ( os . path . join ( model_path , \"model.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) input_names = [ inp . name for inp in self . model . get_inputs ()] self . token_type_support = \"token_type_ids\" in input_names special_tokens_map_path = os . path . join ( model_path , \"special_tokens_map.json\" ) with open ( special_tokens_map_path , \"r\" ) as f : self . special_tokens = json . load ( f ) self . pad_token_id = self . tokenizer . encode ( self . special_tokens [ \"pad_token\" ]) . ids [ 0 ] self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) self . tokenizer . enable_padding ( direction = \"right\" , pad_id = self . pad_token_id , pad_type_id = 0 , pad_token = self . tokenizer . decode ( [ self . pad_token_id ], skip_special_tokens = False ), length = None , pad_to_multiple_of = None , ) self . tests = {} compute_scores_batch ( texts ) Compute scores. Parameters: Name Type Description Default texts List [ Union [ str , Tuple [ str , str ]]] Sentence to embed required Returns: Name Type Description scores np . ndarray Scores for each text Source code in npc_engine\\services\\sequence_classifier\\hf_classifier.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def compute_scores_batch ( self , texts : List [ Union [ str , Tuple [ str , str ]]] ) -> np . ndarray : \"\"\"Compute scores. Args: texts: Sentence to embed Returns: scores: Scores for each text \"\"\" tokenized = self . tokenizer . encode_batch ( texts ) ids = np . stack ([ np . asarray ( encoding . ids ) for encoding in tokenized ]) . astype ( np . int64 ) attention_mask = np . stack ( [ np . asarray ( encoding . attention_mask ) for encoding in tokenized ] ) . astype ( np . int64 ) if not self . token_type_support : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask } else : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : np . stack ( [ np . asarray ( encoding . type_ids ) for encoding in tokenized ] ) . astype ( np . int64 ), } outp = self . model . run ( None , input_dict ) return outp [ 0 ]","title":"HfClassifier"},{"location":"inference_engine/reference/#npc_engine.services.sequence_classifier.sequence_classifier_base","text":"Module that implements sequence classification API.","title":"sequence_classifier_base"},{"location":"inference_engine/reference/#npc_engine.services.sequence_classifier.sequence_classifier_base.SequenceClassifierAPI","text":"Bases: BaseService Abstract base class for text classification models. Source code in npc_engine\\services\\sequence_classifier\\sequence_classifier_base.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class SequenceClassifierAPI ( BaseService ): \"\"\"Abstract base class for text classification models.\"\"\" API_METHODS : List [ str ] = [ \"classify\" ] def __init__ ( self , cache_size = 0 , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True self . cache = NumpyLRUCache ( cache_size ) @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"SequenceClassifierAPI\" def classify ( self , texts : List [ str ]) -> List [ List [ float ]]: \"\"\"Classify a list of texts. Args: texts: A list of texts to classify. Returns: List of scores for each text. \"\"\" texts = [ row if isinstance ( row , str ) else tuple ( row ) for row in texts ] scores = self . cache . cache_compute ( texts , lambda values : self . compute_scores_batch ( values ) ) return scores . tolist () @abstractmethod def compute_scores_batch ( self , texts : List [ str ]) -> np . ndarray : \"\"\"Compute scores for a list of texts. Args: texts: A list of texts to compute scores for. Returns: List of scores for each text. \"\"\" pass __init__ ( cache_size = 0 , * args , ** kwargs ) Empty initialization method for API to be similar to other model base classes. Source code in npc_engine\\services\\sequence_classifier\\sequence_classifier_base.py 15 16 17 18 19 def __init__ ( self , cache_size = 0 , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True self . cache = NumpyLRUCache ( cache_size ) classify ( texts ) Classify a list of texts. Parameters: Name Type Description Default texts List [ str ] A list of texts to classify. required Returns: Type Description List [ List [ float ]] List of scores for each text. Source code in npc_engine\\services\\sequence_classifier\\sequence_classifier_base.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def classify ( self , texts : List [ str ]) -> List [ List [ float ]]: \"\"\"Classify a list of texts. Args: texts: A list of texts to classify. Returns: List of scores for each text. \"\"\" texts = [ row if isinstance ( row , str ) else tuple ( row ) for row in texts ] scores = self . cache . cache_compute ( texts , lambda values : self . compute_scores_batch ( values ) ) return scores . tolist () compute_scores_batch ( texts ) abstractmethod Compute scores for a list of texts. Parameters: Name Type Description Default texts List [ str ] A list of texts to compute scores for. required Returns: Type Description np . ndarray List of scores for each text. Source code in npc_engine\\services\\sequence_classifier\\sequence_classifier_base.py 41 42 43 44 45 46 47 48 49 50 51 @abstractmethod def compute_scores_batch ( self , texts : List [ str ]) -> np . ndarray : \"\"\"Compute scores for a list of texts. Args: texts: A list of texts to compute scores for. Returns: List of scores for each text. \"\"\" pass get_api_name () classmethod Get the API name. Source code in npc_engine\\services\\sequence_classifier\\sequence_classifier_base.py 21 22 23 24 @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"SequenceClassifierAPI\"","title":"SequenceClassifierAPI"},{"location":"inference_engine/reference/#npc_engine.services.similarity","text":"Similarity model implementations. This module implements specific models and wraps them under the common interface for loading and inference. Example from npc_engine.models.similarity import SimilarityAPI model = SimilarityAPI.load(\"path/to/model_dir\") model.compare(\"hello\", [\"Hello, world!\"])","title":"similarity"},{"location":"inference_engine/reference/#npc_engine.services.similarity.similarity_base","text":"Module that implements semantic similarity model API.","title":"similarity_base"},{"location":"inference_engine/reference/#npc_engine.services.similarity.similarity_base.SimilarityAPI","text":"Bases: BaseService Abstract base class for text similarity models. Source code in npc_engine\\services\\similarity\\similarity_base.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class SimilarityAPI ( BaseService ): \"\"\"Abstract base class for text similarity models.\"\"\" API_METHODS : List [ str ] = [ \"compare\" , \"cache\" ] def __init__ ( self , cache_size = 0 , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True self . lru_cache = NumpyLRUCache ( cache_size ) @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"SimilarityAPI\" def compare ( self , query : str , context : List [ str ]) -> List [ float ]: \"\"\"Compare a query to the context. Args: query: A sentence to compare. context: A list of sentences to compare to. This will be cached if caching is enabled Returns: List of similarities \"\"\" embedding_a = self . compute_embedding ( query ) embedding_b = self . lru_cache . cache_compute ( context , lambda values : self . compute_embedding_batch ( values ) ) similarities = self . metric ( embedding_a , embedding_b ) return similarities . tolist () def cache ( self , context : List [ str ]): \"\"\"Cache embeddings of given sequences. Args: context: A list of sentences to cache. \"\"\" self . lru_cache . cache_compute ( context , lambda values : self . compute_embedding_batch ( values ) ) @abstractmethod def compute_embedding_batch ( self , lines : List [ str ]) -> np . ndarray : \"\"\"Compute line embeddings in batch. Args: lines: List of sentences to embed Returns: Embedding batch of shape (batch_size, embedding_size) \"\"\" return None @abstractmethod def compute_embedding ( self , line : str ) -> np . ndarray : \"\"\"Compute sentence embedding. Args: line: Sentence to embed Returns: Embedding of shape (1, embedding_size) \"\"\" return None @abstractmethod def metric ( self , embedding_a : np . ndarray , embedding_b : np . ndarray ) -> np . ndarray : \"\"\"Compute distance between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Args: embedding_a: Embedding of shape (1 or batch_size, embedding_size) embedding_b: Embedding of shape (1 or batch_size, embedding_size) Returns: Vector of distances (batch_size or 1,) \"\"\" return None __init__ ( cache_size = 0 , * args , ** kwargs ) Empty initialization method for API to be similar to other model base classes. Source code in npc_engine\\services\\similarity\\similarity_base.py 15 16 17 18 19 def __init__ ( self , cache_size = 0 , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True self . lru_cache = NumpyLRUCache ( cache_size ) cache ( context ) Cache embeddings of given sequences. Parameters: Name Type Description Default context List [ str ] A list of sentences to cache. required Source code in npc_engine\\services\\similarity\\similarity_base.py 43 44 45 46 47 48 49 50 51 def cache ( self , context : List [ str ]): \"\"\"Cache embeddings of given sequences. Args: context: A list of sentences to cache. \"\"\" self . lru_cache . cache_compute ( context , lambda values : self . compute_embedding_batch ( values ) ) compare ( query , context ) Compare a query to the context. Parameters: Name Type Description Default query str A sentence to compare. required context List [ str ] A list of sentences to compare to. This will be cached if caching is enabled required Returns: Type Description List [ float ] List of similarities Source code in npc_engine\\services\\similarity\\similarity_base.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def compare ( self , query : str , context : List [ str ]) -> List [ float ]: \"\"\"Compare a query to the context. Args: query: A sentence to compare. context: A list of sentences to compare to. This will be cached if caching is enabled Returns: List of similarities \"\"\" embedding_a = self . compute_embedding ( query ) embedding_b = self . lru_cache . cache_compute ( context , lambda values : self . compute_embedding_batch ( values ) ) similarities = self . metric ( embedding_a , embedding_b ) return similarities . tolist () compute_embedding ( line ) abstractmethod Compute sentence embedding. Parameters: Name Type Description Default line str Sentence to embed required Returns: Type Description np . ndarray Embedding of shape (1, embedding_size) Source code in npc_engine\\services\\similarity\\similarity_base.py 65 66 67 68 69 70 71 72 73 74 75 @abstractmethod def compute_embedding ( self , line : str ) -> np . ndarray : \"\"\"Compute sentence embedding. Args: line: Sentence to embed Returns: Embedding of shape (1, embedding_size) \"\"\" return None compute_embedding_batch ( lines ) abstractmethod Compute line embeddings in batch. Parameters: Name Type Description Default lines List [ str ] List of sentences to embed required Returns: Type Description np . ndarray Embedding batch of shape (batch_size, embedding_size) Source code in npc_engine\\services\\similarity\\similarity_base.py 53 54 55 56 57 58 59 60 61 62 63 @abstractmethod def compute_embedding_batch ( self , lines : List [ str ]) -> np . ndarray : \"\"\"Compute line embeddings in batch. Args: lines: List of sentences to embed Returns: Embedding batch of shape (batch_size, embedding_size) \"\"\" return None get_api_name () classmethod Get the API name. Source code in npc_engine\\services\\similarity\\similarity_base.py 21 22 23 24 @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"SimilarityAPI\" metric ( embedding_a , embedding_b ) abstractmethod Compute distance between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Parameters: Name Type Description Default embedding_a np . ndarray Embedding of shape (1 or batch_size, embedding_size) required embedding_b np . ndarray Embedding of shape (1 or batch_size, embedding_size) required Returns: Type Description np . ndarray Vector of distances (batch_size or 1,) Source code in npc_engine\\services\\similarity\\similarity_base.py 77 78 79 80 81 82 83 84 85 86 87 88 89 @abstractmethod def metric ( self , embedding_a : np . ndarray , embedding_b : np . ndarray ) -> np . ndarray : \"\"\"Compute distance between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Args: embedding_a: Embedding of shape (1 or batch_size, embedding_size) embedding_b: Embedding of shape (1 or batch_size, embedding_size) Returns: Vector of distances (batch_size or 1,) \"\"\" return None","title":"SimilarityAPI"},{"location":"inference_engine/reference/#npc_engine.services.similarity.similarity_transformers","text":"Module that implements Huggingface transformers semantic similarity.","title":"similarity_transformers"},{"location":"inference_engine/reference/#npc_engine.services.similarity.similarity_transformers.TransformerSemanticSimilarity","text":"Bases: SimilarityAPI Huggingface transformers semantic similarity. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with biencoder architecture. Also requires a tokenizer.json with huggingface tokenizers definition. model.onnx spec: - inputs: `input_ids` of shape `(batch_size, sequence)` `attention_mask` of shape `(batch_size, sequence)` (Optional) `input_type_ids` of shape `(batch_size, sequence)` - outputs: `token_embeddings` of shape `(batch_size, sequence, hidden_size)` Source code in npc_engine\\services\\similarity\\similarity_transformers.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 class TransformerSemanticSimilarity ( SimilarityAPI ): \"\"\"Huggingface transformers semantic similarity. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with biencoder architecture. Also requires a tokenizer.json with huggingface tokenizers definition. model.onnx spec: - inputs: `input_ids` of shape `(batch_size, sequence)` `attention_mask` of shape `(batch_size, sequence)` (Optional) `input_type_ids` of shape `(batch_size, sequence)` - outputs: `token_embeddings` of shape `(batch_size, sequence, hidden_size)` \"\"\" def __init__ ( self , model_path : str , metric : str = \"dot\" , * args , ** kwargs ): \"\"\"Create and load biencoder model for semantic similarity. Args: model_path: A path where model config and weights are metric: distance to compute semantic similarity \"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_ALL self . model = rt . InferenceSession ( os . path . join ( model_path , \"model.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) input_names = [ inp . name for inp in self . model . get_inputs ()] self . token_type_support = \"token_type_ids\" in input_names special_tokens_map_path = os . path . join ( model_path , \"special_tokens_map.json\" ) with open ( special_tokens_map_path , \"r\" ) as f : self . special_tokens = json . load ( f ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) self . pad_token_id = self . tokenizer . encode ( self . special_tokens [ \"pad_token\" ]) . ids [ 0 ] self . tokenizer . enable_padding ( direction = \"right\" , pad_id = self . pad_token_id , pad_type_id = 0 , pad_token = self . tokenizer . decode ( [ self . pad_token_id ], skip_special_tokens = False ), length = None , pad_to_multiple_of = None , ) self . tests = {} self . metric_type = metric def compute_embedding ( self , line : str ) -> np . ndarray : \"\"\"Compute sentence embedding. Args: line: Sentence to embed Returns: Embedding of shape (1, embedding_size) \"\"\" ids = ( np . asarray ( self . tokenizer . encode ( line ) . ids ) . reshape ([ 1 , - 1 ]) . astype ( np . int64 ) ) attention_mask = np . ones_like ( ids ) if not self . token_type_support : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask } else : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : np . zeros_like ( ids ), } outp = self . model . run ( None , input_dict ) return self . _mean_pooling ( outp , attention_mask ) def compute_embedding_batch ( self , lines : List [ str ]) -> np . ndarray : \"\"\"Compute line embeddings in batch. Args: lines: List of sentences to embed Returns: Embedding batch of shape (batch_size, embedding_size) \"\"\" tokenized = self . tokenizer . encode_batch ( lines ) ids = np . stack ([ np . asarray ( encoding . ids ) for encoding in tokenized ]) . astype ( np . int64 ) attention_mask = np . stack ( [ np . asarray ( encoding . attention_mask ) for encoding in tokenized ] ) . astype ( np . int64 ) if not self . token_type_support : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask } else : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : np . stack ( [ np . asarray ( encoding . type_ids ) for encoding in tokenized ] ) . astype ( np . int64 ), } outp = self . model . run ( None , input_dict ) return self . _mean_pooling ( outp , attention_mask ) def _mean_pooling ( self , model_output , attention_mask ): token_embeddings = model_output [ 0 ] attention_mask = np . expand_dims ( attention_mask , - 1 ) sum_embeddings = np . sum ( token_embeddings * attention_mask , 1 ) sum_mask = np . clip ( attention_mask . sum ( 1 ), a_min = 1e-9 , a_max = None ) return sum_embeddings / sum_mask def metric ( self , embedding_a : np . ndarray , embedding_b : np . ndarray ) -> np . ndarray : \"\"\"Similarity between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Args: embedding_a: Embedding of shape (1 or batch_size, embedding_size) embedding_b: Embedding of shape (1 or batch_size, embedding_size) Returns: Vector of distances (batch_size or 1,) \"\"\" if self . metric_type == \"dot\" : return - np . dot ( embedding_a , embedding_b . T ) . squeeze ( 0 ) elif self . metric_type == \"cosine\" : return 1 - cdist ( embedding_a , embedding_b , metric = \"cosine\" ) . squeeze ( 0 ) __init__ ( model_path , metric = 'dot' , * args , ** kwargs ) Create and load biencoder model for semantic similarity. Parameters: Name Type Description Default model_path str A path where model config and weights are required metric str distance to compute semantic similarity 'dot' Source code in npc_engine\\services\\similarity\\similarity_transformers.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , model_path : str , metric : str = \"dot\" , * args , ** kwargs ): \"\"\"Create and load biencoder model for semantic similarity. Args: model_path: A path where model config and weights are metric: distance to compute semantic similarity \"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_ALL self . model = rt . InferenceSession ( os . path . join ( model_path , \"model.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) input_names = [ inp . name for inp in self . model . get_inputs ()] self . token_type_support = \"token_type_ids\" in input_names special_tokens_map_path = os . path . join ( model_path , \"special_tokens_map.json\" ) with open ( special_tokens_map_path , \"r\" ) as f : self . special_tokens = json . load ( f ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) self . pad_token_id = self . tokenizer . encode ( self . special_tokens [ \"pad_token\" ]) . ids [ 0 ] self . tokenizer . enable_padding ( direction = \"right\" , pad_id = self . pad_token_id , pad_type_id = 0 , pad_token = self . tokenizer . decode ( [ self . pad_token_id ], skip_special_tokens = False ), length = None , pad_to_multiple_of = None , ) self . tests = {} self . metric_type = metric compute_embedding ( line ) Compute sentence embedding. Parameters: Name Type Description Default line str Sentence to embed required Returns: Type Description np . ndarray Embedding of shape (1, embedding_size) Source code in npc_engine\\services\\similarity\\similarity_transformers.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def compute_embedding ( self , line : str ) -> np . ndarray : \"\"\"Compute sentence embedding. Args: line: Sentence to embed Returns: Embedding of shape (1, embedding_size) \"\"\" ids = ( np . asarray ( self . tokenizer . encode ( line ) . ids ) . reshape ([ 1 , - 1 ]) . astype ( np . int64 ) ) attention_mask = np . ones_like ( ids ) if not self . token_type_support : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask } else : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : np . zeros_like ( ids ), } outp = self . model . run ( None , input_dict ) return self . _mean_pooling ( outp , attention_mask ) compute_embedding_batch ( lines ) Compute line embeddings in batch. Parameters: Name Type Description Default lines List [ str ] List of sentences to embed required Returns: Type Description np . ndarray Embedding batch of shape (batch_size, embedding_size) Source code in npc_engine\\services\\similarity\\similarity_transformers.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def compute_embedding_batch ( self , lines : List [ str ]) -> np . ndarray : \"\"\"Compute line embeddings in batch. Args: lines: List of sentences to embed Returns: Embedding batch of shape (batch_size, embedding_size) \"\"\" tokenized = self . tokenizer . encode_batch ( lines ) ids = np . stack ([ np . asarray ( encoding . ids ) for encoding in tokenized ]) . astype ( np . int64 ) attention_mask = np . stack ( [ np . asarray ( encoding . attention_mask ) for encoding in tokenized ] ) . astype ( np . int64 ) if not self . token_type_support : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask } else : input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : np . stack ( [ np . asarray ( encoding . type_ids ) for encoding in tokenized ] ) . astype ( np . int64 ), } outp = self . model . run ( None , input_dict ) return self . _mean_pooling ( outp , attention_mask ) metric ( embedding_a , embedding_b ) Similarity between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Parameters: Name Type Description Default embedding_a np . ndarray Embedding of shape (1 or batch_size, embedding_size) required embedding_b np . ndarray Embedding of shape (1 or batch_size, embedding_size) required Returns: Type Description np . ndarray Vector of distances (batch_size or 1,) Source code in npc_engine\\services\\similarity\\similarity_transformers.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def metric ( self , embedding_a : np . ndarray , embedding_b : np . ndarray ) -> np . ndarray : \"\"\"Similarity between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Args: embedding_a: Embedding of shape (1 or batch_size, embedding_size) embedding_b: Embedding of shape (1 or batch_size, embedding_size) Returns: Vector of distances (batch_size or 1,) \"\"\" if self . metric_type == \"dot\" : return - np . dot ( embedding_a , embedding_b . T ) . squeeze ( 0 ) elif self . metric_type == \"cosine\" : return 1 - cdist ( embedding_a , embedding_b , metric = \"cosine\" ) . squeeze ( 0 )","title":"TransformerSemanticSimilarity"},{"location":"inference_engine/reference/#npc_engine.services.stt","text":"Speech to text API. This module implements specific models and wraps them under the common interface for loading and inference. Example from npc_engine.models.stt import SpeechToTextAPI model = SpeechToTextAPI.load(\"path/to/model_dir\") text = model.listen() # Say something","title":"stt"},{"location":"inference_engine/reference/#npc_engine.services.stt.nemo_stt","text":"Module that implements Huggingface transformers semantic similarity.","title":"nemo_stt"},{"location":"inference_engine/reference/#npc_engine.services.stt.nemo_stt.NemoSTT","text":"Bases: SpeechToTextAPI Text to speech pipeline based on Nemo toolkit. Uses ONNX export of EncDecCTCModel from Nemo toolkit. Punctuation distillbert model from Nemo toolkit. (requires tokenizer.json as well) Huggingface transformers model for predicting that sentence is finished (Cropped sentence -> 0 label, finished sentence -> 1 label). OpenSLR Librispeech 3-gram model converted to lowercase https://www.openslr.org/11/ References https://github.com/NVIDIA/NeMo https://catalog.ngc.nvidia.com/orgs/nvidia/models/quartznet15x5 https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/punctuation_and_capitalization.html ctc.onnx spec: - inputs: `audio_signal` mel spectogram of shape `(batch_size, 64, mel_sequence)` - outputs: `tokens` of shape `(batch_size, token_sequence, logits)` punctuation.onnx spec: - inputs: `input_ids` mel spectogram of shape `(batch_size, sequence)` `attention_mask` mel spectogram of shape `(batch_size, sequence)` - outputs: `punctuation` of shape `(batch_size, sequence, 4)` `capitalization` of shape `(batch_size, sequence, 2)` Source code in npc_engine\\services\\stt\\nemo_stt.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 class NemoSTT ( SpeechToTextAPI ): \"\"\"Text to speech pipeline based on Nemo toolkit. Uses: - ONNX export of EncDecCTCModel from Nemo toolkit. - Punctuation distillbert model from Nemo toolkit. (requires tokenizer.json as well) - Huggingface transformers model for predicting that sentence is finished (Cropped sentence -> 0 label, finished sentence -> 1 label). - OpenSLR Librispeech 3-gram model converted to lowercase https://www.openslr.org/11/ References: https://github.com/NVIDIA/NeMo https://catalog.ngc.nvidia.com/orgs/nvidia/models/quartznet15x5 https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/punctuation_and_capitalization.html ctc.onnx spec: - inputs: `audio_signal` mel spectogram of shape `(batch_size, 64, mel_sequence)` - outputs: `tokens` of shape `(batch_size, token_sequence, logits)` punctuation.onnx spec: - inputs: `input_ids` mel spectogram of shape `(batch_size, sequence)` `attention_mask` mel spectogram of shape `(batch_size, sequence)` - outputs: `punctuation` of shape `(batch_size, sequence, 4)` `capitalization` of shape `(batch_size, sequence, 2)` \"\"\" def __init__ ( self , model_path : str , frame_size : int = 1000 , sample_rate : int = 16000 , predict_punctuation : bool = False , * args , ** kwargs , ): \"\"\"Create and load biencoder model for semantic similarity. Args: model_path: A path where model config and weights are stored. frame_size: Size of the audio frame in milliseconds. sample_rate: Sample rate of the audio. predict_punctuation: Whether to predict punctuation and capitalization. \"\"\" super () . __init__ ( sample_rate = sample_rate , model_path = model_path , frame_size = frame_size , * args , ** kwargs , ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_BASIC provider = rt . get_available_providers ()[ 0 ] logger . info ( f \"STT uses { provider } provider\" ) self . stft_filterbanks = librosa . filters . mel ( sr = 16000 , n_fft = 512 , n_mels = 64 , fmin = 0 , fmax = 8000 ) self . stft_window = librosa . filters . get_window ( \"hann\" , 320 , fftbins = False ) self . mel_mean , self . mel_std = self . _fixed_normalization () self . asr_model = rt . InferenceSession ( path . join ( model_path , \"ctc.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . predict_punctuation = predict_punctuation if self . predict_punctuation : self . punctuation = rt . InferenceSession ( path . join ( model_path , \"punctuation.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( path . join ( model_path , \"tokenizer.json\" ) ) self . asr_vocab = \" abcdefghijklmnopqrstuvwxyz'\" self . asr_vocab = list ( self . asr_vocab ) self . asr_vocab . append ( \"\" ) self . punct_labels = \"O,.?\" self . capit_labels = \"OU\" self . decoder = build_ctcdecoder ( self . asr_vocab ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_ALL self . sentence_model = rt . InferenceSession ( path . join ( model_path , \"sentence_prediction.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . sentence_tokenizer = Tokenizer . from_file ( path . join ( model_path , \"sentence_tokenizer.json\" ) ) logger . info ( f \"Sentence classifier uses { rt . get_available_providers ()[ 0 ] } provider\" ) def transcribe ( self , audio : List [ float ]) -> np . ndarray : \"\"\"Transcribe audio usign this pipeline. Args: audio: ndarray of int16 of shape (samples,). Returns: Transcribed text from the audio. \"\"\" logits = self . _predict ( np . asarray ( audio , dtype = np . float32 )) return logits def decode ( self , logits : np . ndarray ) -> str : \"\"\"Decode logits into text. Args: logits: ndarray of float32 of shape (timesteps, vocab_size). Returns: Decoded string. \"\"\" return self . decoder . decode ( logits ) def decide_finished ( self , context : str , text : str ) -> bool : \"\"\"Decide if audio transcription should be finished. Args: context: Text context of the speech recognized (e.g. a question to which speech recognized is a reply to). text: Recognized speech so far Returns: Decision to stop recognition and finalize results. \"\"\" decision = self . _decide_sentence_finished ( context , text ) done = bool ( decision ) return done def postprocess ( self , text : str ) -> str : \"\"\"Add punctuation and capitalization. Args: text: audio transcription. Returns: Postprocessed text transcribtion. \"\"\" if self . predict_punctuation : enc = self . tokenizer . encode ( text ) punct , capit = self . punctuation . run ( None , { \"input_ids\" : np . asarray ( enc . ids , np . int64 ) . reshape ([ 1 , - 1 ]), \"attention_mask\" : np . asarray ( enc . attention_mask , np . int64 ) . reshape ( [ 1 , - 1 ] ), }, ) punct = punct . argmax ( 2 ) capit = capit . argmax ( 2 ) punctuated_capitalized = self . _apply_punct_capit_predictions ( text , punct [ 0 ][ 1 : - 1 ] . tolist (), capit [ 0 ][ 1 : - 1 ] . tolist () ) return punctuated_capitalized else : return text def _decide_sentence_finished ( self , context , text ): tokenized = self . sentence_tokenizer . encode ( context , text ) ids = np . asarray ( tokenized . ids ) . reshape ([ 1 , - 1 ]) . astype ( np . int64 ) type_ids = np . asarray ( tokenized . type_ids ) . reshape ([ 1 , - 1 ]) . astype ( np . int64 ) attention_mask = np . ones_like ( ids ) input_dict = { \"input_ids\" : ids , \"attention_mask\" : attention_mask , \"token_type_ids\" : type_ids , } logits = self . sentence_model . run ( None , input_dict )[ 0 ] return logits . argmax ( - 1 )[ 0 ] def _predict ( self , audio : np . ndarray ) -> np . ndarray : signal = audio . reshape ([ 1 , - 1 ]) audio_signal = self . _preprocess_signal ( signal ) . astype ( np . float32 ) return self . asr_model . run ( None , { \"audio_signal\" : audio_signal })[ 0 ][ 0 ] def _preprocess_signal ( self , signal ): audio_signal = signal . reshape ([ 1 , - 1 ]) # audio_signal += np.random.rand(*audio_signal.shape) * 1e-5 audio_signal = np . concatenate ( ( audio_signal [:, 0 ] . reshape ([ - 1 , 1 ]), audio_signal [:, 1 :] - 0.97 * audio_signal [:, : - 1 ], ), axis = 1 , ) audio_signal = audio_signal . reshape ([ - 1 ]) spectogram = librosa . stft ( audio_signal , n_fft = 512 , hop_length = 160 , win_length = 320 , window = self . stft_window , center = True , ) spectogram = np . stack ([ spectogram . real , spectogram . imag ], - 1 ) spectogram = np . sqrt (( spectogram ** 2 ) . sum ( - 1 )) spectogram = spectogram ** 2 spectogram = np . dot ( self . stft_filterbanks , spectogram ) spectogram = np . expand_dims ( spectogram , 0 ) spectogram = np . log ( spectogram + ( 2 ** - 24 )) spectogram = spectogram - np . asarray ( self . mel_mean ) . reshape ([ 1 , 64 , 1 ]) spectogram = spectogram / np . asarray ( self . mel_std ) . reshape ([ 1 , 64 , 1 ]) return spectogram def _apply_punct_capit_predictions ( self , query : str , punct_preds , capit_preds ) -> str : \"\"\"Restores punctuation and capitalization in ``query``. Args: query: a string without punctuation and capitalization punct_preds: ids of predicted punctuation labels capit_preds: ids of predicted capitalization labels Returns: a query with restored punctuation and capitalization \"\"\" query = query . strip () . split () query_with_punct_and_capit = \"\" skip = 0 for j , word in enumerate ( query ): word_enc = self . tokenizer . encode ( word , add_special_tokens = False ) . ids skip += len ( word_enc ) - 1 punct_label = self . punct_labels [ punct_preds [ j + skip ]] capit_label = self . capit_labels [ capit_preds [ j + skip ]] if capit_label != \"O\" : word = word . capitalize () query_with_punct_and_capit += word if punct_label != \"O\" : query_with_punct_and_capit += punct_label query_with_punct_and_capit += \" \" return query_with_punct_and_capit [: - 1 ] def _fixed_normalization ( self ): \"\"\"From https://github.com/NVIDIA/NeMo/blob/stable/tutorials/asr/Online_ASR_Microphone_Demo.ipynb . For whatever reason they use these values to normalize the mel spectogram. \"\"\" mel_fixed_mean = [ - 14.95827016 , - 12.71798736 , - 11.76067913 , - 10.83311182 , - 10.6746914 , - 10.15163465 , - 10.05378331 , - 9.53918999 , - 9.41858904 , - 9.23382904 , - 9.46470918 , - 9.56037 , - 9.57434245 , - 9.47498732 , - 9.7635205 , - 10.08113074 , - 10.05454561 , - 9.81112681 , - 9.68673603 , - 9.83652977 , - 9.90046248 , - 9.85404766 , - 9.92560366 , - 9.95440354 , - 10.17162966 , - 9.90102482 , - 9.47471025 , - 9.54416855 , - 10.07109475 , - 9.98249912 , - 9.74359465 , - 9.55632283 , - 9.23399915 , - 9.36487649 , - 9.81791084 , - 9.56799225 , - 9.70630899 , - 9.85148006 , - 9.8594418 , - 10.01378735 , - 9.98505315 , - 9.62016094 , - 10.342285 , - 10.41070709 , - 10.10687659 , - 10.14536695 , - 10.30828702 , - 10.23542833 , - 10.88546868 , - 11.31723646 , - 11.46087382 , - 11.54877829 , - 11.62400934 , - 11.92190509 , - 12.14063815 , - 11.65130117 , - 11.58308531 , - 12.22214663 , - 12.42927197 , - 12.58039805 , - 13.10098969 , - 13.14345864 , - 13.31835645 , - 14.47345634 , ] mel_fixed_std = [ 3.81402054 , 4.12647781 , 4.05007065 , 3.87790987 , 3.74721178 , 3.68377423 , 3.69344 , 3.54001005 , 3.59530412 , 3.63752368 , 3.62826417 , 3.56488469 , 3.53740577 , 3.68313898 , 3.67138151 , 3.55707266 , 3.54919572 , 3.55721289 , 3.56723346 , 3.46029304 , 3.44119672 , 3.49030548 , 3.39328435 , 3.28244406 , 3.28001423 , 3.26744937 , 3.46692348 , 3.35378948 , 2.96330901 , 2.97663111 , 3.04575148 , 2.89717604 , 2.95659301 , 2.90181116 , 2.7111687 , 2.93041291 , 2.86647897 , 2.73473181 , 2.71495654 , 2.75543763 , 2.79174615 , 2.96076456 , 2.57376336 , 2.68789782 , 2.90930817 , 2.90412004 , 2.76187531 , 2.89905006 , 2.65896173 , 2.81032176 , 2.87769857 , 2.84665271 , 2.80863137 , 2.80707634 , 2.83752184 , 3.01914511 , 2.92046439 , 2.78461139 , 2.90034605 , 2.94599508 , 2.99099718 , 3.0167554 , 3.04649716 , 2.94116777 , ] return mel_fixed_mean , mel_fixed_std __init__ ( model_path , frame_size = 1000 , sample_rate = 16000 , predict_punctuation = False , * args , ** kwargs ) Create and load biencoder model for semantic similarity. Parameters: Name Type Description Default model_path str A path where model config and weights are stored. required frame_size int Size of the audio frame in milliseconds. 1000 sample_rate int Sample rate of the audio. 16000 predict_punctuation bool Whether to predict punctuation and capitalization. False Source code in npc_engine\\services\\stt\\nemo_stt.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def __init__ ( self , model_path : str , frame_size : int = 1000 , sample_rate : int = 16000 , predict_punctuation : bool = False , * args , ** kwargs , ): \"\"\"Create and load biencoder model for semantic similarity. Args: model_path: A path where model config and weights are stored. frame_size: Size of the audio frame in milliseconds. sample_rate: Sample rate of the audio. predict_punctuation: Whether to predict punctuation and capitalization. \"\"\" super () . __init__ ( sample_rate = sample_rate , model_path = model_path , frame_size = frame_size , * args , ** kwargs , ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_BASIC provider = rt . get_available_providers ()[ 0 ] logger . info ( f \"STT uses { provider } provider\" ) self . stft_filterbanks = librosa . filters . mel ( sr = 16000 , n_fft = 512 , n_mels = 64 , fmin = 0 , fmax = 8000 ) self . stft_window = librosa . filters . get_window ( \"hann\" , 320 , fftbins = False ) self . mel_mean , self . mel_std = self . _fixed_normalization () self . asr_model = rt . InferenceSession ( path . join ( model_path , \"ctc.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . predict_punctuation = predict_punctuation if self . predict_punctuation : self . punctuation = rt . InferenceSession ( path . join ( model_path , \"punctuation.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( path . join ( model_path , \"tokenizer.json\" ) ) self . asr_vocab = \" abcdefghijklmnopqrstuvwxyz'\" self . asr_vocab = list ( self . asr_vocab ) self . asr_vocab . append ( \"\" ) self . punct_labels = \"O,.?\" self . capit_labels = \"OU\" self . decoder = build_ctcdecoder ( self . asr_vocab ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = opt_level . ORT_ENABLE_ALL self . sentence_model = rt . InferenceSession ( path . join ( model_path , \"sentence_prediction.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . sentence_tokenizer = Tokenizer . from_file ( path . join ( model_path , \"sentence_tokenizer.json\" ) ) logger . info ( f \"Sentence classifier uses { rt . get_available_providers ()[ 0 ] } provider\" ) _apply_punct_capit_predictions ( query , punct_preds , capit_preds ) Restores punctuation and capitalization in query . Parameters: Name Type Description Default query str a string without punctuation and capitalization required punct_preds ids of predicted punctuation labels required capit_preds ids of predicted capitalization labels required Returns: Type Description str a query with restored punctuation and capitalization Source code in npc_engine\\services\\stt\\nemo_stt.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 def _apply_punct_capit_predictions ( self , query : str , punct_preds , capit_preds ) -> str : \"\"\"Restores punctuation and capitalization in ``query``. Args: query: a string without punctuation and capitalization punct_preds: ids of predicted punctuation labels capit_preds: ids of predicted capitalization labels Returns: a query with restored punctuation and capitalization \"\"\" query = query . strip () . split () query_with_punct_and_capit = \"\" skip = 0 for j , word in enumerate ( query ): word_enc = self . tokenizer . encode ( word , add_special_tokens = False ) . ids skip += len ( word_enc ) - 1 punct_label = self . punct_labels [ punct_preds [ j + skip ]] capit_label = self . capit_labels [ capit_preds [ j + skip ]] if capit_label != \"O\" : word = word . capitalize () query_with_punct_and_capit += word if punct_label != \"O\" : query_with_punct_and_capit += punct_label query_with_punct_and_capit += \" \" return query_with_punct_and_capit [: - 1 ] _fixed_normalization () From https://github.com/NVIDIA/NeMo/blob/stable/tutorials/asr/Online_ASR_Microphone_Demo.ipynb . For whatever reason they use these values to normalize the mel spectogram. Source code in npc_engine\\services\\stt\\nemo_stt.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 def _fixed_normalization ( self ): \"\"\"From https://github.com/NVIDIA/NeMo/blob/stable/tutorials/asr/Online_ASR_Microphone_Demo.ipynb . For whatever reason they use these values to normalize the mel spectogram. \"\"\" mel_fixed_mean = [ - 14.95827016 , - 12.71798736 , - 11.76067913 , - 10.83311182 , - 10.6746914 , - 10.15163465 , - 10.05378331 , - 9.53918999 , - 9.41858904 , - 9.23382904 , - 9.46470918 , - 9.56037 , - 9.57434245 , - 9.47498732 , - 9.7635205 , - 10.08113074 , - 10.05454561 , - 9.81112681 , - 9.68673603 , - 9.83652977 , - 9.90046248 , - 9.85404766 , - 9.92560366 , - 9.95440354 , - 10.17162966 , - 9.90102482 , - 9.47471025 , - 9.54416855 , - 10.07109475 , - 9.98249912 , - 9.74359465 , - 9.55632283 , - 9.23399915 , - 9.36487649 , - 9.81791084 , - 9.56799225 , - 9.70630899 , - 9.85148006 , - 9.8594418 , - 10.01378735 , - 9.98505315 , - 9.62016094 , - 10.342285 , - 10.41070709 , - 10.10687659 , - 10.14536695 , - 10.30828702 , - 10.23542833 , - 10.88546868 , - 11.31723646 , - 11.46087382 , - 11.54877829 , - 11.62400934 , - 11.92190509 , - 12.14063815 , - 11.65130117 , - 11.58308531 , - 12.22214663 , - 12.42927197 , - 12.58039805 , - 13.10098969 , - 13.14345864 , - 13.31835645 , - 14.47345634 , ] mel_fixed_std = [ 3.81402054 , 4.12647781 , 4.05007065 , 3.87790987 , 3.74721178 , 3.68377423 , 3.69344 , 3.54001005 , 3.59530412 , 3.63752368 , 3.62826417 , 3.56488469 , 3.53740577 , 3.68313898 , 3.67138151 , 3.55707266 , 3.54919572 , 3.55721289 , 3.56723346 , 3.46029304 , 3.44119672 , 3.49030548 , 3.39328435 , 3.28244406 , 3.28001423 , 3.26744937 , 3.46692348 , 3.35378948 , 2.96330901 , 2.97663111 , 3.04575148 , 2.89717604 , 2.95659301 , 2.90181116 , 2.7111687 , 2.93041291 , 2.86647897 , 2.73473181 , 2.71495654 , 2.75543763 , 2.79174615 , 2.96076456 , 2.57376336 , 2.68789782 , 2.90930817 , 2.90412004 , 2.76187531 , 2.89905006 , 2.65896173 , 2.81032176 , 2.87769857 , 2.84665271 , 2.80863137 , 2.80707634 , 2.83752184 , 3.01914511 , 2.92046439 , 2.78461139 , 2.90034605 , 2.94599508 , 2.99099718 , 3.0167554 , 3.04649716 , 2.94116777 , ] return mel_fixed_mean , mel_fixed_std decide_finished ( context , text ) Decide if audio transcription should be finished. Parameters: Name Type Description Default context str Text context of the speech recognized (e.g. a question to which speech recognized is a reply to). required text str Recognized speech so far required Returns: Type Description bool Decision to stop recognition and finalize results. Source code in npc_engine\\services\\stt\\nemo_stt.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def decide_finished ( self , context : str , text : str ) -> bool : \"\"\"Decide if audio transcription should be finished. Args: context: Text context of the speech recognized (e.g. a question to which speech recognized is a reply to). text: Recognized speech so far Returns: Decision to stop recognition and finalize results. \"\"\" decision = self . _decide_sentence_finished ( context , text ) done = bool ( decision ) return done decode ( logits ) Decode logits into text. Parameters: Name Type Description Default logits np . ndarray ndarray of float32 of shape (timesteps, vocab_size). required Returns: Type Description str Decoded string. Source code in npc_engine\\services\\stt\\nemo_stt.py 136 137 138 139 140 141 142 143 144 145 def decode ( self , logits : np . ndarray ) -> str : \"\"\"Decode logits into text. Args: logits: ndarray of float32 of shape (timesteps, vocab_size). Returns: Decoded string. \"\"\" return self . decoder . decode ( logits ) postprocess ( text ) Add punctuation and capitalization. Parameters: Name Type Description Default text str audio transcription. required Returns: Type Description str Postprocessed text transcribtion. Source code in npc_engine\\services\\stt\\nemo_stt.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def postprocess ( self , text : str ) -> str : \"\"\"Add punctuation and capitalization. Args: text: audio transcription. Returns: Postprocessed text transcribtion. \"\"\" if self . predict_punctuation : enc = self . tokenizer . encode ( text ) punct , capit = self . punctuation . run ( None , { \"input_ids\" : np . asarray ( enc . ids , np . int64 ) . reshape ([ 1 , - 1 ]), \"attention_mask\" : np . asarray ( enc . attention_mask , np . int64 ) . reshape ( [ 1 , - 1 ] ), }, ) punct = punct . argmax ( 2 ) capit = capit . argmax ( 2 ) punctuated_capitalized = self . _apply_punct_capit_predictions ( text , punct [ 0 ][ 1 : - 1 ] . tolist (), capit [ 0 ][ 1 : - 1 ] . tolist () ) return punctuated_capitalized else : return text transcribe ( audio ) Transcribe audio usign this pipeline. Parameters: Name Type Description Default audio List [ float ] ndarray of int16 of shape (samples,). required Returns: Type Description np . ndarray Transcribed text from the audio. Source code in npc_engine\\services\\stt\\nemo_stt.py 124 125 126 127 128 129 130 131 132 133 134 def transcribe ( self , audio : List [ float ]) -> np . ndarray : \"\"\"Transcribe audio usign this pipeline. Args: audio: ndarray of int16 of shape (samples,). Returns: Transcribed text from the audio. \"\"\" logits = self . _predict ( np . asarray ( audio , dtype = np . float32 )) return logits","title":"NemoSTT"},{"location":"inference_engine/reference/#npc_engine.services.stt.stt_base","text":"Module that implements speech to text model API.","title":"stt_base"},{"location":"inference_engine/reference/#npc_engine.services.stt.stt_base.SpeechToTextAPI","text":"Bases: BaseService Abstract base class for speech to text models. Source code in npc_engine\\services\\stt\\stt_base.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 class SpeechToTextAPI ( BaseService ): \"\"\"Abstract base class for speech to text models.\"\"\" API_METHODS : List [ str ] = [ \"listen\" , \"stt\" , \"get_devices\" , \"select_device\" , \"initialize_microphone_input\" , ] def __init__ ( self , min_speech_duration = 100 , max_silence_duration = 1000 , vad_mode = None , sample_rate = 16000 , vad_frame_ms = 10 , pad_size = 1000 , * args , ** kwargs , ): \"\"\"Initialize VAD part of the API.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True sd . default . samplerate = sample_rate self . max_silence_duration = max_silence_duration self . min_speech_duration = min_speech_duration self . vad = webrtcvad . Vad () if vad_mode is not None : self . vad . set_mode ( vad_mode ) self . sample_rate = sample_rate self . listen_queue = Queue ( 10 ) self . vad_frame_ms = vad_frame_ms self . vad_frame_size = int (( vad_frame_ms * sample_rate ) / 1000 ) self . running = False self . microphone_initialized = False self . silence_buffer = np . empty ([ 0 ]) self . pad_size = pad_size @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"SpeechToTextAPI\" def __del__ ( self ): \"\"\"Stop listening on destruction.\"\"\" if self . microphone_initialized : self . stream . stop () def initialize_microphone_input ( self ): \"\"\"Initialize microphone.\"\"\" if self . microphone_initialized : return self . running = False self . microphone_initialized = True def callback ( in_data , frame_count , time_info , status ): if self . running : try : self . listen_queue . put ( in_data . reshape ( - 1 ), block = False ) except Exception : return else : if not self . _vad_frame ( in_data . reshape ( - 1 ) ): # Register only silence for buffer self . silence_buffer = np . append ( self . silence_buffer , in_data . reshape ( - 1 ) ) self . silence_buffer = self . silence_buffer [ - self . pad_size :] self . stream = sd . InputStream ( samplerate = self . sample_rate , channels = 1 , blocksize = self . vad_frame_size , callback = callback , ) self . stream . start () while self . silence_buffer . shape [ 0 ] < self . pad_size : pass def listen ( self , context : str = None ) -> str : # pragma: no cover \"\"\"Listen for speech input and return text from speech when done. Listens for speech, if speech is active for longer than self.frame_size in milliseconds then starts transcribing it. On each voice activity detection (VAD) pause uses context to decide if transcribed text is a finished response to a context. If it is, applies preprocessing and returns the result. If transcribed text is not a response to a context but VAD pause persists through max_silence_duration then returns the results anyway. Requires a microphone input to be initialized. Args: context: A last line of the dialogue used to decide when to stop listening. It allows our STT system to not wait for a VAD timeout (max_silence_duration in ms). Returns: Recognized text from the audio. \"\"\" if not self . microphone_initialized : raise RuntimeError ( \"Microphone not initialized.\" ) self . listen_queue . queue . clear () context = re . sub ( r \"[^A-Za-z0-9 ]+\" , \"\" , context ) . lower () if context else None text = self . _transcribe_vad_pause ( context ) processed = self . postprocess ( text ) self . listen_queue . queue . clear () return processed def _transcribe_vad_pause ( self , context ) -> str : done = False total_pause_ms = 0 total_speech_ms = 0 speech_appeared = False tested_pause = False logits = None signal = self . silence_buffer self . running = True while not done : try : vad_frame = self . listen_queue . get ( block = False ) except Exception : continue is_speech = self . _vad_frame ( vad_frame ) total_speech_ms , total_pause_ms = self . _update_vad_stats ( is_speech , total_speech_ms , total_pause_ms ) if total_speech_ms > self . min_speech_duration : speech_appeared = True if total_pause_ms == 0 : tested_pause = False signal = np . append ( signal , vad_frame ) if not speech_appeared and total_speech_ms < self . min_speech_duration : # Keep only last minimum detectable speech duration + buffer signal = signal [ - self . _ms_to_samplenum ( self . min_speech_duration * 2 ) :] if signal . shape [ 0 ] >= self . _ms_to_samplenum ( self . min_speech_duration ): if speech_appeared and total_pause_ms > self . max_silence_duration : wrapped_signal = self . _wrap_signal ( signal ) logits = self . transcribe ( wrapped_signal ) text = self . decode ( logits ) done = True self . running = False return text elif ( speech_appeared and total_pause_ms > self . min_speech_duration and not tested_pause ): tested_pause = True logits = self . transcribe ( np . pad ( signal , ( 0 , 1000 ), \"wrap\" )) text = self . decode ( logits ) done = self . decide_finished ( context , text ) self . running = not done self . running = False return text def stt ( self , audio : List [ int ]) -> str : \"\"\"Transcribe speech. Args: audio: PMC data with bit depth 16. Returns: Recognized text from the audio. \"\"\" logits = self . transcribe ( audio ) text = self . decode ( logits ) text = self . postprocess ( text ) return text def get_devices ( self ) -> Dict [ int , str ]: # pragma: no cover \"\"\"Get available audio devices.\"\"\" return [ device [ \"name\" ] for device in sd . query_devices ()] def select_device ( self , device_id : int ): # pragma: no cover \"\"\"Get available audio devices.\"\"\" device_id = int ( device_id ) if device_id >= len ( sd . query_devices ()) or device_id < 0 : raise ValueError ( f \"Bad device id, valid device ids in range [0; { len ( sd . query_devices ()) } )\" ) sd . default . device = device_id def _wrap_signal ( self , signal : np . ndarray ) -> np . ndarray : \"\"\"Append silence buffer at both ends of the signal.\"\"\" signal_with_silence = np . append ( np . append ( self . silence_buffer , signal ), self . silence_buffer ) return signal_with_silence def _vad_frame ( self , frame ): # pragma: no cover \"\"\"Detect voice activity in a frame.\"\"\" is_speech = self . vad . is_speech ( np . frombuffer (( frame * 32767 ) . astype ( np . int16 ) . tobytes (), np . int8 ), sample_rate = 16000 , ) return is_speech def _ms_to_samplenum ( self , time ): return int ( time * self . sample_rate / 1000 ) def _samples_to_ms ( self , samples ): return samples * 1000 / self . sample_rate def _update_vad_stats ( self , is_speech , total_speech_ms , total_pause_ms ): \"\"\"Increase total_speech_ms if is_speech is true and appears for min_speech_duration frames consequently.\"\"\" if is_speech : total_speech_ms += self . vad_frame_ms total_pause_ms = 0 else : total_pause_ms += self . vad_frame_ms total_speech_ms = 0 return total_speech_ms , total_pause_ms @abstractmethod def transcribe ( self , audio : np . ndarray ) -> np . ndarray : \"\"\"Abstract method for audio transcription. Should be implemented by the specific model. Args: audio: ndarray of int16 of shape (samples,). Returns: Transcribed logits from the audio. \"\"\" return None @abstractmethod def decode ( self , logits : np . ndarray ) -> str : \"\"\"Decode logits into text. Args: logits: ndarray of float32 of shape (timesteps, vocab_size). Returns: Decoded string. \"\"\" return None @abstractmethod def decide_finished ( self , context : str , text : str ) -> bool : \"\"\"Abstract method for deciding if audio transcription should be finished. Should be implemented by the specific model. Args: context: Text context of the speech recognized (e.g. a question to which speech recognized is a reply to). text: Recognized speech so far. Returns: Decision to stop recognition and finalize results. \"\"\" return None @abstractmethod def postprocess ( self , text : str ) -> str : \"\"\"Abstract method for audio transcription postprocessing. Should be implemented by the specific model. Args: text: audio transcription. Returns: Postprocessed text transcribtion. \"\"\" return None __del__ () Stop listening on destruction. Source code in npc_engine\\services\\stt\\stt_base.py 62 63 64 65 def __del__ ( self ): \"\"\"Stop listening on destruction.\"\"\" if self . microphone_initialized : self . stream . stop () __init__ ( min_speech_duration = 100 , max_silence_duration = 1000 , vad_mode = None , sample_rate = 16000 , vad_frame_ms = 10 , pad_size = 1000 , * args , ** kwargs ) Initialize VAD part of the API. Source code in npc_engine\\services\\stt\\stt_base.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , min_speech_duration = 100 , max_silence_duration = 1000 , vad_mode = None , sample_rate = 16000 , vad_frame_ms = 10 , pad_size = 1000 , * args , ** kwargs , ): \"\"\"Initialize VAD part of the API.\"\"\" super () . __init__ ( * args , ** kwargs ) self . initialized = True sd . default . samplerate = sample_rate self . max_silence_duration = max_silence_duration self . min_speech_duration = min_speech_duration self . vad = webrtcvad . Vad () if vad_mode is not None : self . vad . set_mode ( vad_mode ) self . sample_rate = sample_rate self . listen_queue = Queue ( 10 ) self . vad_frame_ms = vad_frame_ms self . vad_frame_size = int (( vad_frame_ms * sample_rate ) / 1000 ) self . running = False self . microphone_initialized = False self . silence_buffer = np . empty ([ 0 ]) self . pad_size = pad_size _update_vad_stats ( is_speech , total_speech_ms , total_pause_ms ) Increase total_speech_ms if is_speech is true and appears for min_speech_duration frames consequently. Source code in npc_engine\\services\\stt\\stt_base.py 231 232 233 234 235 236 237 238 239 def _update_vad_stats ( self , is_speech , total_speech_ms , total_pause_ms ): \"\"\"Increase total_speech_ms if is_speech is true and appears for min_speech_duration frames consequently.\"\"\" if is_speech : total_speech_ms += self . vad_frame_ms total_pause_ms = 0 else : total_pause_ms += self . vad_frame_ms total_speech_ms = 0 return total_speech_ms , total_pause_ms _vad_frame ( frame ) Detect voice activity in a frame. Source code in npc_engine\\services\\stt\\stt_base.py 217 218 219 220 221 222 223 def _vad_frame ( self , frame ): # pragma: no cover \"\"\"Detect voice activity in a frame.\"\"\" is_speech = self . vad . is_speech ( np . frombuffer (( frame * 32767 ) . astype ( np . int16 ) . tobytes (), np . int8 ), sample_rate = 16000 , ) return is_speech _wrap_signal ( signal ) Append silence buffer at both ends of the signal. Source code in npc_engine\\services\\stt\\stt_base.py 210 211 212 213 214 215 def _wrap_signal ( self , signal : np . ndarray ) -> np . ndarray : \"\"\"Append silence buffer at both ends of the signal.\"\"\" signal_with_silence = np . append ( np . append ( self . silence_buffer , signal ), self . silence_buffer ) return signal_with_silence decide_finished ( context , text ) abstractmethod Abstract method for deciding if audio transcription should be finished. Should be implemented by the specific model. Parameters: Name Type Description Default context str Text context of the speech recognized (e.g. a question to which speech recognized is a reply to). required text str Recognized speech so far. required Returns: Type Description bool Decision to stop recognition and finalize results. Source code in npc_engine\\services\\stt\\stt_base.py 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 @abstractmethod def decide_finished ( self , context : str , text : str ) -> bool : \"\"\"Abstract method for deciding if audio transcription should be finished. Should be implemented by the specific model. Args: context: Text context of the speech recognized (e.g. a question to which speech recognized is a reply to). text: Recognized speech so far. Returns: Decision to stop recognition and finalize results. \"\"\" return None decode ( logits ) abstractmethod Decode logits into text. Parameters: Name Type Description Default logits np . ndarray ndarray of float32 of shape (timesteps, vocab_size). required Returns: Type Description str Decoded string. Source code in npc_engine\\services\\stt\\stt_base.py 255 256 257 258 259 260 261 262 263 264 265 @abstractmethod def decode ( self , logits : np . ndarray ) -> str : \"\"\"Decode logits into text. Args: logits: ndarray of float32 of shape (timesteps, vocab_size). Returns: Decoded string. \"\"\" return None get_api_name () classmethod Get the API name. Source code in npc_engine\\services\\stt\\stt_base.py 57 58 59 60 @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"SpeechToTextAPI\" get_devices () Get available audio devices. Source code in npc_engine\\services\\stt\\stt_base.py 197 198 199 def get_devices ( self ) -> Dict [ int , str ]: # pragma: no cover \"\"\"Get available audio devices.\"\"\" return [ device [ \"name\" ] for device in sd . query_devices ()] initialize_microphone_input () Initialize microphone. Source code in npc_engine\\services\\stt\\stt_base.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def initialize_microphone_input ( self ): \"\"\"Initialize microphone.\"\"\" if self . microphone_initialized : return self . running = False self . microphone_initialized = True def callback ( in_data , frame_count , time_info , status ): if self . running : try : self . listen_queue . put ( in_data . reshape ( - 1 ), block = False ) except Exception : return else : if not self . _vad_frame ( in_data . reshape ( - 1 ) ): # Register only silence for buffer self . silence_buffer = np . append ( self . silence_buffer , in_data . reshape ( - 1 ) ) self . silence_buffer = self . silence_buffer [ - self . pad_size :] self . stream = sd . InputStream ( samplerate = self . sample_rate , channels = 1 , blocksize = self . vad_frame_size , callback = callback , ) self . stream . start () while self . silence_buffer . shape [ 0 ] < self . pad_size : pass listen ( context = None ) Listen for speech input and return text from speech when done. Listens for speech, if speech is active for longer than self.frame_size in milliseconds then starts transcribing it. On each voice activity detection (VAD) pause uses context to decide if transcribed text is a finished response to a context. If it is, applies preprocessing and returns the result. If transcribed text is not a response to a context but VAD pause persists through max_silence_duration then returns the results anyway. Requires a microphone input to be initialized. Parameters: Name Type Description Default context str A last line of the dialogue used to decide when to stop listening. It allows our STT system to not wait for a VAD timeout (max_silence_duration in ms). None Returns: Type Description str Recognized text from the audio. Source code in npc_engine\\services\\stt\\stt_base.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def listen ( self , context : str = None ) -> str : # pragma: no cover \"\"\"Listen for speech input and return text from speech when done. Listens for speech, if speech is active for longer than self.frame_size in milliseconds then starts transcribing it. On each voice activity detection (VAD) pause uses context to decide if transcribed text is a finished response to a context. If it is, applies preprocessing and returns the result. If transcribed text is not a response to a context but VAD pause persists through max_silence_duration then returns the results anyway. Requires a microphone input to be initialized. Args: context: A last line of the dialogue used to decide when to stop listening. It allows our STT system to not wait for a VAD timeout (max_silence_duration in ms). Returns: Recognized text from the audio. \"\"\" if not self . microphone_initialized : raise RuntimeError ( \"Microphone not initialized.\" ) self . listen_queue . queue . clear () context = re . sub ( r \"[^A-Za-z0-9 ]+\" , \"\" , context ) . lower () if context else None text = self . _transcribe_vad_pause ( context ) processed = self . postprocess ( text ) self . listen_queue . queue . clear () return processed postprocess ( text ) abstractmethod Abstract method for audio transcription postprocessing. Should be implemented by the specific model. Parameters: Name Type Description Default text str audio transcription. required Returns: Type Description str Postprocessed text transcribtion. Source code in npc_engine\\services\\stt\\stt_base.py 283 284 285 286 287 288 289 290 291 292 293 294 295 @abstractmethod def postprocess ( self , text : str ) -> str : \"\"\"Abstract method for audio transcription postprocessing. Should be implemented by the specific model. Args: text: audio transcription. Returns: Postprocessed text transcribtion. \"\"\" return None select_device ( device_id ) Get available audio devices. Source code in npc_engine\\services\\stt\\stt_base.py 201 202 203 204 205 206 207 208 def select_device ( self , device_id : int ): # pragma: no cover \"\"\"Get available audio devices.\"\"\" device_id = int ( device_id ) if device_id >= len ( sd . query_devices ()) or device_id < 0 : raise ValueError ( f \"Bad device id, valid device ids in range [0; { len ( sd . query_devices ()) } )\" ) sd . default . device = device_id stt ( audio ) Transcribe speech. Parameters: Name Type Description Default audio List [ int ] PMC data with bit depth 16. required Returns: Type Description str Recognized text from the audio. Source code in npc_engine\\services\\stt\\stt_base.py 183 184 185 186 187 188 189 190 191 192 193 194 195 def stt ( self , audio : List [ int ]) -> str : \"\"\"Transcribe speech. Args: audio: PMC data with bit depth 16. Returns: Recognized text from the audio. \"\"\" logits = self . transcribe ( audio ) text = self . decode ( logits ) text = self . postprocess ( text ) return text transcribe ( audio ) abstractmethod Abstract method for audio transcription. Should be implemented by the specific model. Parameters: Name Type Description Default audio np . ndarray ndarray of int16 of shape (samples,). required Returns: Type Description np . ndarray Transcribed logits from the audio. Source code in npc_engine\\services\\stt\\stt_base.py 241 242 243 244 245 246 247 248 249 250 251 252 253 @abstractmethod def transcribe ( self , audio : np . ndarray ) -> np . ndarray : \"\"\"Abstract method for audio transcription. Should be implemented by the specific model. Args: audio: ndarray of int16 of shape (samples,). Returns: Transcribed logits from the audio. \"\"\" return None","title":"SpeechToTextAPI"},{"location":"inference_engine/reference/#npc_engine.services.text_generation","text":"Text generation API services. This module implements specific models and wraps them under the common interface for loading and inference. Example from npc_engine.services.text_generation import TextGenerationAPI model = TextGenerationAPI.load(\"path/to/model_dir\") model.generate_reply(context, temperature=0.8, topk=None,)","title":"text_generation"},{"location":"inference_engine/reference/#npc_engine.services.text_generation.bart","text":"BART based chatbot implementation.","title":"bart"},{"location":"inference_engine/reference/#npc_engine.services.text_generation.bart.BartChatbot","text":"Bases: TextGenerationAPI BART based chatbot implementation class. This model class requires two ONNX models encoder_bart.onnx and decoder_bart.onnx that correspond to encoder and decoder from transformers EncoderDecoderModel and a tokenizer.json with huggingface tokenizers definition. encoder_bart.onnx spec: - inputs: `input_ids` - outputs: `encoder_hidden_state` decoder_bart.onnx spec: - inputs: `encoder_hidden_state` `decoder_input_ids` - outputs: `logits` Source code in npc_engine\\services\\text_generation\\bart.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 class BartChatbot ( TextGenerationAPI ): \"\"\"BART based chatbot implementation class. This model class requires two ONNX models `encoder_bart.onnx` and `decoder_bart.onnx` that correspond to encoder and decoder from transformers [EncoderDecoderModel](https://huggingface.co/transformers/model_doc/encoderdecoder.html) and a tokenizer.json with huggingface tokenizers definition. encoder_bart.onnx spec: - inputs: `input_ids` - outputs: `encoder_hidden_state` decoder_bart.onnx spec: - inputs: `encoder_hidden_state` `decoder_input_ids` - outputs: `logits` \"\"\" def __init__ ( self , model_path , max_steps = 100 , min_length = 2 , repetition_penalty = 1 , bos_token_id = 0 , eos_token_id = 2 , pad_token_id = 1 , sep_token_id = None , trunc_length = 512 , * args , ** kwargs , ): \"\"\"Create the chatbot from config args and kwargs. Args: model_path: path to scan for model files (weights and configs) max_steps: stop generation at this number of tokens min_length: model can't stop generating text before it's atleast this long in tokens repetition_penalty: probability coef for same tokens to appear multiple times bos_token_id: beginning of sequence token id eos_token_id: end of sequence token id pad_token_id: padding token id sep_token_id: token id for separating sequence into multiple parts \"\"\" super () . __init__ ( * args , ** kwargs ) self . bos_token_id = bos_token_id self . eos_token_id = eos_token_id self . sep_token_id = eos_token_id if sep_token_id is None else sep_token_id self . pad_token_id = pad_token_id sess_options = rt . SessionOptions () sess_options . graph_optimization_level = ( rt . GraphOptimizationLevel . ORT_ENABLE_ALL ) self . encoder_model = rt . InferenceSession ( os . path . join ( model_path , \"encoder_bart.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . decoder_model = rt . InferenceSession ( os . path . join ( model_path , \"decoder_bart.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) added_tokens_path = os . path . join ( model_path , \"added_tokens.txt\" ) if os . path . exists ( added_tokens_path ): with open ( added_tokens_path ) as f : added_tokens = json . load ( f ) added_tokens = [ key for key , _ in sorted ( list ( added_tokens . items ()), key = lambda x : x [ 1 ]) ] self . tokenizer . add_tokens ( added_tokens ) self . special_tokens = { \"bos_token\" : self . tokenizer . decode ( [ bos_token_id ], skip_special_tokens = False ), \"eos_token\" : self . tokenizer . decode ( [ eos_token_id ], skip_special_tokens = False ), \"sep_token\" : self . tokenizer . decode ( [ self . sep_token_id ], skip_special_tokens = False ), \"pad_token\" : self . tokenizer . decode ( [ pad_token_id ], skip_special_tokens = False ), ** { f \"added_token { self . tokenizer . token_to_id ( token ) } \" : token for token in added_tokens }, } self . max_steps = max_steps self . min_length = min_length self . repetition_penalty = repetition_penalty self . trunc_length = trunc_length def run ( self , prompt : str , temperature : float = 1.0 , topk : int = None ) -> str : \"\"\"Run text generation from given prompt and parameters. Args: prompt: Fromatted prompt. temperature: Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness topk: If not none selects top n of predictions to sample from during generation. Returns: Generated text \"\"\" tokens = self . tokenizer . encode ( prompt ) total = np . asarray ( tokens . ids , dtype = np . int64 ) . reshape ([ 1 , - 1 ]) total_enc = self . encoder_model . run ( None , { \"input_ids\" : total })[ 0 ] utterance = np . asarray ([ self . eos_token_id ], dtype = np . int64 ) . reshape ([ 1 , 1 ]) for i in range ( self . max_steps ): o = self . decoder_model . run ( None , { \"encoder_hidden_state\" : total_enc , \"decoder_input_ids\" : utterance }, ) logits = o [ 0 ][ 0 , - 1 , :] if i < self . min_length : logits [ self . eos_token_id ] = float ( \"-inf\" ) if topk is not None : ind = np . argpartition ( logits , - topk )[ - topk :] new_logits = np . zeros ( logits . shape ) new_logits [ ind ] = logits [ ind ] logits = new_logits probs = scp . softmax ( logits / temperature , axis = 0 ) token = np . random . choice ( np . arange ( probs . shape [ 0 ]), p = probs ) token = token . reshape ([ 1 , 1 ]) utterance = np . concatenate ([ utterance , token ], axis = 1 ) if token [ 0 , 0 ] == self . eos_token_id : break return self . tokenizer . decode ( utterance [ 0 , :] . tolist (), skip_special_tokens = True ) def get_special_tokens ( self ) -> Dict [ str , str ]: \"\"\"Retrun dict of special tokens to be renderable from template.\"\"\" return self . special_tokens def string_too_long ( self , prompt ): \"\"\"Check if prompt is too long for the model.\"\"\" return len ( self . tokenizer . encode ( prompt )) > self . trunc_length __init__ ( model_path , max_steps = 100 , min_length = 2 , repetition_penalty = 1 , bos_token_id = 0 , eos_token_id = 2 , pad_token_id = 1 , sep_token_id = None , trunc_length = 512 , * args , ** kwargs ) Create the chatbot from config args and kwargs. Parameters: Name Type Description Default model_path path to scan for model files (weights and configs) required max_steps stop generation at this number of tokens 100 min_length model can't stop generating text before it's atleast this long in tokens 2 repetition_penalty probability coef for same tokens to appear multiple times 1 bos_token_id beginning of sequence token id 0 eos_token_id end of sequence token id 2 pad_token_id padding token id 1 sep_token_id token id for separating sequence into multiple parts None Source code in npc_engine\\services\\text_generation\\bart.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def __init__ ( self , model_path , max_steps = 100 , min_length = 2 , repetition_penalty = 1 , bos_token_id = 0 , eos_token_id = 2 , pad_token_id = 1 , sep_token_id = None , trunc_length = 512 , * args , ** kwargs , ): \"\"\"Create the chatbot from config args and kwargs. Args: model_path: path to scan for model files (weights and configs) max_steps: stop generation at this number of tokens min_length: model can't stop generating text before it's atleast this long in tokens repetition_penalty: probability coef for same tokens to appear multiple times bos_token_id: beginning of sequence token id eos_token_id: end of sequence token id pad_token_id: padding token id sep_token_id: token id for separating sequence into multiple parts \"\"\" super () . __init__ ( * args , ** kwargs ) self . bos_token_id = bos_token_id self . eos_token_id = eos_token_id self . sep_token_id = eos_token_id if sep_token_id is None else sep_token_id self . pad_token_id = pad_token_id sess_options = rt . SessionOptions () sess_options . graph_optimization_level = ( rt . GraphOptimizationLevel . ORT_ENABLE_ALL ) self . encoder_model = rt . InferenceSession ( os . path . join ( model_path , \"encoder_bart.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . decoder_model = rt . InferenceSession ( os . path . join ( model_path , \"decoder_bart.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) added_tokens_path = os . path . join ( model_path , \"added_tokens.txt\" ) if os . path . exists ( added_tokens_path ): with open ( added_tokens_path ) as f : added_tokens = json . load ( f ) added_tokens = [ key for key , _ in sorted ( list ( added_tokens . items ()), key = lambda x : x [ 1 ]) ] self . tokenizer . add_tokens ( added_tokens ) self . special_tokens = { \"bos_token\" : self . tokenizer . decode ( [ bos_token_id ], skip_special_tokens = False ), \"eos_token\" : self . tokenizer . decode ( [ eos_token_id ], skip_special_tokens = False ), \"sep_token\" : self . tokenizer . decode ( [ self . sep_token_id ], skip_special_tokens = False ), \"pad_token\" : self . tokenizer . decode ( [ pad_token_id ], skip_special_tokens = False ), ** { f \"added_token { self . tokenizer . token_to_id ( token ) } \" : token for token in added_tokens }, } self . max_steps = max_steps self . min_length = min_length self . repetition_penalty = repetition_penalty self . trunc_length = trunc_length get_special_tokens () Retrun dict of special tokens to be renderable from template. Source code in npc_engine\\services\\text_generation\\bart.py 159 160 161 def get_special_tokens ( self ) -> Dict [ str , str ]: \"\"\"Retrun dict of special tokens to be renderable from template.\"\"\" return self . special_tokens run ( prompt , temperature = 1.0 , topk = None ) Run text generation from given prompt and parameters. Parameters: Name Type Description Default prompt str Fromatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1.0 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text Source code in npc_engine\\services\\text_generation\\bart.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def run ( self , prompt : str , temperature : float = 1.0 , topk : int = None ) -> str : \"\"\"Run text generation from given prompt and parameters. Args: prompt: Fromatted prompt. temperature: Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness topk: If not none selects top n of predictions to sample from during generation. Returns: Generated text \"\"\" tokens = self . tokenizer . encode ( prompt ) total = np . asarray ( tokens . ids , dtype = np . int64 ) . reshape ([ 1 , - 1 ]) total_enc = self . encoder_model . run ( None , { \"input_ids\" : total })[ 0 ] utterance = np . asarray ([ self . eos_token_id ], dtype = np . int64 ) . reshape ([ 1 , 1 ]) for i in range ( self . max_steps ): o = self . decoder_model . run ( None , { \"encoder_hidden_state\" : total_enc , \"decoder_input_ids\" : utterance }, ) logits = o [ 0 ][ 0 , - 1 , :] if i < self . min_length : logits [ self . eos_token_id ] = float ( \"-inf\" ) if topk is not None : ind = np . argpartition ( logits , - topk )[ - topk :] new_logits = np . zeros ( logits . shape ) new_logits [ ind ] = logits [ ind ] logits = new_logits probs = scp . softmax ( logits / temperature , axis = 0 ) token = np . random . choice ( np . arange ( probs . shape [ 0 ]), p = probs ) token = token . reshape ([ 1 , 1 ]) utterance = np . concatenate ([ utterance , token ], axis = 1 ) if token [ 0 , 0 ] == self . eos_token_id : break return self . tokenizer . decode ( utterance [ 0 , :] . tolist (), skip_special_tokens = True ) string_too_long ( prompt ) Check if prompt is too long for the model. Source code in npc_engine\\services\\text_generation\\bart.py 163 164 165 def string_too_long ( self , prompt ): \"\"\"Check if prompt is too long for the model.\"\"\" return len ( self . tokenizer . encode ( prompt )) > self . trunc_length","title":"BartChatbot"},{"location":"inference_engine/reference/#npc_engine.services.text_generation.hf_text_generation","text":"BART based chatbot implementation.","title":"hf_text_generation"},{"location":"inference_engine/reference/#npc_engine.services.text_generation.hf_text_generation.HfChatbot","text":"Bases: TextGenerationAPI Chatbot that uses Huggingface transformer architectures. ONNX export of Huggingface transformer is required (see https://huggingface.co/docs/transformers/serialization). Features seq2seq-lm, causal-lm, seq2seq-lm-with-past, causal-lm-with-past are supported Source code in npc_engine\\services\\text_generation\\hf_text_generation.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 class HfChatbot ( TextGenerationAPI ): \"\"\"Chatbot that uses Huggingface transformer architectures. ONNX export of Huggingface transformer is required (see https://huggingface.co/docs/transformers/serialization). Features seq2seq-lm, causal-lm, seq2seq-lm-with-past, causal-lm-with-past are supported \"\"\" def __init__ ( self , model_path : str , max_length : int = 100 , min_length : int = 2 , repetition_penalty : float = 1 , trunc_length : int = 512 , * args , ** kwargs , ): \"\"\"Create the chatbot from config args and kwargs. Args: model_path: path to scan for model files (weights and configs) max_length: stop generation at this number of tokens min_length: model can't stop generating text before it's atleast this long in tokens repetition_penalty: probability coef for same tokens to appear multiple times trunc_length: length to truncate model prompt to \"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = rt . GraphOptimizationLevel . ORT_ENABLE_ALL self . model = rt . InferenceSession ( os . path . join ( model_path , \"model.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) self . max_steps = max_length self . min_length = min_length self . repetition_penalty = repetition_penalty special_tokens_map_path = os . path . join ( model_path , \"special_tokens_map.json\" ) with open ( special_tokens_map_path , \"r\" ) as f : self . special_tokens = json . load ( f ) self . eos_token_id = self . tokenizer . encode ( self . special_tokens [ \"eos_token\" ]) . ids [ 0 ] self . model_inputs = self . model . get_inputs () self . is_encdec = ( len ([ i . name for i in self . model_inputs if \"decoder\" in i . name ]) > 0 ) self . with_past = ( len ([ i . name for i in self . model_inputs if \"past_key_values\" in i . name ]) > 0 ) self . shape_dict = { \"batch\" : 1 , \"past_encoder_sequence\" : 0 , \"past_decoder_sequence\" : 0 , \"past_sequence + sequence\" : 0 , } self . dtypes = { i . name : DTYPE_MAP [ i . type ] for i in self . model_inputs } self . trunc_length = trunc_length def run ( self , prompt : str , temperature : float = 1.0 , topk : int = None ) -> str : \"\"\"Run text generation from given prompt and parameters. Args: prompt: Formatted prompt. temperature: Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness topk: If not none selects top n of predictions to sample from during generation. Returns: Generated text \"\"\" inputs = self . create_starter_inputs ( prompt ) utterance = [] for i in range ( self . max_steps ): o = self . model . run ( None , inputs , ) logit = o [ 0 ][ 0 , - 1 , :] if i < self . min_length : logit [ self . eos_token_id ] = float ( \"-inf\" ) token = self . decode_logit ( logit , temperature , topk ) utterance . append ( token ) result_dict = { outp . name : o [ i ] for i , outp in enumerate ( self . model . get_outputs ()) } inputs = self . update_inputs_with_results ( inputs , result_dict , token ) if token == self . eos_token_id : break return self . tokenizer . decode ( utterance , skip_special_tokens = True ) def create_starter_inputs ( self , prompt : str = \"\" ) -> Dict [ str , Any ]: \"\"\"Create starter inputs for the model. Args: prompt: Prompt to start generation from. Returns: Dict of inputs to the model \"\"\" tokens = self . tokenizer . encode ( prompt ) . ids inputs = {} if self . is_encdec : prompt_start = tokens [ - 1 :] inputs [ \"input_ids\" ] = np . asarray ( tokens [: - 1 ], dtype = self . dtypes [ \"input_ids\" ] ) . reshape ([ 1 , - 1 ]) inputs [ \"decoder_input_ids\" ] = np . asarray ( prompt_start , dtype = self . dtypes [ \"decoder_input_ids\" ] ) . reshape ([ 1 , - 1 ]) inputs [ \"attention_mask\" ] = np . ones ( [ 1 , 6 ], dtype = self . dtypes [ \"attention_mask\" ] ) inputs [ \"decoder_attention_mask\" ] = np . ones ( [ 1 , 3 ], dtype = self . dtypes [ \"decoder_attention_mask\" ] ) else : inputs [ \"input_ids\" ] = np . asarray ( tokens , dtype = self . dtypes [ \"input_ids\" ] ) . reshape ([ 1 , - 1 ]) inputs [ \"attention_mask\" ] = np . ones_like ( inputs [ \"input_ids\" ], dtype = self . dtypes [ \"attention_mask\" ] ) if self . with_past : for i in self . model_inputs : if \"past_key_values\" in i . name : shape_tuple = [ self . shape_dict . get ( dim , dim ) for dim in i . shape ] inputs [ i . name ] = np . empty ( shape_tuple , dtype = self . dtypes [ i . name ]) return inputs def decode_logit ( self , logit : np . ndarray , temperature : float , topk : int ) -> int : \"\"\"Decode logit to token. Args: logit: Logit to decode of shape (vocab_size,) Returns: Decoded token of shape \"\"\" if topk is not None : ind = np . argpartition ( logit , - topk )[ - topk :] new_logits = np . zeros ( logit . shape ) new_logits [ ind ] = logit [ ind ] logit = new_logits probs = scp . softmax ( logit / temperature , axis = 0 ) token = np . random . choice ( np . arange ( probs . shape [ 0 ]), p = probs ) token = token . ravel ()[ 0 ] return token def update_inputs_with_results ( self , inputs : Dict [ str , np . ndarray ], results : Dict [ str , np . ndarray ], decoded_token : int , ) -> Dict [ str , np . ndarray ]: \"\"\"Update inputs with results from model. Args: inputs: Inputs to the model results: Results from the model Returns: Updated inputs Finished generation \"\"\" ids_name = \"decoder_input_ids\" if self . is_encdec else \"input_ids\" att_mask_name = \"decoder_attention_mask\" if self . is_encdec else \"attention_mask\" if self . with_past : inputs [ ids_name ] = np . asarray ( [ decoded_token ], dtype = self . dtypes [ ids_name ] ) . reshape ([ 1 , - 1 ]) inputs [ att_mask_name ] = np . ones ( [ 1 , inputs [ att_mask_name ] . shape [ - 1 ] + 1 ], dtype = self . dtypes [ att_mask_name ], ) for inp in self . model_inputs : if \"past_key_values\" in inp . name : inputs [ inp . name ] = results [ inp . name . replace ( \"past_key_values\" , \"present\" ) ] if self . is_encdec : inputs . pop ( \"input_ids\" , None ) else : decoder_input_ids = inputs [ ids_name ] decoder_attention_mask = inputs [ att_mask_name ] decoder_input_ids = np . concatenate ( [ decoder_input_ids , np . asarray ([[ decoded_token ]], dtype = np . int32 )], axis = 1 , ) decoder_attention_mask = np . ones_like ( decoder_input_ids , dtype = self . dtypes [ att_mask_name ] ) inputs [ ids_name ] = decoder_input_ids inputs [ att_mask_name ] = decoder_attention_mask return inputs def get_special_tokens ( self ) -> Dict [ str , str ]: \"\"\"Return dict of special tokens to be renderable from template.\"\"\" return self . special_tokens def string_too_long ( self , prompt ): \"\"\"Check if prompt is too long for the model.\"\"\" return len ( self . tokenizer . encode ( prompt )) > self . trunc_length __init__ ( model_path , max_length = 100 , min_length = 2 , repetition_penalty = 1 , trunc_length = 512 , * args , ** kwargs ) Create the chatbot from config args and kwargs. Parameters: Name Type Description Default model_path str path to scan for model files (weights and configs) required max_length int stop generation at this number of tokens 100 min_length int model can't stop generating text before it's atleast this long in tokens 2 repetition_penalty float probability coef for same tokens to appear multiple times 1 trunc_length int length to truncate model prompt to 512 Source code in npc_engine\\services\\text_generation\\hf_text_generation.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , model_path : str , max_length : int = 100 , min_length : int = 2 , repetition_penalty : float = 1 , trunc_length : int = 512 , * args , ** kwargs , ): \"\"\"Create the chatbot from config args and kwargs. Args: model_path: path to scan for model files (weights and configs) max_length: stop generation at this number of tokens min_length: model can't stop generating text before it's atleast this long in tokens repetition_penalty: probability coef for same tokens to appear multiple times trunc_length: length to truncate model prompt to \"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = rt . SessionOptions () sess_options . graph_optimization_level = rt . GraphOptimizationLevel . ORT_ENABLE_ALL self . model = rt . InferenceSession ( os . path . join ( model_path , \"model.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . tokenizer = Tokenizer . from_file ( os . path . join ( model_path , \"tokenizer.json\" )) self . max_steps = max_length self . min_length = min_length self . repetition_penalty = repetition_penalty special_tokens_map_path = os . path . join ( model_path , \"special_tokens_map.json\" ) with open ( special_tokens_map_path , \"r\" ) as f : self . special_tokens = json . load ( f ) self . eos_token_id = self . tokenizer . encode ( self . special_tokens [ \"eos_token\" ]) . ids [ 0 ] self . model_inputs = self . model . get_inputs () self . is_encdec = ( len ([ i . name for i in self . model_inputs if \"decoder\" in i . name ]) > 0 ) self . with_past = ( len ([ i . name for i in self . model_inputs if \"past_key_values\" in i . name ]) > 0 ) self . shape_dict = { \"batch\" : 1 , \"past_encoder_sequence\" : 0 , \"past_decoder_sequence\" : 0 , \"past_sequence + sequence\" : 0 , } self . dtypes = { i . name : DTYPE_MAP [ i . type ] for i in self . model_inputs } self . trunc_length = trunc_length create_starter_inputs ( prompt = '' ) Create starter inputs for the model. Parameters: Name Type Description Default prompt str Prompt to start generation from. '' Returns: Type Description Dict [ str , Any ] Dict of inputs to the model Source code in npc_engine\\services\\text_generation\\hf_text_generation.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def create_starter_inputs ( self , prompt : str = \"\" ) -> Dict [ str , Any ]: \"\"\"Create starter inputs for the model. Args: prompt: Prompt to start generation from. Returns: Dict of inputs to the model \"\"\" tokens = self . tokenizer . encode ( prompt ) . ids inputs = {} if self . is_encdec : prompt_start = tokens [ - 1 :] inputs [ \"input_ids\" ] = np . asarray ( tokens [: - 1 ], dtype = self . dtypes [ \"input_ids\" ] ) . reshape ([ 1 , - 1 ]) inputs [ \"decoder_input_ids\" ] = np . asarray ( prompt_start , dtype = self . dtypes [ \"decoder_input_ids\" ] ) . reshape ([ 1 , - 1 ]) inputs [ \"attention_mask\" ] = np . ones ( [ 1 , 6 ], dtype = self . dtypes [ \"attention_mask\" ] ) inputs [ \"decoder_attention_mask\" ] = np . ones ( [ 1 , 3 ], dtype = self . dtypes [ \"decoder_attention_mask\" ] ) else : inputs [ \"input_ids\" ] = np . asarray ( tokens , dtype = self . dtypes [ \"input_ids\" ] ) . reshape ([ 1 , - 1 ]) inputs [ \"attention_mask\" ] = np . ones_like ( inputs [ \"input_ids\" ], dtype = self . dtypes [ \"attention_mask\" ] ) if self . with_past : for i in self . model_inputs : if \"past_key_values\" in i . name : shape_tuple = [ self . shape_dict . get ( dim , dim ) for dim in i . shape ] inputs [ i . name ] = np . empty ( shape_tuple , dtype = self . dtypes [ i . name ]) return inputs decode_logit ( logit , temperature , topk ) Decode logit to token. Parameters: Name Type Description Default logit np . ndarray Logit to decode of shape (vocab_size,) required Returns: Type Description int Decoded token of shape Source code in npc_engine\\services\\text_generation\\hf_text_generation.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def decode_logit ( self , logit : np . ndarray , temperature : float , topk : int ) -> int : \"\"\"Decode logit to token. Args: logit: Logit to decode of shape (vocab_size,) Returns: Decoded token of shape \"\"\" if topk is not None : ind = np . argpartition ( logit , - topk )[ - topk :] new_logits = np . zeros ( logit . shape ) new_logits [ ind ] = logit [ ind ] logit = new_logits probs = scp . softmax ( logit / temperature , axis = 0 ) token = np . random . choice ( np . arange ( probs . shape [ 0 ]), p = probs ) token = token . ravel ()[ 0 ] return token get_special_tokens () Return dict of special tokens to be renderable from template. Source code in npc_engine\\services\\text_generation\\hf_text_generation.py 219 220 221 def get_special_tokens ( self ) -> Dict [ str , str ]: \"\"\"Return dict of special tokens to be renderable from template.\"\"\" return self . special_tokens run ( prompt , temperature = 1.0 , topk = None ) Run text generation from given prompt and parameters. Parameters: Name Type Description Default prompt str Formatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1.0 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text Source code in npc_engine\\services\\text_generation\\hf_text_generation.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def run ( self , prompt : str , temperature : float = 1.0 , topk : int = None ) -> str : \"\"\"Run text generation from given prompt and parameters. Args: prompt: Formatted prompt. temperature: Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness topk: If not none selects top n of predictions to sample from during generation. Returns: Generated text \"\"\" inputs = self . create_starter_inputs ( prompt ) utterance = [] for i in range ( self . max_steps ): o = self . model . run ( None , inputs , ) logit = o [ 0 ][ 0 , - 1 , :] if i < self . min_length : logit [ self . eos_token_id ] = float ( \"-inf\" ) token = self . decode_logit ( logit , temperature , topk ) utterance . append ( token ) result_dict = { outp . name : o [ i ] for i , outp in enumerate ( self . model . get_outputs ()) } inputs = self . update_inputs_with_results ( inputs , result_dict , token ) if token == self . eos_token_id : break return self . tokenizer . decode ( utterance , skip_special_tokens = True ) string_too_long ( prompt ) Check if prompt is too long for the model. Source code in npc_engine\\services\\text_generation\\hf_text_generation.py 223 224 225 def string_too_long ( self , prompt ): \"\"\"Check if prompt is too long for the model.\"\"\" return len ( self . tokenizer . encode ( prompt )) > self . trunc_length update_inputs_with_results ( inputs , results , decoded_token ) Update inputs with results from model. Parameters: Name Type Description Default inputs Dict [ str , np . ndarray ] Inputs to the model required results Dict [ str , np . ndarray ] Results from the model required Returns: Type Description Dict [ str , np . ndarray ] Updated inputs Dict [ str , np . ndarray ] Finished generation Source code in npc_engine\\services\\text_generation\\hf_text_generation.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 def update_inputs_with_results ( self , inputs : Dict [ str , np . ndarray ], results : Dict [ str , np . ndarray ], decoded_token : int , ) -> Dict [ str , np . ndarray ]: \"\"\"Update inputs with results from model. Args: inputs: Inputs to the model results: Results from the model Returns: Updated inputs Finished generation \"\"\" ids_name = \"decoder_input_ids\" if self . is_encdec else \"input_ids\" att_mask_name = \"decoder_attention_mask\" if self . is_encdec else \"attention_mask\" if self . with_past : inputs [ ids_name ] = np . asarray ( [ decoded_token ], dtype = self . dtypes [ ids_name ] ) . reshape ([ 1 , - 1 ]) inputs [ att_mask_name ] = np . ones ( [ 1 , inputs [ att_mask_name ] . shape [ - 1 ] + 1 ], dtype = self . dtypes [ att_mask_name ], ) for inp in self . model_inputs : if \"past_key_values\" in inp . name : inputs [ inp . name ] = results [ inp . name . replace ( \"past_key_values\" , \"present\" ) ] if self . is_encdec : inputs . pop ( \"input_ids\" , None ) else : decoder_input_ids = inputs [ ids_name ] decoder_attention_mask = inputs [ att_mask_name ] decoder_input_ids = np . concatenate ( [ decoder_input_ids , np . asarray ([[ decoded_token ]], dtype = np . int32 )], axis = 1 , ) decoder_attention_mask = np . ones_like ( decoder_input_ids , dtype = self . dtypes [ att_mask_name ] ) inputs [ ids_name ] = decoder_input_ids inputs [ att_mask_name ] = decoder_attention_mask return inputs","title":"HfChatbot"},{"location":"inference_engine/reference/#npc_engine.services.text_generation.text_generation_base","text":"Module that implements text generation model API.","title":"text_generation_base"},{"location":"inference_engine/reference/#npc_engine.services.text_generation.text_generation_base.TextGenerationAPI","text":"Bases: BaseService Abstract base class for text generation models. Source code in npc_engine\\services\\text_generation\\text_generation_base.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 class TextGenerationAPI ( BaseService ): \"\"\"Abstract base class for text generation models.\"\"\" API_METHODS : List [ str ] = [ \"generate_reply\" , \"get_prompt_template\" , \"get_special_tokens\" , \"get_context_template\" , ] def __init__ ( self , template_string : str = None , context_template : str = None , history_template : str = None , * args , ** kwargs , ): \"\"\"Initialize prompt formatting variables. Args: template_string: Template string to be rendered as prompt. \"\"\" super () . __init__ ( * args , ** kwargs ) if template_string is not None : logger . warning ( \"Using legacy template string. It might cause context to be cropped and models functioning incorrectly.\" ) self . template_string = template_string self . template = Template ( template_string ) self . legacy = True else : self . template_string = None self . legacy = False self . context_template_string = context_template self . history_template_string = history_template self . context_template = Template ( context_template ) self . history_template = Template ( history_template ) self . initialized = True @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"TextGenerationAPI\" def generate_reply ( self , context : Dict [ str , Any ], * args , ** kwargs ) -> str : \"\"\"Format the model prompt and generate response. Args: context: Prompt context. *args **kwargs Returns: Text response to a prompt. \"\"\" if not self . initialized : raise AssertionError ( \"Can not generate replies before Base Service class was initialized\" ) if self . legacy : prompt = self . template . render ( ** context , ** self . get_special_tokens ()) else : history = context . get ( \"history\" , []) history_prompt = self . history_template . render ( ** context , ** self . get_special_tokens () ) context_prompt = self . context_template . render ( ** context , ** self . get_special_tokens () ) prompt = context_prompt + \"\" . join ( history_prompt ) if isinstance ( history , list ): while self . string_too_long ( prompt ): history . pop ( 0 ) history_prompt = self . history_template . render ( ** context , ** self . get_special_tokens () ) prompt = context_prompt + history_prompt if len ( history ) == 0 : break else : history_prompt = self . history_template . render ( ** context , ** self . get_special_tokens () ) prompt = context_prompt + history_prompt return self . run ( prompt , * args , ** kwargs ) def get_prompt_template ( self ) -> str : \"\"\"Return prompt template string used to render model prompt. Returns: A template string. \"\"\" if self . legacy : return self . template_string else : return self . context_template_string + self . history_template_string def get_context_template ( self ) -> Dict [ str , Any ]: \"\"\"Return context template. Returns: Example context \"\"\" if self . legacy : return schema_to_json ( to_json_schema ( infer ( self . template_string ))) else : context_dict = schema_to_json ( to_json_schema ( infer ( self . context_template_string )) ) history_dict = schema_to_json ( to_json_schema ( infer ( self . history_template_string )) ) combined_dict = {} for key in chain ( context_dict , history_dict ): if key in history_dict and key in context_dict : if context_dict [ key ] != history_dict [ key ]: raise AssertionError ( f \"Context and history templates have different values for { key } \" ) if key in history_dict : combined_dict [ key ] = history_dict [ key ] else : combined_dict [ key ] = context_dict [ key ] return combined_dict @abstractmethod def run ( self , prompt : str , temperature : float = 1 , topk : int = None ) -> str : \"\"\"Abstract method for concrete implementation of generation. Args: prompt: Fromatted prompt. temperature: Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness topk: If not none selects top n of predictions to sample from during generation. Returns: Generated text \"\"\" return None @abstractmethod def string_too_long ( self , prompt : str ) -> bool : \"\"\"Check if prompt is too long. Args: prompt: Prompt to check. Returns: True if prompt is too long, False otherwise. \"\"\" return False @abstractmethod def get_special_tokens ( self ) -> Dict [ str , str ]: \"\"\"Return dictionary mapping for special tokens. To be implemented by child class. Can then be used in template string as fields Returns: Dictionary of special tokens \"\"\" return None __init__ ( template_string = None , context_template = None , history_template = None , * args , ** kwargs ) Initialize prompt formatting variables. Parameters: Name Type Description Default template_string str Template string to be rendered as prompt. None Source code in npc_engine\\services\\text_generation\\text_generation_base.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def __init__ ( self , template_string : str = None , context_template : str = None , history_template : str = None , * args , ** kwargs , ): \"\"\"Initialize prompt formatting variables. Args: template_string: Template string to be rendered as prompt. \"\"\" super () . __init__ ( * args , ** kwargs ) if template_string is not None : logger . warning ( \"Using legacy template string. It might cause context to be cropped and models functioning incorrectly.\" ) self . template_string = template_string self . template = Template ( template_string ) self . legacy = True else : self . template_string = None self . legacy = False self . context_template_string = context_template self . history_template_string = history_template self . context_template = Template ( context_template ) self . history_template = Template ( history_template ) self . initialized = True generate_reply ( context , * args , ** kwargs ) Format the model prompt and generate response. Parameters: Name Type Description Default context Dict [ str , Any ] Prompt context. required Returns: Type Description str Text response to a prompt. Source code in npc_engine\\services\\text_generation\\text_generation_base.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def generate_reply ( self , context : Dict [ str , Any ], * args , ** kwargs ) -> str : \"\"\"Format the model prompt and generate response. Args: context: Prompt context. *args **kwargs Returns: Text response to a prompt. \"\"\" if not self . initialized : raise AssertionError ( \"Can not generate replies before Base Service class was initialized\" ) if self . legacy : prompt = self . template . render ( ** context , ** self . get_special_tokens ()) else : history = context . get ( \"history\" , []) history_prompt = self . history_template . render ( ** context , ** self . get_special_tokens () ) context_prompt = self . context_template . render ( ** context , ** self . get_special_tokens () ) prompt = context_prompt + \"\" . join ( history_prompt ) if isinstance ( history , list ): while self . string_too_long ( prompt ): history . pop ( 0 ) history_prompt = self . history_template . render ( ** context , ** self . get_special_tokens () ) prompt = context_prompt + history_prompt if len ( history ) == 0 : break else : history_prompt = self . history_template . render ( ** context , ** self . get_special_tokens () ) prompt = context_prompt + history_prompt return self . run ( prompt , * args , ** kwargs ) get_api_name () classmethod Get the API name. Source code in npc_engine\\services\\text_generation\\text_generation_base.py 54 55 56 57 @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"TextGenerationAPI\" get_context_template () Return context template. Returns: Type Description Dict [ str , Any ] Example context Source code in npc_engine\\services\\text_generation\\text_generation_base.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def get_context_template ( self ) -> Dict [ str , Any ]: \"\"\"Return context template. Returns: Example context \"\"\" if self . legacy : return schema_to_json ( to_json_schema ( infer ( self . template_string ))) else : context_dict = schema_to_json ( to_json_schema ( infer ( self . context_template_string )) ) history_dict = schema_to_json ( to_json_schema ( infer ( self . history_template_string )) ) combined_dict = {} for key in chain ( context_dict , history_dict ): if key in history_dict and key in context_dict : if context_dict [ key ] != history_dict [ key ]: raise AssertionError ( f \"Context and history templates have different values for { key } \" ) if key in history_dict : combined_dict [ key ] = history_dict [ key ] else : combined_dict [ key ] = context_dict [ key ] return combined_dict get_prompt_template () Return prompt template string used to render model prompt. Returns: Type Description str A template string. Source code in npc_engine\\services\\text_generation\\text_generation_base.py 102 103 104 105 106 107 108 109 110 111 def get_prompt_template ( self ) -> str : \"\"\"Return prompt template string used to render model prompt. Returns: A template string. \"\"\" if self . legacy : return self . template_string else : return self . context_template_string + self . history_template_string get_special_tokens () abstractmethod Return dictionary mapping for special tokens. To be implemented by child class. Can then be used in template string as fields Returns: Type Description Dict [ str , str ] Dictionary of special tokens Source code in npc_engine\\services\\text_generation\\text_generation_base.py 168 169 170 171 172 173 174 175 176 177 @abstractmethod def get_special_tokens ( self ) -> Dict [ str , str ]: \"\"\"Return dictionary mapping for special tokens. To be implemented by child class. Can then be used in template string as fields Returns: Dictionary of special tokens \"\"\" return None run ( prompt , temperature = 1 , topk = None ) abstractmethod Abstract method for concrete implementation of generation. Parameters: Name Type Description Default prompt str Fromatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text Source code in npc_engine\\services\\text_generation\\text_generation_base.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 @abstractmethod def run ( self , prompt : str , temperature : float = 1 , topk : int = None ) -> str : \"\"\"Abstract method for concrete implementation of generation. Args: prompt: Fromatted prompt. temperature: Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness topk: If not none selects top n of predictions to sample from during generation. Returns: Generated text \"\"\" return None string_too_long ( prompt ) abstractmethod Check if prompt is too long. Parameters: Name Type Description Default prompt str Prompt to check. required Returns: Type Description bool True if prompt is too long, False otherwise. Source code in npc_engine\\services\\text_generation\\text_generation_base.py 156 157 158 159 160 161 162 163 164 165 166 @abstractmethod def string_too_long ( self , prompt : str ) -> bool : \"\"\"Check if prompt is too long. Args: prompt: Prompt to check. Returns: True if prompt is too long, False otherwise. \"\"\" return False","title":"TextGenerationAPI"},{"location":"inference_engine/reference/#npc_engine.services.tts","text":"Text to speech specific model implementations. This module implements specific models and wraps them under the common interface for loading and inference. Example from npc_engine.models.tts import TextToSpeechAPI model = TextToSpeechAPI.load(\"path/to/model_dir\") model.run(speaker_id=0, text=\"Hello, world!\")","title":"tts"},{"location":"inference_engine/reference/#npc_engine.services.tts.espnet_onnx","text":"espnet_onnx text to speech inference implementation.","title":"espnet_onnx"},{"location":"inference_engine/reference/#npc_engine.services.tts.espnet_onnx.ESPNetTTS","text":"Bases: TextToSpeechAPI Service implementation for the espnet_onnx text to speech models. Source code in npc_engine\\services\\tts\\espnet_onnx.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class ESPNetTTS ( TextToSpeechAPI ): \"\"\"Service implementation for the espnet_onnx text to speech models.\"\"\" def __init__ ( self , model_path : str , speaker_num : int , * args , ** kwargs ): \"\"\"Create and load espnet_onnx Text2Speech model. Args: model_path: Path to the model directory. speaker_num: Number of speakers model supports. \"\"\" super () . __init__ ( * args , ** kwargs ) provider = self . get_providers () self . t2s = Text2Speech ( model_path , providers = provider ) logging . info ( \"ESPNetTTS using providers {} \" . format ( provider )) if not self . t2s . tts_model . use_sids : self . speaker_ids = range ( speaker_num ) else : self . speaker_ids = list () def get_speaker_ids ( self ) -> List [ str ]: \"\"\"Return available ids of different speakers.\"\"\" return self . speaker_ids def run ( self , speaker_id : str , text : str , n_chunks : int ) -> Iterator [ np . ndarray ]: \"\"\"Create a generator for iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. Returns: Generator that yields next chunk of speech in the form of f32 ndarray. \"\"\" try : speaker_id = int ( speaker_id ) except ValueError : raise ValueError ( \"Speaker id in espnet models must be an integer\" ) if speaker_id not in self . speaker_ids : raise ValueError ( \"Speaker id {} not supported\" . format ( speaker_id )) return iter ([ self . t2s ( text , sids = np . asarray ([ int ( speaker_id )]))[ \"wav\" ]]) __init__ ( model_path , speaker_num , * args , ** kwargs ) Create and load espnet_onnx Text2Speech model. Parameters: Name Type Description Default model_path str Path to the model directory. required speaker_num int Number of speakers model supports. required Source code in npc_engine\\services\\tts\\espnet_onnx.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def __init__ ( self , model_path : str , speaker_num : int , * args , ** kwargs ): \"\"\"Create and load espnet_onnx Text2Speech model. Args: model_path: Path to the model directory. speaker_num: Number of speakers model supports. \"\"\" super () . __init__ ( * args , ** kwargs ) provider = self . get_providers () self . t2s = Text2Speech ( model_path , providers = provider ) logging . info ( \"ESPNetTTS using providers {} \" . format ( provider )) if not self . t2s . tts_model . use_sids : self . speaker_ids = range ( speaker_num ) else : self . speaker_ids = list () get_speaker_ids () Return available ids of different speakers. Source code in npc_engine\\services\\tts\\espnet_onnx.py 28 29 30 def get_speaker_ids ( self ) -> List [ str ]: \"\"\"Return available ids of different speakers.\"\"\" return self . speaker_ids run ( speaker_id , text , n_chunks ) Create a generator for iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Returns: Type Description Iterator [ np . ndarray ] Generator that yields next chunk of speech in the form of f32 ndarray. Source code in npc_engine\\services\\tts\\espnet_onnx.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def run ( self , speaker_id : str , text : str , n_chunks : int ) -> Iterator [ np . ndarray ]: \"\"\"Create a generator for iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. Returns: Generator that yields next chunk of speech in the form of f32 ndarray. \"\"\" try : speaker_id = int ( speaker_id ) except ValueError : raise ValueError ( \"Speaker id in espnet models must be an integer\" ) if speaker_id not in self . speaker_ids : raise ValueError ( \"Speaker id {} not supported\" . format ( speaker_id )) return iter ([ self . t2s ( text , sids = np . asarray ([ int ( speaker_id )]))[ \"wav\" ]])","title":"ESPNetTTS"},{"location":"inference_engine/reference/#npc_engine.services.tts.flowtron","text":"","title":"flowtron"},{"location":"inference_engine/reference/#npc_engine.services.tts.flowtron.flowtron","text":"Flowtron (https://github.com/NVIDIA/flowtron) text to speech inference implementation. FlowtronTTS Bases: TextToSpeechAPI Implements Flowtron architecture inference. Paper: arXiv:2005.05957 Code: https://github.com/NVIDIA/flowtron Onnx export script can be found in this fork https://github.com/npc-engine/flowtron. This model class requires four ONNX models encoder.onnx , backward_flow.onnx , forward_flow.onnx and vocoder.onnx where first three are layers from Flowtron architecture ( flow corresponding to one direction pass of affine coupling layers) and vocoder.onnx is neural vocoder. For detailed specs refer to https://github.com/npc-engine/flowtron. Source code in npc_engine\\services\\tts\\flowtron\\flowtron.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 class FlowtronTTS ( TextToSpeechAPI ): \"\"\"Implements Flowtron architecture inference. Paper: [arXiv:2005.05957](https://arxiv.org/abs/2005.05957) Code: https://github.com/NVIDIA/flowtron Onnx export script can be found in this fork https://github.com/npc-engine/flowtron. This model class requires four ONNX models `encoder.onnx`, `backward_flow.onnx`, `forward_flow.onnx` and `vocoder.onnx` where first three are layers from Flowtron architecture (`flow` corresponding to one direction pass of affine coupling layers) and `vocoder.onnx` is neural vocoder. For detailed specs refer to https://github.com/npc-engine/flowtron. \"\"\" def __init__ ( self , model_path , max_frames = 400 , gate_threshold = 0.5 , sigma = 0.8 , smoothing_window = 3 , smoothing_weight = 0.5 , * args , ** kwargs ): \"\"\"Create and load Flowtron and vocoder models.\"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = onnxruntime . SessionOptions () sess_options . graph_optimization_level = ( onnxruntime . GraphOptimizationLevel . ORT_ENABLE_ALL ) provider = onnxruntime . get_available_providers ()[ 0 ] logging . info ( \"FlowtronTTS using provider {} \" . format ( provider )) self . max_frames = max_frames self . gate_threshold = gate_threshold self . sigma = sigma self . smoothing_window = smoothing_window self . smoothing_weight = smoothing_weight self . encoder = onnxruntime . InferenceSession ( path . join ( model_path , \"encoder.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . backward_flow = onnxruntime . InferenceSession ( path . join ( model_path , \"backward_flow.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . forward_flow = onnxruntime . InferenceSession ( path . join ( model_path , \"forward_flow.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . vocoder = onnxruntime . InferenceSession ( path . join ( model_path , \"vocoder.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . speaker_ids = [ str ( i ) for i in range ( 127 )] self . speaker_ids_map = { idx : i for i , idx in enumerate ( self . speaker_ids )} def get_speaker_ids ( self ) -> List [ str ]: \"\"\"Return available ids of different speakers.\"\"\" return self . speaker_ids def run ( self , speaker_id : str , text : str , n_chunks : int ) -> Iterator [ np . ndarray ]: \"\"\"Create a generator for iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. Returns: Generator that yields next chunk of speech in the form of f32 ndarray. \"\"\" text = self . _get_text ( text ) speaker_id = np . asarray ([[ self . speaker_ids_map [ speaker_id ]]], dtype = np . int64 ) enc_outps_ortvalue = onnxruntime . OrtValue . ortvalue_from_shape_and_type ( [ text . shape [ 1 ], 1 , 640 ], np . float32 , \"cpu\" , 0 ) io_binding = self . encoder . io_binding () io_binding . bind_ortvalue_output ( \"text_emb\" , enc_outps_ortvalue ) io_binding . bind_cpu_input ( \"speaker_vecs\" , speaker_id ) io_binding . bind_cpu_input ( \"text\" , text . reshape ([ 1 , - 1 ])) self . encoder . run_with_iobinding ( io_binding ) residual = np . random . normal ( 0 , self . sigma , size = [ self . max_frames , 1 , 80 ] ) . astype ( np . float32 ) residual = self . _run_backward_flow ( residual , enc_outps_ortvalue ) residual = self . _run_forward_flow ( residual , enc_outps_ortvalue , num_split = self . max_frames // n_chunks ) last_audio = None for residual in residual : residual = np . transpose ( residual , axes = ( 1 , 2 , 0 )) audio = self . vocoder . run ( None , { \"mels\" : residual })[ 0 ] # audio = np.where( # (audio > (audio.mean() - audio.std())) # | (audio < (audio.mean() + audio.std())), # audio, # audio.mean(), # ) tmp = audio if last_audio is None : audio = audio [:, 1000 :] if last_audio is not None : cumsum_vec = np . cumsum ( np . concatenate ([ last_audio , audio ], axis = 1 ), axis = 1 ) ma_vec = ( cumsum_vec [:, self . smoothing_window :] - cumsum_vec [:, : - self . smoothing_window ] ) / self . smoothing_window audio = ( 1 - self . smoothing_weight ) * audio [ :, self . smoothing_window : ] + self . smoothing_weight * ma_vec [:, last_audio . shape [ 1 ] :] last_audio = tmp audio = audio . reshape ( - 1 ) # audio = audio / np.abs(audio).max() yield audio def _get_text ( self , text : str ): text = _clean_text ( text , [ \"flowtron_cleaners\" ]) words = re . findall ( r \"\\S*\\{.*?\\}\\S*|\\S+\" , text ) text = \" \" . join ( words ) text_norm = np . asarray ( text_to_sequence ( text ), dtype = np . int64 ) . reshape ([ 1 , - 1 ]) return text_norm def _run_backward_flow ( self , residual , enc_outps_ortvalue ): residual_o , hidden_att , hidden_lstm = self . _init_states ( residual ) hidden_att_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 0 ], \"cpu\" , 0 ) hidden_att_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 1 ], \"cpu\" , 0 ) hidden_lstm_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 0 ], \"cpu\" , 0 ) hidden_lstm_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 1 ], \"cpu\" , 0 ) hidden_att_o_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 0 ], \"cpu\" , 0 ) hidden_att_o_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 1 ], \"cpu\" , 0 ) hidden_lstm_o_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 0 ], \"cpu\" , 0 ) hidden_lstm_o_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 1 ], \"cpu\" , 0 ) residual_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( residual_o , \"cpu\" , 0 ) residual_outp = [ residual_ortvalue ] for i in range ( residual . shape [ 0 ] - 1 , - 1 , - 1 ): io_binding = self . backward_flow . io_binding () io_binding . bind_cpu_input ( \"residual\" , residual [ i ]) io_binding . bind_ortvalue_input ( \"text\" , enc_outps_ortvalue ) io_binding . bind_ortvalue_input ( \"last_output\" , residual_outp [ 0 ]) io_binding . bind_ortvalue_input ( \"hidden_att\" , hidden_att_ortvalue ) io_binding . bind_ortvalue_input ( \"hidden_att_c\" , hidden_att_c_ortvalue ) io_binding . bind_ortvalue_input ( \"hidden_lstm\" , hidden_lstm_ortvalue ) io_binding . bind_ortvalue_input ( \"hidden_lstm_c\" , hidden_lstm_c_ortvalue ) io_binding . bind_output ( \"output\" , \"cpu\" ) io_binding . bind_output ( \"gate\" , \"cpu\" ) io_binding . bind_ortvalue_output ( \"hidden_att_o\" , hidden_att_o_ortvalue ) io_binding . bind_ortvalue_output ( \"hidden_att_o_c\" , hidden_att_o_c_ortvalue ) io_binding . bind_ortvalue_output ( \"hidden_lstm_o\" , hidden_lstm_o_ortvalue ) io_binding . bind_ortvalue_output ( \"hidden_lstm_o_c\" , hidden_lstm_o_c_ortvalue ) self . backward_flow . run_with_iobinding ( io_binding ) outp = io_binding . get_outputs () gates = outp [ 1 ] . numpy () residual_outp = [ outp [ 0 ]] + residual_outp if ( gates > self . gate_threshold ) . any (): break # Switch input and output to use latest output as input ( hidden_att_ortvalue , hidden_att_o_ortvalue ) = ( hidden_att_o_ortvalue , hidden_att_ortvalue , ) ( hidden_att_c_ortvalue , hidden_att_o_c_ortvalue ) = ( hidden_att_o_c_ortvalue , hidden_att_c_ortvalue , ) ( hidden_lstm_ortvalue , hidden_lstm_o_ortvalue ) = ( hidden_lstm_o_ortvalue , hidden_lstm_ortvalue , ) ( hidden_lstm_c_ortvalue , hidden_lstm_o_c_ortvalue ) = ( hidden_lstm_o_c_ortvalue , hidden_lstm_c_ortvalue , ) residual = np . concatenate ( [ residual_ort . numpy () for residual_ort in residual_outp ], axis = 0 ) return residual def _run_forward_flow ( self , residual , enc_outps_ortvalue , num_split ): residual_o , hidden_att , hidden_lstm = self . _init_states ( residual ) hidden_att_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 0 ], \"cpu\" , 0 ) hidden_att_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 1 ], \"cpu\" , 0 ) hidden_lstm_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 0 ], \"cpu\" , 0 ) hidden_lstm_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 1 ], \"cpu\" , 0 ) hidden_att_o_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 0 ], \"cpu\" , 0 ) hidden_att_o_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_att [ 1 ], \"cpu\" , 0 ) hidden_lstm_o_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 0 ], \"cpu\" , 0 ) hidden_lstm_o_c_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( hidden_lstm [ 1 ], \"cpu\" , 0 ) residual_ortvalue = onnxruntime . OrtValue . ortvalue_from_numpy ( residual_o , \"cpu\" , 0 ) residual_outp = [ residual_ortvalue ] last_output = residual_ortvalue for i in range ( residual . shape [ 0 ]): io_binding = self . forward_flow . io_binding () io_binding . bind_cpu_input ( \"residual\" , residual [ i ]) io_binding . bind_ortvalue_input ( \"text\" , enc_outps_ortvalue ) io_binding . bind_ortvalue_input ( \"last_output\" , last_output ) io_binding . bind_ortvalue_input ( \"hidden_att\" , hidden_att_ortvalue ) io_binding . bind_ortvalue_input ( \"hidden_att_c\" , hidden_att_c_ortvalue ) io_binding . bind_ortvalue_input ( \"hidden_lstm\" , hidden_lstm_ortvalue ) io_binding . bind_ortvalue_input ( \"hidden_lstm_c\" , hidden_lstm_c_ortvalue ) io_binding . bind_output ( \"output\" , \"cpu\" ) io_binding . bind_output ( \"gate\" , \"cpu\" ) io_binding . bind_ortvalue_output ( \"hidden_att_o\" , hidden_att_o_ortvalue ) io_binding . bind_ortvalue_output ( \"hidden_att_o_c\" , hidden_att_o_c_ortvalue ) io_binding . bind_ortvalue_output ( \"hidden_lstm_o\" , hidden_lstm_o_ortvalue ) io_binding . bind_ortvalue_output ( \"hidden_lstm_o_c\" , hidden_lstm_o_c_ortvalue ) self . forward_flow . run_with_iobinding ( io_binding ) outp = io_binding . get_outputs () gates = outp [ 1 ] . numpy () residual_outp . append ( outp [ 0 ]) last_output = outp [ 0 ] if ( gates > self . gate_threshold ) . any (): break # Switch input and output to use latest output as input ( hidden_att_ortvalue , hidden_att_o_ortvalue ) = ( hidden_att_o_ortvalue , hidden_att_ortvalue , ) ( hidden_att_c_ortvalue , hidden_att_o_c_ortvalue ) = ( hidden_att_o_c_ortvalue , hidden_att_c_ortvalue , ) ( hidden_lstm_ortvalue , hidden_lstm_o_ortvalue ) = ( hidden_lstm_o_ortvalue , hidden_lstm_ortvalue , ) ( hidden_lstm_c_ortvalue , hidden_lstm_o_c_ortvalue ) = ( hidden_lstm_o_c_ortvalue , hidden_lstm_c_ortvalue , ) if len ( residual_outp ) % num_split == 0 and i != 0 : residual_o = np . concatenate ( [ residual_ort . numpy () for residual_ort in residual_outp ], axis = 0 ) yield residual_o residual_outp = [] if len ( residual_outp ) > 0 : residual_o = np . concatenate ( [ residual_ort . numpy () for residual_ort in residual_outp ], axis = 0 ) yield residual_o def _init_states ( self , residual ): last_outputs = np . zeros ( [ 1 , residual . shape [ 1 ], residual . shape [ 2 ]], dtype = np . float32 ) hidden_att = [ np . zeros ([ 1 , 1 , 1024 ], dtype = np . float32 ), np . zeros ([ 1 , 1 , 1024 ], dtype = np . float32 ), ] hidden_lstm = [ np . zeros ([ 2 , 1 , 1024 ], dtype = np . float32 ), np . zeros ([ 2 , 1 , 1024 ], dtype = np . float32 ), ] return last_outputs , hidden_att , hidden_lstm __init__ ( model_path , max_frames = 400 , gate_threshold = 0.5 , sigma = 0.8 , smoothing_window = 3 , smoothing_weight = 0.5 , * args , ** kwargs ) Create and load Flowtron and vocoder models. Source code in npc_engine\\services\\tts\\flowtron\\flowtron.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def __init__ ( self , model_path , max_frames = 400 , gate_threshold = 0.5 , sigma = 0.8 , smoothing_window = 3 , smoothing_weight = 0.5 , * args , ** kwargs ): \"\"\"Create and load Flowtron and vocoder models.\"\"\" super () . __init__ ( * args , ** kwargs ) sess_options = onnxruntime . SessionOptions () sess_options . graph_optimization_level = ( onnxruntime . GraphOptimizationLevel . ORT_ENABLE_ALL ) provider = onnxruntime . get_available_providers ()[ 0 ] logging . info ( \"FlowtronTTS using provider {} \" . format ( provider )) self . max_frames = max_frames self . gate_threshold = gate_threshold self . sigma = sigma self . smoothing_window = smoothing_window self . smoothing_weight = smoothing_weight self . encoder = onnxruntime . InferenceSession ( path . join ( model_path , \"encoder.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . backward_flow = onnxruntime . InferenceSession ( path . join ( model_path , \"backward_flow.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . forward_flow = onnxruntime . InferenceSession ( path . join ( model_path , \"forward_flow.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . vocoder = onnxruntime . InferenceSession ( path . join ( model_path , \"vocoder.onnx\" ), providers = self . get_providers (), sess_options = sess_options , ) self . speaker_ids = [ str ( i ) for i in range ( 127 )] self . speaker_ids_map = { idx : i for i , idx in enumerate ( self . speaker_ids )} get_speaker_ids () Return available ids of different speakers. Source code in npc_engine\\services\\tts\\flowtron\\flowtron.py 81 82 83 def get_speaker_ids ( self ) -> List [ str ]: \"\"\"Return available ids of different speakers.\"\"\" return self . speaker_ids run ( speaker_id , text , n_chunks ) Create a generator for iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Returns: Type Description Iterator [ np . ndarray ] Generator that yields next chunk of speech in the form of f32 ndarray. Source code in npc_engine\\services\\tts\\flowtron\\flowtron.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def run ( self , speaker_id : str , text : str , n_chunks : int ) -> Iterator [ np . ndarray ]: \"\"\"Create a generator for iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. Returns: Generator that yields next chunk of speech in the form of f32 ndarray. \"\"\" text = self . _get_text ( text ) speaker_id = np . asarray ([[ self . speaker_ids_map [ speaker_id ]]], dtype = np . int64 ) enc_outps_ortvalue = onnxruntime . OrtValue . ortvalue_from_shape_and_type ( [ text . shape [ 1 ], 1 , 640 ], np . float32 , \"cpu\" , 0 ) io_binding = self . encoder . io_binding () io_binding . bind_ortvalue_output ( \"text_emb\" , enc_outps_ortvalue ) io_binding . bind_cpu_input ( \"speaker_vecs\" , speaker_id ) io_binding . bind_cpu_input ( \"text\" , text . reshape ([ 1 , - 1 ])) self . encoder . run_with_iobinding ( io_binding ) residual = np . random . normal ( 0 , self . sigma , size = [ self . max_frames , 1 , 80 ] ) . astype ( np . float32 ) residual = self . _run_backward_flow ( residual , enc_outps_ortvalue ) residual = self . _run_forward_flow ( residual , enc_outps_ortvalue , num_split = self . max_frames // n_chunks ) last_audio = None for residual in residual : residual = np . transpose ( residual , axes = ( 1 , 2 , 0 )) audio = self . vocoder . run ( None , { \"mels\" : residual })[ 0 ] # audio = np.where( # (audio > (audio.mean() - audio.std())) # | (audio < (audio.mean() + audio.std())), # audio, # audio.mean(), # ) tmp = audio if last_audio is None : audio = audio [:, 1000 :] if last_audio is not None : cumsum_vec = np . cumsum ( np . concatenate ([ last_audio , audio ], axis = 1 ), axis = 1 ) ma_vec = ( cumsum_vec [:, self . smoothing_window :] - cumsum_vec [:, : - self . smoothing_window ] ) / self . smoothing_window audio = ( 1 - self . smoothing_weight ) * audio [ :, self . smoothing_window : ] + self . smoothing_weight * ma_vec [:, last_audio . shape [ 1 ] :] last_audio = tmp audio = audio . reshape ( - 1 ) # audio = audio / np.abs(audio).max() yield audio","title":"flowtron"},{"location":"inference_engine/reference/#npc_engine.services.tts.flowtron.text","text":"from https://github.com/keithito/tacotron cleaners adapted from https://github.com/keithito/tacotron. Cleaners are transformations that run over the input text at both training and eval time. Cleaners can be selected by passing a comma-delimited list of cleaner names as the \"cleaners\" hyperparameter. Some cleaners are English-specific. You'll typically want to use: 1. \"english_cleaners\" for English text 2. \"transliteration_cleaners\" for non-English text that can be transliterated to ASCII using the Unidecode library (https://pypi.python.org/pypi/Unidecode) 3. \"basic_cleaners\" if you do not want to transliterate (in this case, you should also update the symbols in symbols.py to match your data). flowtron_cleaners ( text ) Clean text with a set of cleaners. Source code in npc_engine\\services\\tts\\flowtron\\text\\cleaners.py 98 99 100 101 102 103 104 105 def flowtron_cleaners ( text ): \"\"\"Clean text with a set of cleaners.\"\"\" text = collapse_whitespace ( text ) text = remove_hyphens ( text ) text = expand_datestime ( text ) text = expand_numbers ( text ) text = expand_safe_abbreviations ( text ) return text numbers from https://github.com/keithito/tacotron symbols from https://github.com/keithito/tacotron","title":"text"},{"location":"inference_engine/reference/#npc_engine.services.tts.tts_base","text":"Module that implements text to speech model API.","title":"tts_base"},{"location":"inference_engine/reference/#npc_engine.services.tts.tts_base.TextToSpeechAPI","text":"Bases: BaseService Abstract base class for text-to-speech models. Source code in npc_engine\\services\\tts\\tts_base.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 class TextToSpeechAPI ( BaseService ): \"\"\"Abstract base class for text-to-speech models.\"\"\" #: Methods that are going to be exposed as services. API_METHODS : List [ str ] = [ \"tts_start\" , \"tts_get_results\" , \"get_speaker_ids\" ] def __init__ ( self , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" self . generator = None super () . __init__ ( * args , ** kwargs ) self . initialized = True @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"TextToSpeechAPI\" def tts_start ( self , speaker_id : str , text : str , n_chunks : int ) -> None : \"\"\"Initiate iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. \"\"\" sentences = re . split ( r \"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\" , text ) self . generator = self . _chain_run ( speaker_id , sentences , n_chunks ) def _chain_run ( self , speaker_id , sentences , n_chunks ) -> Iterable [ np . ndarray ]: \"\"\"Chain the run method to be used in the generator.\"\"\" for sentence in sentences : sentence = sentence . strip () if sentence != \"\" : yield from self . run ( speaker_id , sentence , n_chunks ) def tts_get_results ( self ) -> Iterable [ np . ndarray ]: \"\"\"Retrieve the next chunk of generated speech. Returns: Next chunk of speech in the form of f32 ndarray. \"\"\" if self . generator is not None : return next ( self . generator ) . tolist () else : raise ValueError ( \"Speech generation was not started. Use tts_start to start it\" ) @abstractmethod def run ( self , speaker_id : str , text : str , n_chunks : int ) -> Iterable [ np . ndarray ]: \"\"\"Create a generator for iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. Returns: Generator that yields next chunk of speech in the form of f32 ndarray. \"\"\" return None @abstractmethod def get_speaker_ids ( self ) -> List [ str ]: \"\"\"Get list of available speaker ids. Returns: The return value. True for success, False otherwise. \"\"\" return None __init__ ( * args , ** kwargs ) Empty initialization method for API to be similar to other model base classes. Source code in npc_engine\\services\\tts\\tts_base.py 16 17 18 19 20 def __init__ ( self , * args , ** kwargs ) -> None : \"\"\"Empty initialization method for API to be similar to other model base classes.\"\"\" self . generator = None super () . __init__ ( * args , ** kwargs ) self . initialized = True _chain_run ( speaker_id , sentences , n_chunks ) Chain the run method to be used in the generator. Source code in npc_engine\\services\\tts\\tts_base.py 39 40 41 42 43 44 def _chain_run ( self , speaker_id , sentences , n_chunks ) -> Iterable [ np . ndarray ]: \"\"\"Chain the run method to be used in the generator.\"\"\" for sentence in sentences : sentence = sentence . strip () if sentence != \"\" : yield from self . run ( speaker_id , sentence , n_chunks ) get_api_name () classmethod Get the API name. Source code in npc_engine\\services\\tts\\tts_base.py 22 23 24 25 @classmethod def get_api_name ( cls ) -> str : \"\"\"Get the API name.\"\"\" return \"TextToSpeechAPI\" get_speaker_ids () abstractmethod Get list of available speaker ids. Returns: Type Description List [ str ] The return value. True for success, False otherwise. Source code in npc_engine\\services\\tts\\tts_base.py 73 74 75 76 77 78 79 80 @abstractmethod def get_speaker_ids ( self ) -> List [ str ]: \"\"\"Get list of available speaker ids. Returns: The return value. True for success, False otherwise. \"\"\" return None run ( speaker_id , text , n_chunks ) abstractmethod Create a generator for iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Returns: Type Description Iterable [ np . ndarray ] Generator that yields next chunk of speech in the form of f32 ndarray. Source code in npc_engine\\services\\tts\\tts_base.py 59 60 61 62 63 64 65 66 67 68 69 70 71 @abstractmethod def run ( self , speaker_id : str , text : str , n_chunks : int ) -> Iterable [ np . ndarray ]: \"\"\"Create a generator for iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. Returns: Generator that yields next chunk of speech in the form of f32 ndarray. \"\"\" return None tts_get_results () Retrieve the next chunk of generated speech. Returns: Type Description Iterable [ np . ndarray ] Next chunk of speech in the form of f32 ndarray. Source code in npc_engine\\services\\tts\\tts_base.py 46 47 48 49 50 51 52 53 54 55 56 57 def tts_get_results ( self ) -> Iterable [ np . ndarray ]: \"\"\"Retrieve the next chunk of generated speech. Returns: Next chunk of speech in the form of f32 ndarray. \"\"\" if self . generator is not None : return next ( self . generator ) . tolist () else : raise ValueError ( \"Speech generation was not started. Use tts_start to start it\" ) tts_start ( speaker_id , text , n_chunks ) Initiate iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Source code in npc_engine\\services\\tts\\tts_base.py 27 28 29 30 31 32 33 34 35 36 37 def tts_start ( self , speaker_id : str , text : str , n_chunks : int ) -> None : \"\"\"Initiate iterative generation of speech. Args: speaker_id: Id of the speaker. text: Text to generate speech from. n_chunks: Number of chunks to split generation into. \"\"\" sentences = re . split ( r \"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\" , text ) self . generator = self . _chain_run ( speaker_id , sentences , n_chunks )","title":"TextToSpeechAPI"},{"location":"inference_engine/reference/#npc_engine.services.utils","text":"Utility methods for model handling.","title":"utils"},{"location":"inference_engine/reference/#npc_engine.services.utils.config","text":"Config related functions.","title":"config"},{"location":"inference_engine/reference/#npc_engine.services.utils.config.get_model_type_name","text":"Get model type name. Parameters: Name Type Description Default models_path str Path to the models folder. required model_id str Model id (dirname). required Returns: Type Description str Model type name. Source code in npc_engine\\services\\utils\\config.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def get_model_type_name ( models_path : str , model_id : str ) -> str : \"\"\"Get model type name. Args: models_path: Path to the models folder. model_id: Model id (dirname). Returns: Model type name. \"\"\" model_path = os . path . join ( models_path , model_id ) config_path = os . path . join ( model_path , \"config.yml\" ) with open ( config_path , \"r\" ) as f : config = yaml . safe_load ( f ) return get_type_from_dict ( config )","title":"get_model_type_name()"},{"location":"inference_engine/reference/#npc_engine.services.utils.config.get_type_from_dict","text":"Get model type from config dict. Parameters: Name Type Description Default config_dict dict Config dict. required Returns: Type Description str Model type. Source code in npc_engine\\services\\utils\\config.py 8 9 10 11 12 13 14 15 16 17 def get_type_from_dict ( config_dict : dict ) -> str : \"\"\"Get model type from config dict. Args: config_dict: Config dict. Returns: Model type. \"\"\" return config_dict . get ( \"model_type\" , config_dict . get ( \"type\" , \"\" ))","title":"get_type_from_dict()"},{"location":"inference_engine/reference/#npc_engine.services.utils.config.validate_hub_model","text":"Validate huggingface hub model by id. Parameters: Name Type Description Default models_path str Path to the models folder. required model_id str Huggingface hub model id. required Source code in npc_engine\\services\\utils\\config.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def validate_hub_model ( models_path : str , model_id : str ) -> bool : \"\"\"Validate huggingface hub model by id. Args: models_path: Path to the models folder. model_id: Huggingface hub model id. \"\"\" tmp_model_path = os . path . join ( models_path , model_id . replace ( \"/\" , \"_\" )) model_correct = True try : try : hf_hub_download ( repo_id = model_id , filename = \"config.yml\" , cache_dir = tmp_model_path , force_filename = \"config.yml\" , ) except HTTPError : return False config_path = os . path . join ( models_path , model_id . replace ( \"/\" , \"_\" ), \"config.yml\" ) with open ( config_path ) as f : config_dict = yaml . load ( f , Loader = yaml . Loader ) if \"model_type\" not in config_dict : model_correct = False if os . path . exists ( config_path ): os . remove ( config_path ) except ValueError : model_correct = False if os . path . exists ( tmp_model_path ): os . rmdir ( tmp_model_path ) return model_correct","title":"validate_hub_model()"},{"location":"inference_engine/reference/#npc_engine.services.utils.config.validate_local_model","text":"Validate local model by id. Parameters: Name Type Description Default models_path str Path to the models folder. required model_id str Model id. required Source code in npc_engine\\services\\utils\\config.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def validate_local_model ( models_path : str , model_id : str ) -> bool : \"\"\"Validate local model by id. Args: models_path: Path to the models folder. model_id: Model id. \"\"\" model_correct = True model_path = os . path . join ( models_path , model_id ) if not os . path . exists ( model_path ): model_correct = False else : try : _ = get_model_type_name ( models_path , model_id ) except FileNotFoundError : model_correct = False return model_correct","title":"validate_local_model()"},{"location":"inference_engine/reference/#npc_engine.services.utils.config.validate_model","text":"Validate model by id. Parameters: Name Type Description Default models_path str Path to the models folder. required model_id str Model id. required Source code in npc_engine\\services\\utils\\config.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def validate_model ( models_path : str , model_id : str ) -> bool : \"\"\"Validate model by id. Args: models_path: Path to the models folder. model_id: Model id. \"\"\" model_correct = True model_path = os . path . join ( models_path , model_id ) if not os . path . exists ( model_path ): model_correct = validate_hub_model ( models_path , model_id ) else : model_correct = validate_local_model ( models_path , model_id ) return model_correct","title":"validate_model()"},{"location":"inference_engine/reference/#npc_engine.services.utils.lru_cache","text":"LRU cache.","title":"lru_cache"},{"location":"inference_engine/reference/#npc_engine.services.utils.lru_cache.NumpyLRUCache","text":"Dict based LRU cache for numpy arrays. Source code in npc_engine\\services\\utils\\lru_cache.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class NumpyLRUCache : \"\"\"Dict based LRU cache for numpy arrays.\"\"\" def __init__ ( self , size ): \"\"\"Crate cache.\"\"\" self . size = size self . lru_cache = collections . OrderedDict () self . common_dim = None def _get ( self , key : Any , default = None ) -> np . ndarray : try : value = self . lru_cache . pop ( key ) self . lru_cache [ key ] = value return value except KeyError : return default def _put ( self , key : Any , value : np . ndarray ): try : self . lru_cache . pop ( key ) except KeyError : if len ( self . lru_cache ) >= self . size : self . lru_cache . popitem ( last = False ) self . lru_cache [ key ] = value def cache_compute ( self , keys : List [ Any ], function : Callable ) -> Tuple [ np . ndarray , List [ Any ]]: \"\"\"Get batch from cache and compute missing. Args: keys: List of keys Returns: np.ndarray or None: Found entries concatenated over 0 axis. list(_) or None: Keys that were not found. \"\"\" if len ( self . lru_cache ) == 0 : result = function ( keys ) self . put_batch ( keys , result ) return result else : result = np . zeros (( len ( keys ), * self . common_dim )) items = [ self . _get ( key ) for key in keys ] not_found = [ key for item , key in zip ( items , keys ) if item is None ] if len ( not_found ) > 0 : computed = function ( not_found ) computed_idx = 0 for idx , item in enumerate ( items ): if item is None : result_slc = tuple ([ idx ] + [ slice ( None )] * len ( self . common_dim )) computed_slc = tuple ( [ computed_idx ] + [ slice ( None )] * len ( self . common_dim ) ) result [ result_slc ] = computed [ computed_slc ] computed_idx += 1 else : result_slc = tuple ([ idx ] + [ slice ( None )] * len ( self . common_dim )) result [ result_slc ] = item return result def put_batch ( self , keys : List [ Any ], values : np . ndarray ): \"\"\"Put batch to cache. Args: keys: List of keys values: Ndarray of shape (len(keys), *common_dim) \"\"\" self . _validate_shape ( values ) for key , item in zip ( keys , values ): self . _put ( key , item ) def _validate_shape ( self , value ): if self . common_dim is None : self . common_dim = value . shape [ 1 :] else : if self . common_dim != value . shape [ 1 :]: raise ValueError ( f \"\"\"Cached arrays must have the same shape. Shape expected: { self . common_dim } Shape found: { value . shape [ 1 :] } \"\"\" ) __init__ ( size ) Crate cache. Source code in npc_engine\\services\\utils\\lru_cache.py 10 11 12 13 14 def __init__ ( self , size ): \"\"\"Crate cache.\"\"\" self . size = size self . lru_cache = collections . OrderedDict () self . common_dim = None cache_compute ( keys , function ) Get batch from cache and compute missing. Parameters: Name Type Description Default keys List [ Any ] List of keys required Returns: Type Description np . ndarray np.ndarray or None: Found entries concatenated over 0 axis. List [ Any ] list(_) or None: Keys that were not found. Source code in npc_engine\\services\\utils\\lru_cache.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def cache_compute ( self , keys : List [ Any ], function : Callable ) -> Tuple [ np . ndarray , List [ Any ]]: \"\"\"Get batch from cache and compute missing. Args: keys: List of keys Returns: np.ndarray or None: Found entries concatenated over 0 axis. list(_) or None: Keys that were not found. \"\"\" if len ( self . lru_cache ) == 0 : result = function ( keys ) self . put_batch ( keys , result ) return result else : result = np . zeros (( len ( keys ), * self . common_dim )) items = [ self . _get ( key ) for key in keys ] not_found = [ key for item , key in zip ( items , keys ) if item is None ] if len ( not_found ) > 0 : computed = function ( not_found ) computed_idx = 0 for idx , item in enumerate ( items ): if item is None : result_slc = tuple ([ idx ] + [ slice ( None )] * len ( self . common_dim )) computed_slc = tuple ( [ computed_idx ] + [ slice ( None )] * len ( self . common_dim ) ) result [ result_slc ] = computed [ computed_slc ] computed_idx += 1 else : result_slc = tuple ([ idx ] + [ slice ( None )] * len ( self . common_dim )) result [ result_slc ] = item return result put_batch ( keys , values ) Put batch to cache. Parameters: Name Type Description Default keys List [ Any ] List of keys required values np . ndarray Ndarray of shape (len(keys), *common_dim) required Source code in npc_engine\\services\\utils\\lru_cache.py 68 69 70 71 72 73 74 75 76 77 def put_batch ( self , keys : List [ Any ], values : np . ndarray ): \"\"\"Put batch to cache. Args: keys: List of keys values: Ndarray of shape (len(keys), *common_dim) \"\"\" self . _validate_shape ( values ) for key , item in zip ( keys , values ): self . _put ( key , item )","title":"NumpyLRUCache"},{"location":"inference_engine/reference/#npc_engine.services.utils.mock","text":"Utility script to mock models for testing.","title":"mock"},{"location":"inference_engine/reference/#npc_engine.services.utils.mock.build_output_shape_tensor_","text":"Build output shape tensor for dynamic shape models. Parameters: Name Type Description Default graph Graph to add the output shape tensor to. required output Model output. required dynamic_shape_map Map of input names to their dynamic shape indices. required shape_name Name of the output shape tensor. 'dynamic_shape' Source code in npc_engine\\services\\utils\\mock.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def build_output_shape_tensor_ ( graph , output , dynamic_shape_map , shape_name = \"dynamic_shape\" , override = {} ): \"\"\"Build output shape tensor for dynamic shape models. Args: graph: Graph to add the output shape tensor to. output: Model output. dynamic_shape_map: Map of input names to their dynamic shape indices. shape_name: Name of the output shape tensor. \"\"\" dimensions_retrieved = [] for i , dim in enumerate ( output . type . tensor_type . shape . dim ): if \" + \" in dim . dim_param : dim1 , dim2 = dim . dim_param . split ( \" + \" ) create_dim_variable_ ( graph , shape_name , dim1 , override . get ( dim1 , dim . dim_value ), i , dynamic_shape_map , postfix = \"1\" , ) create_dim_variable_ ( graph , shape_name , dim2 , override . get ( dim2 , dim . dim_value ), i , dynamic_shape_map , postfix = \"2\" , ) so . add_node ( graph , so . node ( \"Add\" , inputs = [ f \" { shape_name } _ { i } _1\" , f \" { shape_name } _ { i } _2\" ], outputs = [ f \" { shape_name } _ { i } \" ], ), ) else : create_dim_variable_ ( graph , shape_name , dim . dim_param , override . get ( dim . dim_param , dim . dim_value ), i , dynamic_shape_map , ) dimensions_retrieved . append ( f \" { shape_name } _ { i } \" ) node = so . node ( \"Concat\" , inputs = dimensions_retrieved , outputs = [ f \" { shape_name } \" ], axis = 0 , ) so . add_node ( graph , node )","title":"build_output_shape_tensor_()"},{"location":"inference_engine/reference/#npc_engine.services.utils.mock.create_dim_variable_","text":"Create a dimension variable for a dynamic shape model. Parameters: Name Type Description Default graph Graph to add the dimension variable to. required shape_name Name of the output shape tensor. required dim_param Dimension parameter name. required dim_value Dimension value. required dim_id Index of the dimension variable. required dynamic_shape_map Map of dynamic axes names to their inputs indices. required Source code in npc_engine\\services\\utils\\mock.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def create_dim_variable_ ( graph , shape_name , dim_param , dim_value , dim_id , dynamic_shape_map , postfix = None ): \"\"\"Create a dimension variable for a dynamic shape model. Args: graph: Graph to add the dimension variable to. shape_name: Name of the output shape tensor. dim_param: Dimension parameter name. dim_value: Dimension value. dim_id: Index of the dimension variable. dynamic_shape_map: Map of dynamic axes names to their inputs indices. \"\"\" if dim_param != \"\" and dim_param in dynamic_shape_map : node1 = so . node ( \"Shape\" , inputs = [ dynamic_shape_map [ dim_param ][ 0 ]], outputs = [ f \" { shape_name } _ { dim_id } \" if postfix is None else f \" { shape_name } _ { dim_id } _ { postfix } \" ], start = dynamic_shape_map [ dim_param ][ 1 ], end = dynamic_shape_map [ dim_param ][ 1 ] + 1 , ) so . add_node ( graph , node1 ) else : so . add_constant ( graph , f \" { shape_name } _ { dim_id } \" if postfix is None else f \" { shape_name } _ { dim_id } _ { postfix } \" , np . array ( [ dim_value if dim_value != 0 else 1 ], dtype = np . int64 , ), data_type = \"INT64\" , )","title":"create_dim_variable_()"},{"location":"inference_engine/reference/#npc_engine.services.utils.mock.create_stub_onnx_model","text":"Create stub onnx model for tests with correct input and output shapes and names. Parameters: Name Type Description Default onnx_model_path str Path to the onnx model. required output_path str Path to the output mock model. required dynamic_shape_values dict Map to specify dynamic dimension values. required Source code in npc_engine\\services\\utils\\mock.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def create_stub_onnx_model ( onnx_model_path : str , output_path : str , dynamic_shape_values : dict ): \"\"\"Create stub onnx model for tests with correct input and output shapes and names. Args: onnx_model_path: Path to the onnx model. output_path: Path to the output mock model. dynamic_shape_values: Map to specify dynamic dimension values. \"\"\" onnx_model = so . graph_from_file ( onnx_model_path ) inputs = onnx_model . input outputs = onnx_model . output mock_graph = so . empty_graph () inverse_data_dict = { value : key for key , value in glob . DATA_TYPES . items ()} dynamic_shape_map = {} for input_ in inputs : so . add_input ( mock_graph , name = input_ . name , dimensions = [ dim . dim_param if dim . dim_param != \"\" else dim . dim_value for dim in input_ . type . tensor_type . shape . dim ], data_type = inverse_data_dict [ input_ . type . tensor_type . elem_type ], ) for i , dim in enumerate ( input_ . type . tensor_type . shape . dim ): if dim . dim_param != \"\" : dynamic_shape_map [ dim . dim_param ] = ( input_ . name , i ) for output in outputs : so . add_output ( mock_graph , name = output . name , dimensions = [ dim . dim_param if dim . dim_param != \"\" else dim . dim_value for dim in output . type . tensor_type . shape . dim ], data_type = inverse_data_dict [ output . type . tensor_type . elem_type ], ) input_names = [ inp . name for inp in inputs ] if output . name not in input_names : build_output_shape_tensor_ ( mock_graph , output , dynamic_shape_map , f \"dynamic_shape_ { output . name } \" , override = dynamic_shape_values , ) node = so . node ( \"ConstantOfShape\" , inputs = [ f \"dynamic_shape_ { output . name } \" ], outputs = [ output . name ], value = xhelp . make_tensor ( name = f \"dynamic_shape_ { output . name } _value\" , data_type = output . type . tensor_type . elem_type , dims = [ 1 ], vals = [ 0 ], ), name = f \"ConstantOfShape_ { output . name } \" , ) so . add_node ( mock_graph , node ) so . graph_to_file ( mock_graph , output_path , onnx_opset_version = 15 )","title":"create_stub_onnx_model()"},{"location":"inference_engine/reference/#npc_engine.services.utils.mock.main","text":"Create stub onnx model for tests with correct input and output shapes and names. Source code in npc_engine\\services\\utils\\mock.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @click . command () @click . option ( \"-d\" , \"--dynamic-dim\" , type = str , multiple = True , help = \"Specify dynamic dimension in format `<dim name>:<dim value>`.\" , ) @click . option ( \"-m\" , \"--onnx-model-path\" , required = True , type = str ) @click . option ( \"-o\" , \"--output-path\" , required = True , type = str ) def main ( onnx_model_path : str , output_path : str , dynamic_dim : List [ str ]): \"\"\"Create stub onnx model for tests with correct input and output shapes and names.\"\"\" dynamic_dims_map = {} for dim in dynamic_dim : dim_name , dim_value = dim . split ( \":\" ) dynamic_dims_map [ dim_name ] = int ( dim_value ) create_stub_onnx_model ( onnx_model_path , output_path , dynamic_dims_map )","title":"main()"},{"location":"inference_engine/reference/#npc_engine.version","text":"This module contains project version information.","title":"version"},{"location":"inference_engine/running_server/","text":"First lets get the npc-engine . The simplest way to install npc-engine is to use the pip command. pip install npc-engine[dml] If you are running it on linux you should specify cpu extra instead of dml as DirectML works only on Windows. To be able to ship it with your game you will need pyinstaller packaged version from: Releases page Resulting folder after following build instructions If using packaged version you should run cli.exe inside the npc-engine folder instead of npc-engine command. You can check all the possible commands via: npc-engine --help To start the server create models directory: mkdir models and execute cli.exe with run command npc-engine run --models-path models --port 5555 This will start a server using ZMQ sockets but if no models were added to the folder it will expose only control API. It's also possible to start the server with HTTP interface: npc-engine run --models-path models --port 5555 --http You can download default models via npc-engine download-default-models --models-path models See descriptions of the default models in Default Models section. NOTE Service API usage examples can be found in npc-engine\\tests\\integration . Now lets test npc-engine with this example request from python: First start the server on port 5555: Now run the following python script: import zmq context = zmq.Context() # Socket to talk to server print(\"Connecting to npc-engine server\") socket = context.socket(zmq.REQ) socket.RCVTIMEO = 2000 socket.connect(\"tcp://localhost:5555\") request = { \"jsonrpc\": \"2.0\", \"method\": \"start_service\", \"id\": 0, \"params\": [\"SimilarityAPI\"], } socket.send_json(request) message = socket.recv_json() request = { \"jsonrpc\": \"2.0\", \"method\": \"compare\", \"id\": 0, \"params\": [\"I will help you\", [\"I shall provide you my assistance\"]], } socket.send_json(request) message = socket.recv_json() print(f\"Response message {message}\") # You can also provide socket identity to select a specific service socket = context.socket(zmq.REQ) socket.setsockopt(zmq.IDENTITY, b\"control\") socket.RCVTIMEO = 2000 socket.connect(\"tcp://localhost:5555\") request = { \"jsonrpc\": \"2.0\", \"method\": \"get_services_metadata\", \"id\": 0, \"params\": [], } socket.send_json(request) message = socket.recv_json() print(f\"Services metadata {message}\")","title":"Running The Server"}]}