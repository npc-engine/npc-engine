{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome NPC Engine is a deep learning and natural language processing toolkit aimed at designing game NPC AI using natural language. It uses state of the art deep learning models and allows you to: Generate NPC lines based on the natural language descriptions of a character (e.g. his persona , location). Voice those lines with multiple voices (current model has 127) Trigger discrete actions based on semantic similarity between an action description and a player or NPC line. Create APIs to your own deep learning models. Use npc-engine Unity Unreal Engine 4 Godot Other Extend npc-engine Export model using existing API Add new API","title":"Home"},{"location":"#welcome","text":"NPC Engine is a deep learning and natural language processing toolkit aimed at designing game NPC AI using natural language. It uses state of the art deep learning models and allows you to: Generate NPC lines based on the natural language descriptions of a character (e.g. his persona , location). Voice those lines with multiple voices (current model has 127) Trigger discrete actions based on semantic similarity between an action description and a player or NPC line. Create APIs to your own deep learning models.","title":"Welcome"},{"location":"#use-npc-engine","text":"Unity Unreal Engine 4 Godot Other","title":"Use npc-engine"},{"location":"#extend-npc-engine","text":"Export model using existing API Add new API","title":"Extend npc-engine"},{"location":"not_ready/","text":"This integration is not ready yet But don't get discouraged, it's quite easy to create! Check out the integration checklist . If you plan to create one, please consider contributing! To contribute a new integration: Create an issue to discuss the integration architecture with us. Develop the integration, preferably with demo scene. Share it with permissive open source license. Create a pull request to npc-engine repo with documentation about your integration.","title":"This integration is not ready yet"},{"location":"not_ready/#this-integration-is-not-ready-yet","text":"But don't get discouraged, it's quite easy to create! Check out the integration checklist . If you plan to create one, please consider contributing! To contribute a new integration: Create an issue to discuss the integration architecture with us. Develop the integration, preferably with demo scene. Share it with permissive open source license. Create a pull request to npc-engine repo with documentation about your integration.","title":"This integration is not ready yet"},{"location":"inference_engine/api_classes/","text":"API class is an abstract class that corresponds to a certain task a service should perform (e.g. text-to-speech or chatbot) and defines interface methods for such a task as well as abstract methods for specific servicess to implement. All API classes are children of the BaseService class that handles registering service implementations and running them. Important It also should list the methods that are to be exposed as API via API_METHODS class variable. Important To be discovered correctly api classes must be imported into npc_engine.services module Existing APIs These are the existing API classes and corresponding API_METHODS: npc_engine.services.sequence_classifier.sequence_classifier_base.SequenceClassifierAPI Bases: BaseService Abstract base class for text classification models. cache = NumpyLRUCache ( cache_size ) instance-attribute npc_engine.services.text_generation.text_generation_base.TextGenerationAPI Bases: BaseService Abstract base class for text generation models. generate_reply ( context , * args , ** kwargs ) Format the model prompt and generate response. Parameters: Name Type Description Default context Dict [ str , Any ] Prompt context. required Returns: Type Description str Text response to a prompt. get_prompt_template () Return prompt template string used to render model prompt. Returns: Type Description str A template string. npc_engine.services.similarity.similarity_base.SimilarityAPI Bases: BaseService Abstract base class for text similarity models. compare ( query , context ) Compare a query to the context. Parameters: Name Type Description Default query str A sentence to compare. required context List [ str ] A list of sentences to compare to. This will be cached if caching is enabled required Returns: Type Description List [ float ] List of similarities cache ( context ) Cache embeddings of given sequences. Parameters: Name Type Description Default context List [ str ] A list of sentences to cache. required npc_engine.services.tts.tts_base.TextToSpeechAPI Bases: BaseService Abstract base class for text-to-speech models. tts_start ( speaker_id , text , n_chunks ) Initiate iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required tts_get_results () Retrieve the next chunk of generated speech. Returns: Type Description Iterable [ np . ndarray ] Next chunk of speech in the form of f32 ndarray. get_speaker_ids () abstractmethod Get list of available speaker ids. Returns: Type Description List [ str ] The return value. True for success, False otherwise. Creating new APIs You can use this dummy API example to create your own: from npc_engine.services.base_service import BaseService class EchoAPI(BaseService): API_METHODS: List[str] = [\"echo\"] def __init__(self, *args, **kwargs): pass def echo(self, text): return text Dont forget Import new API to npc-engine.services so that it is discovered. Models that are implemented for the API should appear there too. Calling other APIs There are a set of service clients that can be used to call other APIs that are running on the server. They will block other services so they should be used carefully. Module implementing the clients for services.","title":"API Classes"},{"location":"inference_engine/api_classes/#existing-apis","text":"These are the existing API classes and corresponding API_METHODS:","title":"Existing APIs"},{"location":"inference_engine/api_classes/#npc_engine.services.sequence_classifier.sequence_classifier_base.SequenceClassifierAPI","text":"Bases: BaseService Abstract base class for text classification models.","title":"SequenceClassifierAPI"},{"location":"inference_engine/api_classes/#npc_engine.services.sequence_classifier.sequence_classifier_base.SequenceClassifierAPI.cache","text":"","title":"cache"},{"location":"inference_engine/api_classes/#npc_engine.services.text_generation.text_generation_base.TextGenerationAPI","text":"Bases: BaseService Abstract base class for text generation models.","title":"TextGenerationAPI"},{"location":"inference_engine/api_classes/#npc_engine.services.text_generation.text_generation_base.TextGenerationAPI.generate_reply","text":"Format the model prompt and generate response. Parameters: Name Type Description Default context Dict [ str , Any ] Prompt context. required Returns: Type Description str Text response to a prompt.","title":"generate_reply()"},{"location":"inference_engine/api_classes/#npc_engine.services.text_generation.text_generation_base.TextGenerationAPI.get_prompt_template","text":"Return prompt template string used to render model prompt. Returns: Type Description str A template string.","title":"get_prompt_template()"},{"location":"inference_engine/api_classes/#npc_engine.services.similarity.similarity_base.SimilarityAPI","text":"Bases: BaseService Abstract base class for text similarity models.","title":"SimilarityAPI"},{"location":"inference_engine/api_classes/#npc_engine.services.similarity.similarity_base.SimilarityAPI.compare","text":"Compare a query to the context. Parameters: Name Type Description Default query str A sentence to compare. required context List [ str ] A list of sentences to compare to. This will be cached if caching is enabled required Returns: Type Description List [ float ] List of similarities","title":"compare()"},{"location":"inference_engine/api_classes/#npc_engine.services.similarity.similarity_base.SimilarityAPI.cache","text":"Cache embeddings of given sequences. Parameters: Name Type Description Default context List [ str ] A list of sentences to cache. required","title":"cache()"},{"location":"inference_engine/api_classes/#npc_engine.services.tts.tts_base.TextToSpeechAPI","text":"Bases: BaseService Abstract base class for text-to-speech models.","title":"TextToSpeechAPI"},{"location":"inference_engine/api_classes/#npc_engine.services.tts.tts_base.TextToSpeechAPI.tts_start","text":"Initiate iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required","title":"tts_start()"},{"location":"inference_engine/api_classes/#npc_engine.services.tts.tts_base.TextToSpeechAPI.tts_get_results","text":"Retrieve the next chunk of generated speech. Returns: Type Description Iterable [ np . ndarray ] Next chunk of speech in the form of f32 ndarray.","title":"tts_get_results()"},{"location":"inference_engine/api_classes/#npc_engine.services.tts.tts_base.TextToSpeechAPI.get_speaker_ids","text":"Get list of available speaker ids. Returns: Type Description List [ str ] The return value. True for success, False otherwise.","title":"get_speaker_ids()"},{"location":"inference_engine/api_classes/#creating-new-apis","text":"You can use this dummy API example to create your own: from npc_engine.services.base_service import BaseService class EchoAPI(BaseService): API_METHODS: List[str] = [\"echo\"] def __init__(self, *args, **kwargs): pass def echo(self, text): return text Dont forget Import new API to npc-engine.services so that it is discovered. Models that are implemented for the API should appear there too.","title":"Creating new APIs"},{"location":"inference_engine/api_classes/#calling-other-apis","text":"There are a set of service clients that can be used to call other APIs that are running on the server. They will block other services so they should be used carefully. Module implementing the clients for services.","title":"Calling other APIs"},{"location":"inference_engine/benchmarks/","text":"","title":"Benchmarks"},{"location":"inference_engine/building/","text":"Build on Windows Create virtualenv and activate it python3 -m venv npc-engine-venv .\\npc-engine-venv\\activate.bat Install dependencies pip install -e .[dev,dml] (Optional) Compile, build and install your custom ONNX python runtime\" Build instructions can be found here (Optional) Run tests tox -e py38 Compile to exe with > pyinstaller --hidden-import=\"sklearn.utils._cython_blas\" --hidden-import=\"sklearn.neighbors.typedefs\" ^ --hidden-import=\"sklearn.neighbors.quad_tree\" --hidden-import=\"sklearn.tree._utils\" ^ --hidden-import=\"sklearn.neighbors._typedefs\" --hidden-import=\"sklearn.utils._typedefs\" ^ --hidden-import=\"sklearn.neighbors._partition_nodes\" --additional-hooks-dir hooks ^ --exclude-module tkinter --exclude-module matplotlib .\\npc_engine\\cli.py --onedir","title":"Build From Source"},{"location":"inference_engine/building/#build-on-windows","text":"","title":"Build on Windows"},{"location":"inference_engine/building/#create-virtualenv-and-activate-it","text":"python3 -m venv npc-engine-venv .\\npc-engine-venv\\activate.bat","title":"Create virtualenv and activate it"},{"location":"inference_engine/building/#install-dependencies","text":"pip install -e .[dev,dml]","title":"Install dependencies"},{"location":"inference_engine/building/#optional-compile-build-and-install-your-custom-onnx-python-runtime","text":"Build instructions can be found here","title":"(Optional) Compile, build and install your custom ONNX python runtime\""},{"location":"inference_engine/building/#optional-run-tests","text":"tox -e py38","title":"(Optional) Run tests"},{"location":"inference_engine/building/#compile-to-exe-with","text":"> pyinstaller --hidden-import=\"sklearn.utils._cython_blas\" --hidden-import=\"sklearn.neighbors.typedefs\" ^ --hidden-import=\"sklearn.neighbors.quad_tree\" --hidden-import=\"sklearn.tree._utils\" ^ --hidden-import=\"sklearn.neighbors._typedefs\" --hidden-import=\"sklearn.utils._typedefs\" ^ --hidden-import=\"sklearn.neighbors._partition_nodes\" --additional-hooks-dir hooks ^ --exclude-module tkinter --exclude-module matplotlib .\\npc_engine\\cli.py --onedir","title":"Compile to exe with"},{"location":"inference_engine/exporting_models/","text":"It's possible to convert models from popular libraries into npc-engine services via npc-engine-import-wizard . You can find the installation instructions and list of supported libraries in it's README. Most import wizards require some extras installed, here is the list of possible extras: transformers for \ud83e\udd17 Transformers integration. flowtron-tts for NVIDIA's Flowtron integration. espnet for ESPNet2 integration. Some extras might require additional dependencies installed outside of pip, they will usually report them missing via error messages. Tutorials To export model you need to use npc-engine-import-wizard import command. With Huggingface models you can use Huggingface Hub model id as well as path to the model. npc-engine-import-wizard import --models-path <path-to-models-folder> <model_id or folder> This will prompt you to select correct Exporter class and take you through filling in the required parameters. We will try to cover all the import wizards in the upcoming video tutorials that will appear here.","title":"Exporting models"},{"location":"inference_engine/exporting_models/#tutorials","text":"To export model you need to use npc-engine-import-wizard import command. With Huggingface models you can use Huggingface Hub model id as well as path to the model. npc-engine-import-wizard import --models-path <path-to-models-folder> <model_id or folder> This will prompt you to select correct Exporter class and take you through filling in the required parameters. We will try to cover all the import wizards in the upcoming video tutorials that will appear here.","title":"Tutorials"},{"location":"inference_engine/models/","text":"Service classes are specific implementations of API classes. They define the loading process in __init__ function and all the abstract methods required by the API class to function. How services are configured? MetadataManager Scans the models folder. For each discovered subfolder ServiceManager validates the config.yml and creates descriptors with metadata for each service. The mandatory field of config.yml is type (or model_type ) that must contain correct service class that was discovered and registered by BaseService parent class. This service class will be instantiated with parsed dictionary as parameters on ControlService.start_service request to control service. How is their API exposed? When service is started ControlService starts a new process with BaseService message handling loop. BaseService handles ZMQ IPC requests to it's exposed functions from API's API_METHODS class variable to the main process, while main process handles routing requests to this service. Existing service classes npc_engine.services.sequence_classifier.hf_classifier.HfClassifier Bases: SequenceClassifierAPI Huggingface transformers sequence classification. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with sequence-classification feature. Also requires saved tokenizer with huggingface tokenizers. __init__ ( model_path , * args , ** kwargs ) Create and load biencoder model for semantic similarity. Parameters: Name Type Description Default model_path str A path where model config and weights are. required compute_scores_batch ( texts ) Compute scores. Parameters: Name Type Description Default texts List [ Union [ str , Tuple [ str , str ]]] Sentence to embed required Returns: Name Type Description scores np . ndarray Scores for each text npc_engine.services.text_generation.hf_text_generation.HfChatbot Bases: TextGenerationAPI Chatbot that uses Huggingface transformer architectures. ONNX export of Huggingface transformer is required (see https://huggingface.co/docs/transformers/serialization). Features seq2seq-lm, causal-lm, seq2seq-lm-with-past, causal-lm-with-past are supported __init__ ( model_path , max_length = 100 , min_length = 2 , repetition_penalty = 1 , * args , ** kwargs ) Create the chatbot from config args and kwargs. Parameters: Name Type Description Default model_path path to scan for model files (weights and configs) required max_length stop generation at this number of tokens 100 min_length model can't stop generating text before it's atleast this long in tokens 2 repetition_penalty probability coef for same tokens to appear multiple times 1 create_starter_inputs ( prompt = '' ) Create starter inputs for the model. Parameters: Name Type Description Default prompt str Prompt to start generation from. '' Returns: Type Description Dict [ str , Any ] Dict of inputs to the model decode_logit ( logit , temperature , topk ) Decode logit to token. Parameters: Name Type Description Default logit np . ndarray Logit to decode of shape (vocab_size,) required Returns: Type Description int Decoded token of shape get_special_tokens () Retrun dict of special tokens to be renderable from template. run ( prompt , temperature = 1.0 , topk = None ) Run text generation from given prompt and parameters. Parameters: Name Type Description Default prompt str Fromatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1.0 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text update_inputs_with_results ( inputs , results , decoded_token ) Update inputs with results from model. Parameters: Name Type Description Default inputs Dict [ str , np . ndarray ] Inputs to the model required results Dict [ str , np . ndarray ] Results from the model required Returns: Type Description Dict [ str , np . ndarray ] Updated inputs Dict [ str , np . ndarray ] Finished generation npc_engine.services.text_generation.bart.BartChatbot Bases: TextGenerationAPI BART based chatbot implementation class. This model class requires two ONNX models encoder_bart.onnx and decoder_bart.onnx that correspond to encoder and decoder from transformers EncoderDecoderModel and a tokenizer.json with huggingface tokenizers definition. encoder_bart.onnx spec: - inputs: `input_ids` - outputs: `encoder_hidden_state` decoder_bart.onnx spec: - inputs: `encoder_hidden_state` `decoder_input_ids` - outputs: `logits` __init__ ( model_path , max_steps = 100 , min_length = 2 , repetition_penalty = 1 , bos_token_id = 0 , eos_token_id = 2 , pad_token_id = 1 , sep_token_id = None , * args , ** kwargs ) Create the chatbot from config args and kwargs. Parameters: Name Type Description Default model_path path to scan for model files (weights and configs) required max_steps stop generation at this number of tokens 100 min_length model can't stop generating text before it's atleast this long in tokens 2 repetition_penalty probability coef for same tokens to appear multiple times 1 bos_token_id beginning of sequence token id 0 eos_token_id end of sequence token id 2 pad_token_id padding token id 1 sep_token_id token id for separating sequence into multiple parts None get_special_tokens () Retrun dict of special tokens to be renderable from template. run ( prompt , temperature = 1.0 , topk = None ) Run text generation from given prompt and parameters. Parameters: Name Type Description Default prompt str Fromatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1.0 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text npc_engine.services.similarity.similarity_transformers.TransformerSemanticSimilarity Bases: SimilarityAPI Huggingface transformers semantic similarity. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with biencoder architecture. Also requires a tokenizer.json with huggingface tokenizers definition. model.onnx spec: - inputs: `input_ids` of shape `(batch_size, sequence)` `attention_mask` of shape `(batch_size, sequence)` (Optional) `input_type_ids` of shape `(batch_size, sequence)` - outputs: `token_embeddings` of shape `(batch_size, sequence, hidden_size)` __init__ ( model_path , metric = 'dot' , * args , ** kwargs ) Create and load biencoder model for semantic similarity. Parameters: Name Type Description Default model_path str A path where model config and weights are required metric str distance to compute semantic similarity 'dot' compute_embedding ( line ) Compute sentence embedding. Parameters: Name Type Description Default line str Sentence to embed required Returns: Type Description np . ndarray Embedding of shape (1, embedding_size) compute_embedding_batch ( lines ) Compute line embeddings in batch. Parameters: Name Type Description Default lines List [ str ] List of sentences to embed required Returns: Type Description np . ndarray Embedding batch of shape (batch_size, embedding_size) metric ( embedding_a , embedding_b ) Similarity between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Parameters: Name Type Description Default embedding_a np . ndarray Embedding of shape (1 or batch_size, embedding_size) required embedding_b np . ndarray Embedding of shape (1 or batch_size, embedding_size) required Returns: Type Description np . ndarray Vector of distances (batch_size or 1,) npc_engine.services.tts.flowtron.FlowtronTTS Bases: TextToSpeechAPI Implements Flowtron architecture inference. Paper: arXiv:2005.05957 Code: https://github.com/NVIDIA/flowtron Onnx export script can be found in this fork https://github.com/npc-engine/flowtron. This model class requires four ONNX models encoder.onnx , backward_flow.onnx , forward_flow.onnx and vocoder.onnx where first three are layers from Flowtron architecture ( flow corresponding to one direction pass of affine coupling layers) and vocoder.onnx is neural vocoder. For detailed specs refer to https://github.com/npc-engine/flowtron. __init__ ( model_path , max_frames = 400 , gate_threshold = 0.5 , sigma = 0.8 , smoothing_window = 3 , smoothing_weight = 0.5 , * args , ** kwargs ) Create and load Flowtron and vocoder models. get_speaker_ids () Return available ids of different speakers. run ( speaker_id , text , n_chunks ) Create a generator for iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Returns: Type Description Iterator [ np . ndarray ] Generator that yields next chunk of speech in the form of f32 ndarray. Default Models Fantasy Chatbot BartChatbot trained on LIGHT Dataset . Model consumes both self, other personas and location dialogue is happening in. Semantic Similarity sentence-transformers/all-MiniLM-L6-v2 Onnx export of sentence-transformers/all-MiniLM-L6-v2 . FlowtronTTS with Waveglow vocoder Nvidia's FlowtronTTS architecture using Waveglow vocoder. Weights were published by the authors, this model uses Flowtron LibriTTS2K version. Speech to text NeMo models This model is still heavy WIP it is best to use your platform's of choice existing solutions e.g. UnityEngine.Windows.Speech.DictationRecognizer in Unity. This implementation uses several models exported from NeMo toolkit: QuartzNet15x5 for transcription. Punctuation BERT for applying punctuation. Custom transformer for recognizing end of response to the context initialized from all-MiniLM-L6-v2 Creating new Services You can use this dummy model example to create your own: from npc_engine.services.text_generation.text_generation_base import TextGenerationAPI class EchoService(TextGenerationAPI): def __init__(self, model_path:str, *args, **kwargs): print(\"model is in {model_path}\") def get_special_tokens(self): return {} def run(self, prompt, temperature=1, topk=None): return prompt Dont forget Import new model to npc-engine.services so that it is discovered. Using other services from your service You can use other services from your service by using service clients. They can be created from inside the service with self.create_client(name) where name is the name of the dependency. These clients expose the same API as the dependency and can be just called from your service.","title":"Services"},{"location":"inference_engine/models/#how-services-are-configured","text":"MetadataManager Scans the models folder. For each discovered subfolder ServiceManager validates the config.yml and creates descriptors with metadata for each service. The mandatory field of config.yml is type (or model_type ) that must contain correct service class that was discovered and registered by BaseService parent class. This service class will be instantiated with parsed dictionary as parameters on ControlService.start_service request to control service.","title":"How services are configured?"},{"location":"inference_engine/models/#how-is-their-api-exposed","text":"When service is started ControlService starts a new process with BaseService message handling loop. BaseService handles ZMQ IPC requests to it's exposed functions from API's API_METHODS class variable to the main process, while main process handles routing requests to this service.","title":"How is their API exposed?"},{"location":"inference_engine/models/#existing-service-classes","text":"","title":"Existing service classes"},{"location":"inference_engine/models/#npc_engine.services.sequence_classifier.hf_classifier.HfClassifier","text":"Bases: SequenceClassifierAPI Huggingface transformers sequence classification. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with sequence-classification feature. Also requires saved tokenizer with huggingface tokenizers.","title":"HfClassifier"},{"location":"inference_engine/models/#npc_engine.services.sequence_classifier.hf_classifier.HfClassifier.__init__","text":"Create and load biencoder model for semantic similarity. Parameters: Name Type Description Default model_path str A path where model config and weights are. required","title":"__init__()"},{"location":"inference_engine/models/#npc_engine.services.sequence_classifier.hf_classifier.HfClassifier.compute_scores_batch","text":"Compute scores. Parameters: Name Type Description Default texts List [ Union [ str , Tuple [ str , str ]]] Sentence to embed required Returns: Name Type Description scores np . ndarray Scores for each text","title":"compute_scores_batch()"},{"location":"inference_engine/models/#npc_engine.services.text_generation.hf_text_generation.HfChatbot","text":"Bases: TextGenerationAPI Chatbot that uses Huggingface transformer architectures. ONNX export of Huggingface transformer is required (see https://huggingface.co/docs/transformers/serialization). Features seq2seq-lm, causal-lm, seq2seq-lm-with-past, causal-lm-with-past are supported","title":"HfChatbot"},{"location":"inference_engine/models/#npc_engine.services.text_generation.hf_text_generation.HfChatbot.__init__","text":"Create the chatbot from config args and kwargs. Parameters: Name Type Description Default model_path path to scan for model files (weights and configs) required max_length stop generation at this number of tokens 100 min_length model can't stop generating text before it's atleast this long in tokens 2 repetition_penalty probability coef for same tokens to appear multiple times 1","title":"__init__()"},{"location":"inference_engine/models/#npc_engine.services.text_generation.hf_text_generation.HfChatbot.create_starter_inputs","text":"Create starter inputs for the model. Parameters: Name Type Description Default prompt str Prompt to start generation from. '' Returns: Type Description Dict [ str , Any ] Dict of inputs to the model","title":"create_starter_inputs()"},{"location":"inference_engine/models/#npc_engine.services.text_generation.hf_text_generation.HfChatbot.decode_logit","text":"Decode logit to token. Parameters: Name Type Description Default logit np . ndarray Logit to decode of shape (vocab_size,) required Returns: Type Description int Decoded token of shape","title":"decode_logit()"},{"location":"inference_engine/models/#npc_engine.services.text_generation.hf_text_generation.HfChatbot.get_special_tokens","text":"Retrun dict of special tokens to be renderable from template.","title":"get_special_tokens()"},{"location":"inference_engine/models/#npc_engine.services.text_generation.hf_text_generation.HfChatbot.run","text":"Run text generation from given prompt and parameters. Parameters: Name Type Description Default prompt str Fromatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1.0 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text","title":"run()"},{"location":"inference_engine/models/#npc_engine.services.text_generation.hf_text_generation.HfChatbot.update_inputs_with_results","text":"Update inputs with results from model. Parameters: Name Type Description Default inputs Dict [ str , np . ndarray ] Inputs to the model required results Dict [ str , np . ndarray ] Results from the model required Returns: Type Description Dict [ str , np . ndarray ] Updated inputs Dict [ str , np . ndarray ] Finished generation","title":"update_inputs_with_results()"},{"location":"inference_engine/models/#npc_engine.services.text_generation.bart.BartChatbot","text":"Bases: TextGenerationAPI BART based chatbot implementation class. This model class requires two ONNX models encoder_bart.onnx and decoder_bart.onnx that correspond to encoder and decoder from transformers EncoderDecoderModel and a tokenizer.json with huggingface tokenizers definition. encoder_bart.onnx spec: - inputs: `input_ids` - outputs: `encoder_hidden_state` decoder_bart.onnx spec: - inputs: `encoder_hidden_state` `decoder_input_ids` - outputs: `logits`","title":"BartChatbot"},{"location":"inference_engine/models/#npc_engine.services.text_generation.bart.BartChatbot.__init__","text":"Create the chatbot from config args and kwargs. Parameters: Name Type Description Default model_path path to scan for model files (weights and configs) required max_steps stop generation at this number of tokens 100 min_length model can't stop generating text before it's atleast this long in tokens 2 repetition_penalty probability coef for same tokens to appear multiple times 1 bos_token_id beginning of sequence token id 0 eos_token_id end of sequence token id 2 pad_token_id padding token id 1 sep_token_id token id for separating sequence into multiple parts None","title":"__init__()"},{"location":"inference_engine/models/#npc_engine.services.text_generation.bart.BartChatbot.get_special_tokens","text":"Retrun dict of special tokens to be renderable from template.","title":"get_special_tokens()"},{"location":"inference_engine/models/#npc_engine.services.text_generation.bart.BartChatbot.run","text":"Run text generation from given prompt and parameters. Parameters: Name Type Description Default prompt str Fromatted prompt. required temperature float Temperature parameter for sampling. Controls how random model output is: more temperature - more randomness 1.0 topk int If not none selects top n of predictions to sample from during generation. None Returns: Type Description str Generated text","title":"run()"},{"location":"inference_engine/models/#npc_engine.services.similarity.similarity_transformers.TransformerSemanticSimilarity","text":"Bases: SimilarityAPI Huggingface transformers semantic similarity. Uses ONNX export of Huggingface transformers (https://huggingface.co/models) with biencoder architecture. Also requires a tokenizer.json with huggingface tokenizers definition. model.onnx spec: - inputs: `input_ids` of shape `(batch_size, sequence)` `attention_mask` of shape `(batch_size, sequence)` (Optional) `input_type_ids` of shape `(batch_size, sequence)` - outputs: `token_embeddings` of shape `(batch_size, sequence, hidden_size)`","title":"TransformerSemanticSimilarity"},{"location":"inference_engine/models/#npc_engine.services.similarity.similarity_transformers.TransformerSemanticSimilarity.__init__","text":"Create and load biencoder model for semantic similarity. Parameters: Name Type Description Default model_path str A path where model config and weights are required metric str distance to compute semantic similarity 'dot'","title":"__init__()"},{"location":"inference_engine/models/#npc_engine.services.similarity.similarity_transformers.TransformerSemanticSimilarity.compute_embedding","text":"Compute sentence embedding. Parameters: Name Type Description Default line str Sentence to embed required Returns: Type Description np . ndarray Embedding of shape (1, embedding_size)","title":"compute_embedding()"},{"location":"inference_engine/models/#npc_engine.services.similarity.similarity_transformers.TransformerSemanticSimilarity.compute_embedding_batch","text":"Compute line embeddings in batch. Parameters: Name Type Description Default lines List [ str ] List of sentences to embed required Returns: Type Description np . ndarray Embedding batch of shape (batch_size, embedding_size)","title":"compute_embedding_batch()"},{"location":"inference_engine/models/#npc_engine.services.similarity.similarity_transformers.TransformerSemanticSimilarity.metric","text":"Similarity between two embeddings. Embeddings are of broadcastable shapes. (1 or batch_size) Parameters: Name Type Description Default embedding_a np . ndarray Embedding of shape (1 or batch_size, embedding_size) required embedding_b np . ndarray Embedding of shape (1 or batch_size, embedding_size) required Returns: Type Description np . ndarray Vector of distances (batch_size or 1,)","title":"metric()"},{"location":"inference_engine/models/#npc_engine.services.tts.flowtron.FlowtronTTS","text":"Bases: TextToSpeechAPI Implements Flowtron architecture inference. Paper: arXiv:2005.05957 Code: https://github.com/NVIDIA/flowtron Onnx export script can be found in this fork https://github.com/npc-engine/flowtron. This model class requires four ONNX models encoder.onnx , backward_flow.onnx , forward_flow.onnx and vocoder.onnx where first three are layers from Flowtron architecture ( flow corresponding to one direction pass of affine coupling layers) and vocoder.onnx is neural vocoder. For detailed specs refer to https://github.com/npc-engine/flowtron.","title":"FlowtronTTS"},{"location":"inference_engine/models/#npc_engine.services.tts.flowtron.flowtron.FlowtronTTS.__init__","text":"Create and load Flowtron and vocoder models.","title":"__init__()"},{"location":"inference_engine/models/#npc_engine.services.tts.flowtron.flowtron.FlowtronTTS.get_speaker_ids","text":"Return available ids of different speakers.","title":"get_speaker_ids()"},{"location":"inference_engine/models/#npc_engine.services.tts.flowtron.flowtron.FlowtronTTS.run","text":"Create a generator for iterative generation of speech. Parameters: Name Type Description Default speaker_id str Id of the speaker. required text str Text to generate speech from. required n_chunks int Number of chunks to split generation into. required Returns: Type Description Iterator [ np . ndarray ] Generator that yields next chunk of speech in the form of f32 ndarray.","title":"run()"},{"location":"inference_engine/models/#default-models","text":"","title":"Default Models"},{"location":"inference_engine/models/#fantasy-chatbot","text":"BartChatbot trained on LIGHT Dataset . Model consumes both self, other personas and location dialogue is happening in.","title":"Fantasy Chatbot"},{"location":"inference_engine/models/#semantic-similarity-sentence-transformersall-minilm-l6-v2","text":"Onnx export of sentence-transformers/all-MiniLM-L6-v2 .","title":"Semantic Similarity sentence-transformers/all-MiniLM-L6-v2"},{"location":"inference_engine/models/#flowtrontts-with-waveglow-vocoder","text":"Nvidia's FlowtronTTS architecture using Waveglow vocoder. Weights were published by the authors, this model uses Flowtron LibriTTS2K version.","title":"FlowtronTTS with Waveglow vocoder"},{"location":"inference_engine/models/#speech-to-text-nemo-models","text":"This model is still heavy WIP it is best to use your platform's of choice existing solutions e.g. UnityEngine.Windows.Speech.DictationRecognizer in Unity. This implementation uses several models exported from NeMo toolkit: QuartzNet15x5 for transcription. Punctuation BERT for applying punctuation. Custom transformer for recognizing end of response to the context initialized from all-MiniLM-L6-v2","title":"Speech to text NeMo models"},{"location":"inference_engine/models/#creating-new-services","text":"You can use this dummy model example to create your own: from npc_engine.services.text_generation.text_generation_base import TextGenerationAPI class EchoService(TextGenerationAPI): def __init__(self, model_path:str, *args, **kwargs): print(\"model is in {model_path}\") def get_special_tokens(self): return {} def run(self, prompt, temperature=1, topk=None): return prompt Dont forget Import new model to npc-engine.services so that it is discovered.","title":"Creating new Services"},{"location":"inference_engine/models/#using-other-services-from-your-service","text":"You can use other services from your service by using service clients. They can be created from inside the service with self.create_client(name) where name is the name of the dependency. These clients expose the same API as the dependency and can be just called from your service.","title":"Using other services from your service"},{"location":"inference_engine/overview/","text":"NPC Engine is a local python JSON-RPC server. It supports using 0MQ TCP sockets and HTTP protocols. The main goal of this server is to provide access to inference services that handle various functionalities required for storytelling and in-game AI. How does it work Each service is defined in it's own folder in provided --models-path . Service's folder name becomes it's unique Id. On start NPC Engine will expose a special service called control that allows you to get services metadata and to control their lifetime. Services are running in separate processes but handle requests sequentially (including situations when services call each-other). To route requests NPC Engine uses 0MQ identity . It can be defined only before connecting, therefore each connected socket will be refering to it's own service. Service resolution rules are simple: - If no identity provided, request is routed to the first service that implements the method called. - If identity is API name, request is routed to the first service that implements the API. - Same logic in case identity is a service class name. - Identity could be a service ID (folder name), then request is routed to the service with that ID. Service APIs are defined in API classes . Service exposes methods that are listed in API_METHODS class variable. You can find a description of default services available in Default Models section. The specifics of how model is loaded and how inference is done is defined in specific service classes Here is an example of JSON RPC request: request with identity \"some_model\" { \"method\": \"do_smth\", \"params\": [\"hello\"], \"jsonrpc\": \"2.0\", \"id\": 0 } will result in call to some_model.do_smth('hello') on the server, or will fail if the service wasn't started. Creating an integration A checklist for a new integration would be to: Create a class that manages npc-engine subprocess (starts, terminates, checks if it's alive). Create a connection class that talks to npc-engine. Review API classes and wrap JSON-RPC requests into the native functions.","title":"Overview"},{"location":"inference_engine/overview/#how-does-it-work","text":"Each service is defined in it's own folder in provided --models-path . Service's folder name becomes it's unique Id. On start NPC Engine will expose a special service called control that allows you to get services metadata and to control their lifetime. Services are running in separate processes but handle requests sequentially (including situations when services call each-other). To route requests NPC Engine uses 0MQ identity . It can be defined only before connecting, therefore each connected socket will be refering to it's own service. Service resolution rules are simple: - If no identity provided, request is routed to the first service that implements the method called. - If identity is API name, request is routed to the first service that implements the API. - Same logic in case identity is a service class name. - Identity could be a service ID (folder name), then request is routed to the service with that ID. Service APIs are defined in API classes . Service exposes methods that are listed in API_METHODS class variable. You can find a description of default services available in Default Models section. The specifics of how model is loaded and how inference is done is defined in specific service classes Here is an example of JSON RPC request: request with identity \"some_model\" { \"method\": \"do_smth\", \"params\": [\"hello\"], \"jsonrpc\": \"2.0\", \"id\": 0 } will result in call to some_model.do_smth('hello') on the server, or will fail if the service wasn't started.","title":"How does it work"},{"location":"inference_engine/overview/#creating-an-integration","text":"A checklist for a new integration would be to: Create a class that manages npc-engine subprocess (starts, terminates, checks if it's alive). Create a connection class that talks to npc-engine. Review API classes and wrap JSON-RPC requests into the native functions.","title":"Creating an integration"},{"location":"inference_engine/reference/","text":"Server for providing onnx runtime predictions for text generation and speech synthesis. Uses 0MQ REP/REQ sockets with JSONRPC 2.0 protocol.","title":"Reference"},{"location":"inference_engine/running_server/","text":"First lets get the npc-engine . The simplest way to install npc-engine is to use the pip command. pip install npc-engine[dml] If you are running it on linux you should specify cpu extra instead of dml as DirectML works only on Windows. To be able to ship it with your game you will need pyinstaller packaged version from: Releases page Resulting folder after following build instructions If using packaged version you should run cli.exe inside the npc-engine folder instead of npc-engine command. You can check all the possible commands via: npc-engine --help To start the server create models directory: mkdir models and execute cli.exe with run command npc-engine run --models-path models --port 5555 This will start a server using ZMQ sockets but if no models were added to the folder it will expose only control API. It's also possible to start the server with HTTP interface: npc-engine run --models-path models --port 5555 --http You can download default models via npc-engine download-default-models --models-path models See descriptions of the default models in Default Models section. NOTE Service API usage examples can be found in npc-engine\\tests\\integration . Now lets test npc-engine with this example request from python: First start the server on port 5555: Now run the following python script: import zmq context = zmq.Context() # Socket to talk to server print(\"Connecting to npc-engine server\") socket = context.socket(zmq.REQ) socket.RCVTIMEO = 2000 socket.connect(\"tcp://localhost:5555\") request = { \"jsonrpc\": \"2.0\", \"method\": \"start_service\", \"id\": 0, \"params\": [\"SimilarityAPI\"], } socket.send_json(request) message = socket.recv_json() request = { \"jsonrpc\": \"2.0\", \"method\": \"compare\", \"id\": 0, \"params\": [\"I will help you\", [\"I shall provide you my assistance\"]], } socket.send_json(request) message = socket.recv_json() print(f\"Response message {message}\") # You can also provide socket identity to select a specific service socket = context.socket(zmq.REQ) socket.setsockopt(zmq.IDENTITY, b\"control\") socket.RCVTIMEO = 2000 socket.connect(\"tcp://localhost:5555\") request = { \"jsonrpc\": \"2.0\", \"method\": \"get_services_metadata\", \"id\": 0, \"params\": [], } socket.send_json(request) message = socket.recv_json() print(f\"Services metadata {message}\")","title":"Running The Server"}]}